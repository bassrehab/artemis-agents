{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#artemis-agents","title":"ARTEMIS Agents","text":"<p>Adaptive Reasoning Through Evaluation of Multi-agent Intelligent Systems</p> <p>A production-ready framework for structured multi-agent debates with adaptive evaluation, causal reasoning, and built-in safety monitoring.</p>"},{"location":"#what-is-artemis","title":"What is ARTEMIS?","text":"<p>ARTEMIS is an open-source implementation of the Adaptive Reasoning and Evaluation Framework for Multi-agent Intelligent Systems \u2014 a framework designed to improve complex decision-making through structured debates between AI agents.</p> <p>Unlike general-purpose multi-agent frameworks, ARTEMIS is purpose-built for debate-driven decision-making with:</p> <ul> <li>Hierarchical Argument Generation (H-L-DAG): Structured, context-aware argument synthesis at strategic, tactical, and operational levels</li> <li>Adaptive Evaluation with Causal Reasoning (L-AE-CR): Dynamic criteria weighting with causal analysis</li> <li>Jury Scoring Mechanism: Fair, multi-perspective evaluation of arguments</li> <li>Ethical Alignment: Built-in ethical considerations in both generation and evaluation</li> <li>Safety Monitoring: Real-time detection of sandbagging, deception, and manipulation</li> </ul>"},{"location":"#why-artemis","title":"Why ARTEMIS?","text":"Feature AutoGen CrewAI CAMEL ARTEMIS Multi-agent debates Basic Basic 2-3 agents N agents Structured argument generation No No No H-L-DAG Causal reasoning No No No L-AE-CR Adaptive evaluation No No No Dynamic weights Ethical alignment No No No Built-in Sandbagging detection No No No Metacognition Reasoning model support Limited Limited No o1/R1 native MCP server mode No No No Yes"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from artemis import Debate, Agent\n\n# Create debate agents\nagents = [\n    Agent(name=\"Proponent\", role=\"Argues in favor\", model=\"gpt-4o\"),\n    Agent(name=\"Opponent\", role=\"Argues against\", model=\"gpt-4o\"),\n]\n\n# Run the debate\ndebate = Debate(\n    topic=\"Should AI systems be given legal personhood?\",\n    agents=agents,\n    rounds=3\n)\n\nresult = await debate.run()\n\nprint(f\"Verdict: {result.verdict.decision}\")\nprint(f\"Confidence: {result.verdict.confidence:.0%}\")\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#structured-debates","title":"Structured Debates","text":"<p>ARTEMIS implements a rigorous debate structure with:</p> <ul> <li>Opening statements from each agent</li> <li>Multiple argumentation rounds with rebuttals</li> <li>Evidence-based reasoning with causal links</li> <li>Jury deliberation for fair verdicts</li> </ul>"},{"location":"#safety-first","title":"Safety First","text":"<p>Built-in monitors detect problematic AI behavior:</p> <ul> <li>Sandbagging Detection: Identifies when agents deliberately underperform</li> <li>Deception Monitoring: Catches misleading arguments or manipulation</li> <li>Behavior Tracking: Monitors for unexpected behavioral drift</li> <li>Ethics Guard: Ensures debates stay within ethical bounds</li> </ul>"},{"location":"#framework-integrations","title":"Framework Integrations","text":"<p>Use ARTEMIS with your existing tools:</p> <ul> <li>LangChain: As a structured tool</li> <li>LangGraph: As a workflow node</li> <li>CrewAI: As a crew tool</li> <li>MCP: As a universal server</li> </ul>"},{"location":"#research-foundation","title":"Research Foundation","text":"<p>ARTEMIS is based on peer-reviewed research:</p> <p>Adaptive Reasoning and Evaluation Framework for Multi-agent Intelligent Systems in Debate-driven Decision-making Mitra, S. (2025). Technical Disclosure Commons. Read the paper</p>"},{"location":"#benchmarks","title":"Benchmarks","text":"<p>We've run ARTEMIS against AutoGen, CrewAI, and CAMEL across 60 structured debates. See the benchmark results and analysis in the README.</p>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to dive in? Check out the Installation Guide or jump straight to the Quick Start.</p>"},{"location":"#license","title":"License","text":"<p>ARTEMIS is released under the Apache License 2.0. See LICENSE for details.</p>"},{"location":"contributing/","title":"Contributing to ARTEMIS","text":"<p>Thank you for your interest in contributing to ARTEMIS! This guide will help you get started.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Git</li> <li>A GitHub account</li> </ul>"},{"location":"contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<ol> <li>Fork and clone the repository</li> </ol> <pre><code>git clone https://github.com/your-username/artemis-agents.git\ncd artemis-agents\n</code></pre> <ol> <li>Create a virtual environment</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# or: venv\\Scripts\\activate  # Windows\n</code></pre> <ol> <li>Install development dependencies</li> </ol> <pre><code>pip install -e \".[dev]\"\n</code></pre> <ol> <li>Set up environment variables</li> </ol> <pre><code>cp .env.example .env\n# Edit .env with your API keys\n</code></pre> <ol> <li>Run tests to verify setup</li> </ol> <pre><code>pytest tests/ -v\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#creating-a-branch","title":"Creating a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or: git checkout -b fix/your-bug-fix\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest tests/ -v\n\n# Run specific test file\npytest tests/unit/test_debate.py -v\n\n# Run with coverage\npytest tests/ --cov=artemis --cov-report=html\n</code></pre>"},{"location":"contributing/#code-quality","title":"Code Quality","text":"<pre><code># Type checking\nmypy artemis/\n\n# Linting\nruff check artemis/\n\n# Formatting\nruff format artemis/\n</code></pre>"},{"location":"contributing/#pre-commit-checks","title":"Pre-commit Checks","text":"<p>Before committing, run:</p> <pre><code># Format code\nruff format artemis/\n\n# Check for issues\nruff check artemis/\n\n# Run type checks\nmypy artemis/\n\n# Run tests\npytest tests/\n</code></pre>"},{"location":"contributing/#code-guidelines","title":"Code Guidelines","text":""},{"location":"contributing/#type-hints","title":"Type Hints","text":"<p>Use Python 3.10+ type hints everywhere:</p> <pre><code># Good\ndef process_argument(\n    argument: Argument,\n    context: DebateContext | None = None,\n) -&gt; ProcessedArgument:\n    ...\n\n# Avoid\ndef process_argument(argument, context=None):\n    ...\n</code></pre>"},{"location":"contributing/#pydantic-models","title":"Pydantic Models","text":"<p>Use Pydantic v2 for data classes:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Argument(BaseModel):\n    content: str = Field(description=\"The argument content\")\n    level: ArgumentLevel\n    evidence: list[Evidence] = Field(default_factory=list)\n</code></pre>"},{"location":"contributing/#async-code","title":"Async Code","text":"<p>Core debate logic should be async:</p> <pre><code># Good\nasync def generate_argument(self, context: DebateContext) -&gt; Argument:\n    response = await self.model.generate(messages)\n    return self.parse_argument(response)\n\n# Avoid sync in core logic\ndef generate_argument(self, context: DebateContext) -&gt; Argument:\n    ...\n</code></pre>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>async def evaluate(\n    self,\n    argument: Argument,\n    context: DebateContext,\n) -&gt; Evaluation:\n    \"\"\"Evaluate an argument using adaptive criteria.\n\n    Args:\n        argument: The argument to evaluate.\n        context: The debate context for evaluation.\n\n    Returns:\n        Evaluation result with scores and feedback.\n\n    Raises:\n        EvaluationError: If evaluation fails.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"contributing/#error-handling","title":"Error Handling","text":"<p>Use custom exceptions:</p> <pre><code>from artemis.exceptions import DebateError, AgentError\n\ntry:\n    result = await agent.generate_argument(context)\nexcept ModelError as e:\n    raise AgentError(f\"Failed to generate argument: {e}\") from e\n</code></pre>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"contributing/#new-llm-provider","title":"New LLM Provider","text":"<ol> <li>Create <code>artemis/models/{provider}.py</code></li> <li>Implement <code>BaseModel</code> interface</li> <li>Add to <code>artemis/models/__init__.py</code></li> <li>Add tests in <code>tests/unit/models/test_{provider}.py</code></li> <li>Update documentation</li> </ol> <pre><code># artemis/models/newprovider.py\nfrom artemis.models.base import BaseModel\n\nclass NewProviderModel(BaseModel):\n    async def generate(\n        self,\n        messages: list[Message],\n        **kwargs,\n    ) -&gt; Response:\n        ...\n\n    async def generate_with_reasoning(\n        self,\n        messages: list[Message],\n        thinking_budget: int = 8000,\n    ) -&gt; ReasoningResponse:\n        ...\n</code></pre>"},{"location":"contributing/#new-safety-monitor","title":"New Safety Monitor","text":"<ol> <li>Create <code>artemis/safety/{monitor}.py</code></li> <li>Implement <code>SafetyMonitor</code> interface</li> <li>Add tests with mock scenarios</li> <li>Document detection methodology</li> </ol> <pre><code># artemis/safety/newmonitor.py\nfrom artemis.safety.base import SafetyMonitor, SafetyResult\n\nclass NewMonitor(SafetyMonitor):\n    async def analyze(\n        self,\n        turn: Turn,\n        context: DebateContext,\n    ) -&gt; SafetyResult:\n        ...\n</code></pre>"},{"location":"contributing/#new-framework-integration","title":"New Framework Integration","text":"<ol> <li>Create <code>artemis/integrations/{framework}.py</code></li> <li>Follow framework's extension patterns</li> <li>Add example in <code>examples/</code></li> <li>Add integration tests</li> </ol>"},{"location":"contributing/#commit-guidelines","title":"Commit Guidelines","text":""},{"location":"contributing/#format","title":"Format","text":"<pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;\n\n[optional body]\n\n[optional footer]\n</code></pre>"},{"location":"contributing/#types","title":"Types","text":"<ul> <li><code>feat</code>: New feature</li> <li><code>fix</code>: Bug fix</li> <li><code>docs</code>: Documentation</li> <li><code>refactor</code>: Code refactoring</li> <li><code>test</code>: Adding tests</li> <li><code>chore</code>: Maintenance</li> </ul>"},{"location":"contributing/#examples","title":"Examples","text":"<pre><code>feat(core): implement H-L-DAG argument generation\n\nAdd hierarchical argument generation with three levels:\n- Strategic: high-level thesis\n- Tactical: supporting points\n- Operational: specific evidence\n</code></pre> <pre><code>fix(models): handle o1 reasoning token limits\n\nThe o1 model has specific token limits for reasoning.\nThis fix ensures we properly manage the thinking budget.\n\nFixes #123\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Update tests: Add tests for new features</li> <li>Update docs: Document new functionality</li> <li>Run checks: Ensure all tests pass</li> <li>Create PR: Use the PR template</li> <li>Address feedback: Respond to review comments</li> </ol>"},{"location":"contributing/#pr-template","title":"PR Template","text":"<pre><code>## Description\nBrief description of changes.\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\n- [ ] Tests added\n- [ ] All tests pass\n- [ ] Manual testing done\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Self-review completed\n- [ ] Documentation updated\n- [ ] No new warnings\n</code></pre>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#unit-tests","title":"Unit Tests","text":"<p>Test individual components in isolation:</p> <pre><code>@pytest.fixture\ndef mock_model():\n    model = AsyncMock(spec=BaseModel)\n    model.generate.return_value = Response(\n        content=\"Test argument\",\n        usage=Usage(prompt_tokens=100, completion_tokens=50),\n    )\n    return model\n\nasync def test_agent_generates_argument(mock_model):\n    agent = Agent(name=\"test\", role=\"Test agent\", model=mock_model)\n    argument = await agent.generate_argument(context)\n    assert argument.content == \"Test argument\"\n</code></pre>"},{"location":"contributing/#integration-tests","title":"Integration Tests","text":"<p>Test component interactions:</p> <pre><code>async def test_debate_runs_complete():\n    debate = Debate(\n        topic=\"Test topic\",\n        agents=agents,\n        rounds=2,\n    )\n    result = await debate.run()\n    assert result.verdict is not None\n    assert len(result.transcript) == 4  # 2 rounds * 2 agents\n</code></pre>"},{"location":"contributing/#safety-tests","title":"Safety Tests","text":"<p>Verify monitor detection:</p> <pre><code>from artemis.safety import SandbagDetector, MonitorMode\n\nasync def test_detects_sandbagging():\n    detector = SandbagDetector(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.8,\n    )\n\n    # Create turns with declining capability\n    turns = create_declining_capability_turns()\n\n    results = []\n    for turn in turns:\n        result = await detector.process(turn, context)\n        if result:\n            results.append(result)\n\n    assert any(r.severity &gt; 0.7 for r in results)\n</code></pre>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Open a GitHub Discussion</li> <li>Bugs: Open a GitHub Issue</li> <li>Features: Open a GitHub Issue with <code>[Feature Request]</code> prefix</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and inclusive in all interactions. We follow the Contributor Covenant.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the project's MIT License.</p>"},{"location":"api/core/","title":"Core API Reference","text":"<p>This page documents the core ARTEMIS classes and functions.</p>"},{"location":"api/core/#debate","title":"Debate","text":"<p>The main orchestrator class for running debates.</p> <pre><code>from artemis.core.debate import Debate\n</code></pre>"},{"location":"api/core/#constructor","title":"Constructor","text":"<pre><code>Debate(\n    topic: str,\n    agents: list[Agent],\n    rounds: int = 5,\n    config: DebateConfig | None = None,\n    jury: JuryPanel | None = None,\n    evaluator: AdaptiveEvaluator | None = None,\n    safety_monitors: list[Callable] | None = None,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Required Description <code>topic</code> str Yes The debate topic <code>agents</code> list[Agent] Yes List of participating agents (2+) <code>rounds</code> int No Number of debate rounds (default: 5) <code>config</code> DebateConfig No Debate configuration <code>jury</code> JuryPanel No Custom jury panel <code>evaluator</code> AdaptiveEvaluator No Custom argument evaluator <code>safety_monitors</code> list No List of safety monitor process methods"},{"location":"api/core/#methods","title":"Methods","text":""},{"location":"api/core/#run","title":"run","text":"<pre><code>async def run(self) -&gt; DebateResult\n</code></pre> <p>Runs the complete debate and returns results.</p> <p>Returns: <code>DebateResult</code> with verdict, transcript, and safety alerts.</p> <p>Example:</p> <pre><code>debate = Debate(topic=\"Your topic\", agents=agents)\nresult = await debate.run()\nprint(result.verdict.decision)\n</code></pre>"},{"location":"api/core/#assign_positions","title":"assign_positions","text":"<pre><code>def assign_positions(self, positions: dict[str, str]) -&gt; None\n</code></pre> <p>Assigns positions to agents.</p> <p>Parameters:</p> Parameter Type Description <code>positions</code> dict[str, str] Mapping of agent name to position <p>Example:</p> <pre><code>debate.assign_positions({\n    \"pro_agent\": \"supports the proposition\",\n    \"con_agent\": \"opposes the proposition\",\n})\n</code></pre>"},{"location":"api/core/#add_round","title":"add_round","text":"<pre><code>async def add_round(self) -&gt; RoundResult\n</code></pre> <p>Executes a single debate round.</p> <p>Returns: <code>RoundResult</code> with turns from each agent.</p>"},{"location":"api/core/#get_transcript","title":"get_transcript","text":"<pre><code>def get_transcript(self) -&gt; list[Turn]\n</code></pre> <p>Returns the current debate transcript.</p>"},{"location":"api/core/#agent","title":"Agent","text":"<p>Represents a debate participant.</p> <pre><code>from artemis.core.agent import Agent\n</code></pre>"},{"location":"api/core/#constructor_1","title":"Constructor","text":"<pre><code>Agent(\n    name: str,\n    role: str,\n    model: str = \"gpt-4o\",\n    reasoning_config: ReasoningConfig | None = None,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Required Description <code>name</code> str Yes Unique agent identifier <code>role</code> str Yes Agent's role description <code>model</code> str No LLM model to use (default: \"gpt-4o\") <code>reasoning_config</code> ReasoningConfig No Configuration for reasoning models"},{"location":"api/core/#methods_1","title":"Methods","text":""},{"location":"api/core/#generate_argument","title":"generate_argument","text":"<pre><code>async def generate_argument(\n    self,\n    context: DebateContext,\n    round_type: str = \"argument\",\n) -&gt; Argument\n</code></pre> <p>Generates an argument for the current context.</p> <p>Returns: <code>Argument</code> with content, level, and evidence.</p>"},{"location":"api/core/#generate_rebuttal","title":"generate_rebuttal","text":"<pre><code>async def generate_rebuttal(\n    self,\n    opponent_argument: Argument,\n    context: DebateContext,\n) -&gt; Argument\n</code></pre> <p>Generates a rebuttal to an opponent's argument.</p>"},{"location":"api/core/#argument","title":"Argument","text":"<p>Structured argument data.</p> <pre><code>from artemis.core.types import Argument, ArgumentLevel\n</code></pre>"},{"location":"api/core/#class-definition","title":"Class Definition","text":"<pre><code>class Argument(BaseModel):\n    id: str  # Auto-generated UUID\n    agent: str  # Required: agent name\n    level: ArgumentLevel\n    content: str\n    evidence: list[Evidence] = []\n    causal_links: list[CausalLink] = []\n    rebuts: str | None = None  # ID of argument this rebuts\n    supports: str | None = None  # ID of argument this supports\n    ethical_score: float | None = None\n    thinking_trace: str | None = None  # For reasoning models\n    timestamp: datetime\n</code></pre>"},{"location":"api/core/#argumentlevel","title":"ArgumentLevel","text":"<pre><code>class ArgumentLevel(str, Enum):\n    STRATEGIC = \"strategic\"\n    TACTICAL = \"tactical\"\n    OPERATIONAL = \"operational\"\n</code></pre>"},{"location":"api/core/#evaluationmode","title":"EvaluationMode","text":"<pre><code>class EvaluationMode(str, Enum):\n    QUALITY = \"quality\"    # LLM-native evaluation for maximum accuracy\n    BALANCED = \"balanced\"  # Selective LLM use (default)\n    FAST = \"fast\"          # Heuristic-only for minimum cost\n</code></pre> Mode Description <code>QUALITY</code> Uses LLM to evaluate all criteria. Highest accuracy, highest cost. <code>BALANCED</code> Selective LLM use for jury and key decisions. Good accuracy, moderate cost. <code>FAST</code> Heuristic-only evaluation. Lowest cost, backwards compatible."},{"location":"api/core/#evidence","title":"Evidence","text":"<pre><code>class Evidence(BaseModel):\n    id: str  # Auto-generated UUID\n    type: Literal[\"fact\", \"statistic\", \"quote\", \"example\", \"study\", \"expert_opinion\"]\n    content: str  # The evidence content\n    source: str | None = None\n    url: str | None = None\n    confidence: float  # 0.0 to 1.0\n    verified: bool = False\n</code></pre>"},{"location":"api/core/#causallink","title":"CausalLink","text":"<pre><code>class CausalLink(BaseModel):\n    id: str\n    cause: str\n    effect: str\n    mechanism: str | None = None\n    strength: float = 0.5  # 0.0 to 1.0\n    bidirectional: bool = False\n</code></pre>"},{"location":"api/core/#jurypanel","title":"JuryPanel","text":"<p>Multi-perspective evaluation jury.</p> <pre><code>from artemis.core.jury import JuryPanel\nfrom artemis.core.types import JuryPerspective\n</code></pre>"},{"location":"api/core/#constructor_2","title":"Constructor","text":"<pre><code>JuryPanel(\n    evaluators: int = 3,\n    criteria: list[str] | None = None,\n    model: str = \"gpt-4o\",\n    consensus_threshold: float = 0.7,\n    api_key: str | None = None,\n    **model_kwargs,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>evaluators</code> int 3 Number of jury evaluators <code>criteria</code> list[str] None Custom evaluation criteria <code>model</code> str \"gpt-4o\" LLM model for evaluators <code>consensus_threshold</code> float 0.7 Threshold for consensus <code>api_key</code> str None API key (uses env var if not provided)"},{"location":"api/core/#juryperspective","title":"JuryPerspective","text":"<pre><code>class JuryPerspective(str, Enum):\n    ANALYTICAL = \"analytical\"    # Focus on logic and evidence\n    ETHICAL = \"ethical\"          # Focus on moral implications\n    PRACTICAL = \"practical\"      # Focus on feasibility\n    ADVERSARIAL = \"adversarial\"  # Challenge all arguments\n    SYNTHESIZING = \"synthesizing\" # Find common ground\n</code></pre>"},{"location":"api/core/#methods_2","title":"Methods","text":""},{"location":"api/core/#deliberate","title":"deliberate","text":"<pre><code>async def deliberate(self, transcript: list[Turn]) -&gt; Verdict\n</code></pre> <p>Conducts jury deliberation and returns verdict.</p>"},{"location":"api/core/#verdict","title":"Verdict","text":"<p>Final debate verdict.</p> <pre><code>from artemis.core.types import Verdict\n</code></pre>"},{"location":"api/core/#class-definition_1","title":"Class Definition","text":"<pre><code>class Verdict(BaseModel):\n    decision: str  # \"pro\", \"con\", or \"tie\"\n    confidence: float  # 0.0 to 1.0\n    reasoning: str\n    votes: list[Vote] = []\n    deliberation_history: list[DeliberationRound] = []\n</code></pre>"},{"location":"api/core/#debateconfig","title":"DebateConfig","text":"<p>Debate configuration options.</p> <pre><code>from artemis.core.types import DebateConfig\n</code></pre>"},{"location":"api/core/#class-definition_2","title":"Class Definition","text":"<pre><code>class DebateConfig(BaseModel):\n    # Timing\n    turn_timeout: int = 60           # Timeout per turn (seconds)\n    round_timeout: int = 300         # Timeout per round (seconds)\n\n    # Argument generation\n    max_argument_tokens: int = 1000\n    require_evidence: bool = True\n    require_causal_links: bool = True\n    min_evidence_per_argument: int = 0\n\n    # Evaluation\n    evaluation_criteria: EvaluationCriteria = EvaluationCriteria()\n    adaptation_enabled: bool = True\n    adaptation_rate: float = 0.1\n\n    # Safety\n    safety_mode: str = \"passive\"     # \"off\", \"passive\", \"active\"\n    halt_on_safety_violation: bool = False\n\n    # Logging\n    log_level: str = \"INFO\"\n    trace_enabled: bool = False\n</code></pre>"},{"location":"api/core/#debateresult","title":"DebateResult","text":"<p>Complete debate result.</p> <pre><code>from artemis.core.types import DebateResult\n</code></pre>"},{"location":"api/core/#class-definition_3","title":"Class Definition","text":"<pre><code>class DebateResult(BaseModel):\n    debate_id: str\n    topic: str\n    verdict: Verdict\n    transcript: list[Turn]\n    safety_alerts: list[SafetyAlert] = []\n    metadata: DebateMetadata\n    final_state: DebateState = DebateState.COMPLETE\n</code></pre>"},{"location":"api/core/#debatemetadata","title":"DebateMetadata","text":"<pre><code>class DebateMetadata(BaseModel):\n    started_at: datetime\n    ended_at: datetime | None\n    total_rounds: int\n    total_turns: int\n    agents: list[str]\n    jury_size: int\n    safety_monitors: list[str]\n    model_usage: dict[str, dict[str, int]]\n</code></pre>"},{"location":"api/core/#turn","title":"Turn","text":"<p>A single turn in the debate.</p> <pre><code>from artemis.core.types import Turn\n</code></pre>"},{"location":"api/core/#class-definition_4","title":"Class Definition","text":"<pre><code>class Turn(BaseModel):\n    round: int\n    agent: str\n    argument: Argument\n    timestamp: datetime\n    evaluation: Evaluation | None = None\n</code></pre>"},{"location":"api/core/#adaptiveevaluator","title":"AdaptiveEvaluator","text":"<p>L-AE-CR adaptive evaluation.</p> <pre><code>from artemis.core.evaluation import AdaptiveEvaluator\n</code></pre>"},{"location":"api/core/#constructor_3","title":"Constructor","text":"<pre><code>AdaptiveEvaluator(\n    domain: str | None = None,\n    enable_causal_analysis: bool = True,\n    criteria_weights: dict[str, float] | None = None,\n)\n</code></pre>"},{"location":"api/core/#methods_3","title":"Methods","text":""},{"location":"api/core/#evaluate","title":"evaluate","text":"<pre><code>async def evaluate(\n    self,\n    argument: Argument,\n    context: DebateContext,\n    include_feedback: bool = False,\n) -&gt; Evaluation\n</code></pre> <p>Evaluates an argument with adaptive criteria.</p>"},{"location":"api/core/#compare","title":"compare","text":"<pre><code>async def compare(\n    self,\n    argument_a: Argument,\n    argument_b: Argument,\n    context: DebateContext,\n) -&gt; Comparison\n</code></pre> <p>Compares two arguments and determines winner.</p>"},{"location":"api/core/#llm-evaluation","title":"LLM Evaluation","text":"<p>LLM-based evaluation for maximum accuracy.</p> <pre><code>from artemis.core.llm_evaluation import LLMCriterionEvaluator, EvaluatorFactory\n</code></pre>"},{"location":"api/core/#llmcriterionevaluator","title":"LLMCriterionEvaluator","text":"<pre><code>class LLMCriterionEvaluator:\n    def __init__(\n        self,\n        model: str | BaseModel = \"gpt-4o-mini\",\n        api_key: str | None = None,\n        cache_enabled: bool = True,\n        **model_kwargs,\n    ) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model</code> str | BaseModel \"gpt-4o-mini\" Model name or instance <code>api_key</code> str None API key (uses env var if not provided) <code>cache_enabled</code> bool True Cache evaluations by content hash"},{"location":"api/core/#evaluate_argument","title":"evaluate_argument","text":"<pre><code>async def evaluate_argument(\n    self,\n    argument: Argument,\n    context: DebateContext,\n    weights: dict[str, float] | None = None,\n) -&gt; ArgumentEvaluation\n</code></pre> <p>Evaluates an argument using LLM judgment.</p> <p>Returns: <code>ArgumentEvaluation</code> with scores, weights, and reasoning.</p>"},{"location":"api/core/#evaluatorfactory","title":"EvaluatorFactory","text":"<p>Factory for creating evaluators based on evaluation mode.</p> <pre><code>class EvaluatorFactory:\n    @staticmethod\n    def create(\n        mode: EvaluationMode,\n        model: str = \"gpt-4o-mini\",\n        api_key: str | None = None,\n        **kwargs,\n    ) -&gt; LLMCriterionEvaluator | AdaptiveEvaluator\n</code></pre> Mode Returns <code>QUALITY</code> <code>LLMCriterionEvaluator</code> <code>BALANCED</code> <code>AdaptiveEvaluator</code> <code>FAST</code> <code>AdaptiveEvaluator</code>"},{"location":"api/core/#llm-extraction","title":"LLM Extraction","text":"<p>LLM-based extraction for evidence and causal links.</p> <pre><code>from artemis.core.llm_extraction import (\n    LLMCausalExtractor,\n    LLMEvidenceExtractor,\n    HybridCausalExtractor,\n    HybridEvidenceExtractor,\n    clear_extraction_cache,\n)\n</code></pre>"},{"location":"api/core/#llmcausalextractor","title":"LLMCausalExtractor","text":"<p>Extracts causal relationships using LLM analysis.</p> <pre><code>class LLMCausalExtractor:\n    def __init__(\n        self,\n        model: BaseModel | None = None,\n        model_name: str = \"gpt-4o-mini\",\n        use_cache: bool = True,\n    )\n</code></pre>"},{"location":"api/core/#extract","title":"extract","text":"<pre><code>async def extract(self, content: str) -&gt; list[CausalLink]\n</code></pre> <p>Extracts causal links from content.</p>"},{"location":"api/core/#llmevidenceextractor","title":"LLMEvidenceExtractor","text":"<p>Extracts evidence using LLM analysis.</p> <pre><code>class LLMEvidenceExtractor:\n    def __init__(\n        self,\n        model: BaseModel | None = None,\n        model_name: str = \"gpt-4o-mini\",\n        use_cache: bool = True,\n    )\n</code></pre>"},{"location":"api/core/#extract_1","title":"extract","text":"<pre><code>async def extract(self, content: str) -&gt; list[Evidence]\n</code></pre> <p>Extracts evidence from content.</p>"},{"location":"api/core/#hybrid-extractors","title":"Hybrid Extractors","text":"<p>Try regex first, fall back to LLM:</p> <pre><code>class HybridCausalExtractor:\n    async def extract(self, content: str) -&gt; list[CausalLink]\n\nclass HybridEvidenceExtractor:\n    async def extract(self, content: str) -&gt; list[Evidence]\n</code></pre>"},{"location":"api/core/#clear_extraction_cache","title":"clear_extraction_cache","text":"<pre><code>def clear_extraction_cache() -&gt; None\n</code></pre> <p>Clears the global extraction cache.</p>"},{"location":"api/core/#exceptions","title":"Exceptions","text":"<pre><code>from artemis.exceptions import (\n    ArtemisError,\n    DebateError,\n    AgentError,\n    EvaluationError,\n    SafetyError,\n    EthicsViolationError,\n)\n</code></pre>"},{"location":"api/core/#artemiserror","title":"ArtemisError","text":"<p>Base exception for all ARTEMIS errors.</p>"},{"location":"api/core/#debateerror","title":"DebateError","text":"<p>Raised when debate execution fails.</p>"},{"location":"api/core/#agenterror","title":"AgentError","text":"<p>Raised when agent generation fails.</p>"},{"location":"api/core/#safetyerror","title":"SafetyError","text":"<p>Raised when safety violation detected.</p>"},{"location":"api/core/#ethicsviolationerror","title":"EthicsViolationError","text":"<p>Raised when ethics guard blocks content.</p>"},{"location":"api/core/#next-steps","title":"Next Steps","text":"<ul> <li>Models API</li> <li>Safety API</li> <li>Integrations API</li> </ul>"},{"location":"api/integrations/","title":"Integrations API Reference","text":"<p>This page documents the ARTEMIS framework integrations.</p>"},{"location":"api/integrations/#langchain-integration","title":"LangChain Integration","text":""},{"location":"api/integrations/#artemisdebatetool","title":"ArtemisDebateTool","text":"<pre><code>from artemis.integrations import ArtemisDebateTool\n</code></pre>"},{"location":"api/integrations/#constructor","title":"Constructor","text":"<pre><code>ArtemisDebateTool(\n    model: str,\n    default_rounds: int = 3,\n    config: DebateConfig | None = None,\n    safety_monitors: list[SafetyMonitor] | None = None,\n    name: str = \"artemis_debate\",\n    description: str = \"...\",\n    args_schema: Type[BaseModel] | None = None,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model</code> str required LLM model to use <code>default_rounds</code> int 3 Default debate rounds <code>config</code> DebateConfig None Debate configuration <code>safety_monitors</code> list None Safety monitors <code>name</code> str \"artemis_debate\" Tool name <code>description</code> str ... Tool description <code>args_schema</code> Type None Custom input schema"},{"location":"api/integrations/#methods","title":"Methods","text":""},{"location":"api/integrations/#invoke","title":"invoke","text":"<pre><code>def invoke(\n    self,\n    input: dict,\n    config: RunnableConfig | None = None,\n) -&gt; DebateResult\n</code></pre> <p>Runs a synchronous debate.</p>"},{"location":"api/integrations/#ainvoke","title":"ainvoke","text":"<pre><code>async def ainvoke(\n    self,\n    input: dict,\n    config: RunnableConfig | None = None,\n) -&gt; DebateResult\n</code></pre> <p>Runs an asynchronous debate.</p>"},{"location":"api/integrations/#input-schema","title":"Input Schema","text":"<pre><code>{\n    \"topic\": str,                  # Required: debate topic\n    \"rounds\": int,                 # Optional: number of rounds\n    \"agents\": list[AgentConfig],   # Optional: agent configurations\n    \"pro_position\": str,           # Optional: pro-side position (simple mode)\n    \"con_position\": str,           # Optional: con-side position (simple mode)\n}\n</code></pre>"},{"location":"api/integrations/#langgraph-integration","title":"LangGraph Integration","text":""},{"location":"api/integrations/#artemisdebatenode","title":"ArtemisDebateNode","text":"<pre><code>from artemis.integrations import ArtemisDebateNode\n</code></pre>"},{"location":"api/integrations/#constructor_1","title":"Constructor","text":"<pre><code>ArtemisDebateNode(\n    model: str = \"gpt-4o\",\n    agents: list[Agent] | None = None,\n    config: DebateNodeConfig | None = None,\n    debate_config: DebateConfig | None = None,\n)\n</code></pre>"},{"location":"api/integrations/#usage","title":"Usage","text":"<pre><code>from langgraph.graph import StateGraph\nfrom artemis.integrations import ArtemisDebateNode, DebateNodeState\n\nworkflow = StateGraph(DebateNodeState)\nworkflow.add_node(\"debate\", ArtemisDebateNode(model=\"gpt-4o\").run_debate)\n</code></pre>"},{"location":"api/integrations/#debatenodestate","title":"DebateNodeState","text":"<pre><code>from artemis.integrations import DebateNodeState\n</code></pre>"},{"location":"api/integrations/#class-definition","title":"Class Definition","text":"<pre><code>class DebateNodeState(TypedDict, total=False):\n    topic: str\n    agents: list[AgentStateConfig]\n    positions: dict[str, str]\n    rounds: int\n    current_round: int\n    phase: str\n    transcript: list[dict]\n    verdict: dict | None\n    scores: dict[str, float]\n    metadata: dict\n</code></pre>"},{"location":"api/integrations/#create_debate_workflow","title":"create_debate_workflow","text":"<pre><code>from artemis.integrations import create_debate_workflow\n</code></pre>"},{"location":"api/integrations/#signature","title":"Signature","text":"<pre><code>def create_debate_workflow(\n    model: str = \"gpt-4o\",\n    step_by_step: bool = False,\n    agents: list[Agent] | None = None,\n) -&gt; CompiledStateGraph\n</code></pre> <p>Creates a complete debate workflow.</p> <p>Returns: Compiled LangGraph workflow.</p>"},{"location":"api/integrations/#crewai-integration","title":"CrewAI Integration","text":""},{"location":"api/integrations/#artemiscrewtool","title":"ArtemisCrewTool","text":"<pre><code>from artemis.integrations import ArtemisCrewTool\n</code></pre>"},{"location":"api/integrations/#constructor_2","title":"Constructor","text":"<pre><code>ArtemisCrewTool(\n    model: str = \"gpt-4o\",\n    default_rounds: int = 3,\n    agents: list[Agent] | None = None,\n    config: DebateConfig | None = None,\n    verbose: bool = False,\n)\n</code></pre>"},{"location":"api/integrations/#methods_1","title":"Methods","text":""},{"location":"api/integrations/#run","title":"run","text":"<pre><code>def run(\n    self,\n    topic: str,\n    agents: list[dict] | None = None,\n    pro_position: str | None = None,\n    con_position: str | None = None,\n    rounds: int | None = None,\n) -&gt; str\n</code></pre> <p>Runs a synchronous debate and returns formatted string.</p>"},{"location":"api/integrations/#arun","title":"arun","text":"<pre><code>async def arun(\n    self,\n    topic: str,\n    agents: list[dict] | None = None,\n    pro_position: str | None = None,\n    con_position: str | None = None,\n    rounds: int | None = None,\n) -&gt; str\n</code></pre> <p>Runs an asynchronous debate.</p>"},{"location":"api/integrations/#output-schema","title":"Output Schema","text":"<pre><code>{\n    \"verdict\": str,              # \"pro\", \"con\", or \"tie\"\n    \"confidence\": float,         # 0.0 to 1.0\n    \"reasoning\": str,            # Verdict explanation\n    \"transcript\": list[dict],    # Debate transcript\n    \"safety_alerts\": list[dict], # Safety alerts\n    \"metadata\": dict,            # Additional metadata\n}\n</code></pre>"},{"location":"api/integrations/#mcp-integration","title":"MCP Integration","text":""},{"location":"api/integrations/#artemismcpserver","title":"ArtemisMCPServer","text":"<pre><code>from artemis.mcp import ArtemisMCPServer\n</code></pre>"},{"location":"api/integrations/#constructor_3","title":"Constructor","text":"<pre><code>ArtemisMCPServer(\n    default_model: str = \"gpt-4o\",\n    max_sessions: int = 100,\n    config: DebateConfig | None = None,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>default_model</code> str \"gpt-4o\" Default LLM model <code>max_sessions</code> int 100 Max concurrent sessions <code>config</code> DebateConfig None Debate configuration"},{"location":"api/integrations/#methods_2","title":"Methods","text":""},{"location":"api/integrations/#run_stdio","title":"run_stdio","text":"<pre><code>async def run_stdio(self) -&gt; None\n</code></pre> <p>Runs server in stdio mode for MCP clients.</p>"},{"location":"api/integrations/#start","title":"start","text":"<pre><code>async def start(\n    self,\n    host: str = \"127.0.0.1\",\n    port: int = 8080,\n) -&gt; None\n</code></pre> <p>Starts HTTP server.</p>"},{"location":"api/integrations/#handle_tool_call","title":"handle_tool_call","text":"<pre><code>async def handle_tool_call(\n    self,\n    tool_name: str,\n    arguments: dict,\n) -&gt; dict\n</code></pre> <p>Handles a tool call directly.</p>"},{"location":"api/integrations/#available-tools","title":"Available Tools","text":"Tool Description <code>artemis_debate_start</code> Start a new debate <code>artemis_add_round</code> Add a debate round <code>artemis_get_verdict</code> Get jury verdict <code>artemis_get_transcript</code> Get full transcript <code>artemis_list_debates</code> List active debates <code>artemis_get_safety_report</code> Get safety report"},{"location":"api/integrations/#cli","title":"CLI","text":"<pre><code># Basic usage\nartemis-mcp\n\n# HTTP mode\nartemis-mcp --http --port 8080\n\n# With options\nartemis-mcp --model gpt-4-turbo --max-sessions 50 --verbose\n</code></pre>"},{"location":"api/integrations/#common-types","title":"Common Types","text":""},{"location":"api/integrations/#toolresult","title":"ToolResult","text":"<p>Base result from integration tools.</p> <pre><code>class ToolResult(BaseModel):\n    verdict: Verdict\n    transcript: list[Turn]\n    safety_alerts: list[SafetyAlert]\n    metadata: dict\n</code></pre>"},{"location":"api/integrations/#toolconfig","title":"ToolConfig","text":"<p>Configuration for integration tools.</p> <pre><code>class ToolConfig(BaseModel):\n    model: str\n    rounds: int = 3\n    config: DebateConfig | None = None\n    safety_enabled: bool = True\n</code></pre>"},{"location":"api/integrations/#error-handling","title":"Error Handling","text":"<p>All integrations raise standard exceptions:</p> <pre><code>from artemis.exceptions import (\n    ArtemisError,      # Base exception\n    DebateError,       # Debate execution failed\n    IntegrationError,  # Integration-specific error\n)\n\ntry:\n    result = tool.invoke({\"topic\": \"Your topic\"})\nexcept DebateError as e:\n    print(f\"Debate failed: {e}\")\nexcept IntegrationError as e:\n    print(f\"Integration error: {e}\")\n</code></pre>"},{"location":"api/integrations/#next-steps","title":"Next Steps","text":"<ul> <li>Core API</li> <li>Models API</li> <li>Safety API</li> </ul>"},{"location":"api/models/","title":"Models API Reference","text":"<p>This page documents the ARTEMIS model provider interfaces.</p>"},{"location":"api/models/#create_model","title":"create_model","text":"<p>Factory function for creating model instances.</p> <pre><code>from artemis.models import create_model\n</code></pre>"},{"location":"api/models/#signature","title":"Signature","text":"<pre><code>def create_model(\n    model: str,\n    provider: str | None = None,\n    **kwargs,\n) -&gt; BaseModel\n</code></pre> <p>Parameters:</p> Parameter Type Required Description <code>model</code> str Yes Model identifier (e.g., \"gpt-4o\", \"deepseek-reasoner\") <code>provider</code> str No Provider name. If not specified, inferred from model name <code>**kwargs</code> - No Additional arguments passed to the model constructor <p>Example:</p> <pre><code># OpenAI (provider inferred from model name)\nmodel = create_model(\"gpt-4o\")\n\n# DeepSeek with explicit provider\nmodel = create_model(\"deepseek-reasoner\", provider=\"deepseek\")\n\n# With additional options\nmodel = create_model(\"gpt-4o\", timeout=120.0, max_retries=5)\n</code></pre>"},{"location":"api/models/#basemodel","title":"BaseModel","text":"<p>Abstract base class for all model providers.</p> <pre><code>from artemis.models.base import BaseModel\n</code></pre>"},{"location":"api/models/#abstract-methods","title":"Abstract Methods","text":""},{"location":"api/models/#generate","title":"generate","text":"<pre><code>async def generate(\n    self,\n    messages: list[Message],\n    **kwargs,\n) -&gt; Response\n</code></pre> <p>Generates a response from the model.</p> <p>Parameters:</p> Parameter Type Description <code>messages</code> list[Message] Conversation messages <code>**kwargs</code> - Additional options <p>Returns: <code>Response</code> object.</p>"},{"location":"api/models/#generate_with_reasoning","title":"generate_with_reasoning","text":"<pre><code>async def generate_with_reasoning(\n    self,\n    messages: list[Message],\n    thinking_budget: int = 8000,\n    **kwargs,\n) -&gt; ReasoningResponse\n</code></pre> <p>Generates a response with extended thinking.</p> <p>Returns: <code>ReasoningResponse</code> with thinking and output.</p>"},{"location":"api/models/#openaimodel","title":"OpenAIModel","text":"<p>OpenAI model provider.</p> <pre><code>from artemis.models.openai import OpenAIModel\n</code></pre>"},{"location":"api/models/#constructor","title":"Constructor","text":"<pre><code>OpenAIModel(\n    model: str = \"gpt-4o\",\n    api_key: str | None = None,\n    base_url: str | None = None,\n    organization: str | None = None,\n    timeout: float = 60.0,\n    max_retries: int = 3,\n    **kwargs,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model</code> str \"gpt-4o\" Model identifier <code>api_key</code> str | None None API key (reads from OPENAI_API_KEY env var if not provided) <code>base_url</code> str | None None Custom API base URL (for Azure or proxies) <code>organization</code> str | None None OpenAI organization ID <code>timeout</code> float 60.0 Request timeout in seconds <code>max_retries</code> int 3 Maximum retry attempts"},{"location":"api/models/#supported-models","title":"Supported Models","text":"Model Reasoning Description <code>gpt-4o</code> No GPT-4 Optimized <code>gpt-4-turbo</code> No GPT-4 Turbo <code>o1</code> Yes OpenAI Reasoning <code>o1-preview</code> Yes OpenAI Reasoning Preview <code>o1-mini</code> Yes OpenAI Reasoning Mini"},{"location":"api/models/#deepseekmodel","title":"DeepSeekModel","text":"<p>DeepSeek model provider with R1 reasoning support.</p> <pre><code>from artemis.models.deepseek import DeepSeekModel\n</code></pre>"},{"location":"api/models/#constructor_1","title":"Constructor","text":"<pre><code>DeepSeekModel(\n    model: str = \"deepseek-reasoner\",\n    api_key: str | None = None,\n    base_url: str | None = None,\n    timeout: float = 120.0,\n    max_retries: int = 3,\n    **kwargs,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model</code> str \"deepseek-reasoner\" Model identifier <code>api_key</code> str | None None API key (reads from DEEPSEEK_API_KEY env var if not provided) <code>base_url</code> str | None \"https://api.deepseek.com/v1\" Custom API base URL <code>timeout</code> float 120.0 Request timeout in seconds <code>max_retries</code> int 3 Maximum retry attempts"},{"location":"api/models/#supported-models_1","title":"Supported Models","text":"Model Reasoning Description <code>deepseek-chat</code> No Standard chat model <code>deepseek-coder</code> No Code-specialized model <code>deepseek-reasoner</code> Yes Full R1 with extended thinking <code>deepseek-r1-distill-llama-70b</code> Yes Distilled R1 variant"},{"location":"api/models/#googlemodel","title":"GoogleModel","text":"<p>Google/Gemini model provider with Vertex AI support.</p> <pre><code>from artemis.models import GoogleModel\n</code></pre>"},{"location":"api/models/#constructor_2","title":"Constructor","text":"<pre><code>GoogleModel(\n    model: str = \"gemini-2.0-flash\",\n    api_key: str | None = None,\n    project: str | None = None,\n    location: str = \"us-central1\",\n    use_vertex_ai: bool | None = None,\n    timeout: float = 120.0,\n    max_retries: int = 3,\n    **kwargs,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model</code> str \"gemini-2.0-flash\" Model identifier <code>api_key</code> str | None None API key for AI Studio (reads from GOOGLE_API_KEY or GEMINI_API_KEY) <code>project</code> str | None None GCP project ID for Vertex AI <code>location</code> str \"us-central1\" GCP region for Vertex AI <code>use_vertex_ai</code> bool | None None Force Vertex AI (auto-detected if project is set) <code>timeout</code> float 120.0 Request timeout in seconds <code>max_retries</code> int 3 Maximum retry attempts"},{"location":"api/models/#backend-selection","title":"Backend Selection","text":"<p>GoogleModel supports two backends:</p> Backend When Used Authentication AI Studio Default when no project set <code>GOOGLE_API_KEY</code> env var Vertex AI When <code>GOOGLE_CLOUD_PROJECT</code> is set Application Default Credentials <p>Example:</p> <pre><code># AI Studio (simple setup)\nmodel = GoogleModel(model=\"gemini-2.0-flash\")\n\n# Vertex AI (higher rate limits)\nmodel = GoogleModel(\n    model=\"gemini-2.0-flash\",\n    project=\"my-gcp-project\",\n    location=\"us-central1\",\n)\n</code></pre>"},{"location":"api/models/#supported-models_2","title":"Supported Models","text":"Model Reasoning Description <code>gemini-2.0-flash</code> No Fast, efficient model <code>gemini-2.0-flash-exp</code> No Experimental flash variant <code>gemini-1.5-pro</code> No High capability model <code>gemini-1.5-flash</code> No Fast 1.5 variant <code>gemini-2.5-pro</code> Yes Extended thinking support <code>gemini-2.5-flash</code> Yes Fast reasoning model"},{"location":"api/models/#anthropicmodel","title":"AnthropicModel","text":"<p>Anthropic/Claude model provider.</p> <pre><code>from artemis.models import AnthropicModel\n</code></pre>"},{"location":"api/models/#constructor_3","title":"Constructor","text":"<pre><code>AnthropicModel(\n    model: str = \"claude-sonnet-4-20250514\",\n    api_key: str | None = None,\n    timeout: float = 120.0,\n    max_retries: int = 3,\n    **kwargs,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model</code> str \"claude-sonnet-4-20250514\" Model identifier <code>api_key</code> str | None None API key (reads from ANTHROPIC_API_KEY) <code>timeout</code> float 120.0 Request timeout in seconds <code>max_retries</code> int 3 Maximum retry attempts <p>Example:</p> <pre><code>from artemis.models import AnthropicModel\n\nmodel = AnthropicModel(model=\"claude-sonnet-4-20250514\")\n</code></pre>"},{"location":"api/models/#supported-models_3","title":"Supported Models","text":"Model Reasoning Description <code>claude-sonnet-4-20250514</code> Yes Claude Sonnet 4 with extended thinking <code>claude-3-5-sonnet-20241022</code> Yes Claude 3.5 Sonnet <code>claude-3-opus-20240229</code> No Claude 3 Opus <code>claude-3-haiku-20240307</code> No Claude 3 Haiku (fast)"},{"location":"api/models/#reasoningconfig","title":"ReasoningConfig","text":"<p>Configuration for reasoning models.</p> <pre><code>from artemis.models import ReasoningConfig, ReasoningStrategy, create_reasoning_config\n</code></pre>"},{"location":"api/models/#class-definition","title":"Class Definition","text":"<pre><code>class ReasoningConfig(BaseModel):\n    model: str = \"o1\"\n    strategy: ReasoningStrategy = ReasoningStrategy.ADAPTIVE\n    thinking_budget: int = 8000\n    show_thinking: bool = False\n    thinking_style: str = \"thorough\"  # \"thorough\", \"concise\", \"analytical\"\n    temperature: float = 1.0\n    use_system_prompt: bool = True\n</code></pre>"},{"location":"api/models/#reasoningstrategy","title":"ReasoningStrategy","text":"<pre><code>class ReasoningStrategy(str, Enum):\n    ALWAYS = \"always\"     # Always use extended thinking\n    ADAPTIVE = \"adaptive\" # Use reasoning only for complex problems\n    NEVER = \"never\"       # Never use extended thinking\n</code></pre>"},{"location":"api/models/#create_reasoning_config","title":"create_reasoning_config","text":"<pre><code>def create_reasoning_config(\n    model: str,\n    **overrides,\n) -&gt; ReasoningConfig\n</code></pre> <p>Creates appropriate config with model-specific defaults.</p> <p>Example:</p> <pre><code># Automatic configuration for o1\nconfig = create_reasoning_config(\"o1\", thinking_budget=16000)\n\n# For DeepSeek R1\nconfig = create_reasoning_config(\"deepseek-reasoner\", show_thinking=True)\n</code></pre>"},{"location":"api/models/#message","title":"Message","text":"<p>Message structure for model calls.</p> <pre><code>from artemis.core.types import Message\n</code></pre>"},{"location":"api/models/#class-definition_1","title":"Class Definition","text":"<pre><code>class Message(BaseModel):\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n    name: str | None = None  # Optional name for multi-agent scenarios\n</code></pre>"},{"location":"api/models/#modelresponse","title":"ModelResponse","text":"<p>Model response structure.</p> <pre><code>from artemis.core.types import ModelResponse\n</code></pre>"},{"location":"api/models/#class-definition_2","title":"Class Definition","text":"<pre><code>class ModelResponse(BaseModel):\n    content: str\n    usage: Usage\n    model: str | None = None\n    finish_reason: str | None = None\n</code></pre>"},{"location":"api/models/#reasoningresponse","title":"ReasoningResponse","text":"<p>Response from reasoning models.</p> <pre><code>from artemis.core.types import ReasoningResponse\n</code></pre>"},{"location":"api/models/#class-definition_3","title":"Class Definition","text":"<pre><code>class ReasoningResponse(ModelResponse):\n    thinking: str | None = None  # The extended thinking/reasoning trace\n    thinking_tokens: int = 0\n</code></pre>"},{"location":"api/models/#usage","title":"Usage","text":"<p>Token usage information.</p> <pre><code>from artemis.core.types import Usage\n</code></pre>"},{"location":"api/models/#class-definition_4","title":"Class Definition","text":"<pre><code>class Usage(BaseModel):\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    total_tokens: int = 0\n    reasoning_tokens: int | None = None  # For reasoning models\n</code></pre>"},{"location":"api/models/#list_providers","title":"list_providers","text":"<p>List available model providers.</p> <pre><code>from artemis.models import list_providers\n\nproviders = list_providers()\n# ['openai', 'deepseek', 'google', 'anthropic']\n</code></pre>"},{"location":"api/models/#is_reasoning_model","title":"is_reasoning_model","text":"<p>Check if a model supports extended reasoning.</p> <pre><code>from artemis.models import is_reasoning_model\n\nis_reasoning_model(\"o1\")  # True\nis_reasoning_model(\"gpt-4o\")  # False\n</code></pre>"},{"location":"api/models/#next-steps","title":"Next Steps","text":"<ul> <li>Core API</li> <li>Safety API</li> <li>Integrations API</li> </ul>"},{"location":"api/safety/","title":"Safety API Reference","text":"<p>This page documents the ARTEMIS safety monitoring API.</p>"},{"location":"api/safety/#monitormode","title":"MonitorMode","text":"<p>Enum for safety monitor modes.</p> <pre><code>from artemis.safety import MonitorMode\n\nMonitorMode.PASSIVE   # Observe and report only\nMonitorMode.ACTIVE    # Can intervene and halt debate\nMonitorMode.LEARNING  # Learn patterns without alerting\n</code></pre>"},{"location":"api/safety/#sandbagdetector","title":"SandbagDetector","text":"<p>Detects intentional underperformance.</p> <pre><code>from artemis.safety import SandbagDetector, MonitorMode\n</code></pre>"},{"location":"api/safety/#constructor","title":"Constructor","text":"<pre><code>SandbagDetector(\n    mode: MonitorMode = MonitorMode.PASSIVE,\n    sensitivity: float = 0.5,\n    baseline_turns: int = 3,\n    drop_threshold: float = 0.3,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>mode</code> MonitorMode PASSIVE Monitor mode <code>sensitivity</code> float 0.5 Detection sensitivity (0-1) <code>baseline_turns</code> int 3 Turns for baseline establishment <code>drop_threshold</code> float 0.3 Capability drop threshold"},{"location":"api/safety/#methods","title":"Methods","text":""},{"location":"api/safety/#process","title":"process","text":"<pre><code>async def process(\n    self,\n    turn: Turn,\n    context: DebateContext,\n) -&gt; SafetyResult | None\n</code></pre> <p>Processes a turn for sandbagging detection. Used as <code>safety_monitors=[detector.process]</code>.</p>"},{"location":"api/safety/#deceptionmonitor","title":"DeceptionMonitor","text":"<p>Detects deceptive claims.</p> <pre><code>from artemis.safety import DeceptionMonitor, MonitorMode\n</code></pre>"},{"location":"api/safety/#constructor_1","title":"Constructor","text":"<pre><code>DeceptionMonitor(\n    mode: MonitorMode = MonitorMode.PASSIVE,\n    sensitivity: float = 0.5,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>mode</code> MonitorMode PASSIVE Monitor mode <code>sensitivity</code> float 0.5 Detection sensitivity (0-1)"},{"location":"api/safety/#methods_1","title":"Methods","text":""},{"location":"api/safety/#process_1","title":"process","text":"<pre><code>async def process(\n    self,\n    turn: Turn,\n    context: DebateContext,\n) -&gt; SafetyResult | None\n</code></pre> <p>Processes a turn for deception detection.</p>"},{"location":"api/safety/#behaviortracker","title":"BehaviorTracker","text":"<p>Tracks behavioral changes over time.</p> <pre><code>from artemis.safety import BehaviorTracker, MonitorMode\n</code></pre>"},{"location":"api/safety/#constructor_2","title":"Constructor","text":"<pre><code>BehaviorTracker(\n    mode: MonitorMode = MonitorMode.PASSIVE,\n    sensitivity: float = 0.5,\n    window_size: int = 5,\n    drift_threshold: float = 0.3,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>mode</code> MonitorMode PASSIVE Monitor mode <code>sensitivity</code> float 0.5 Detection sensitivity (0-1) <code>window_size</code> int 5 Turns to consider for drift <code>drift_threshold</code> float 0.3 Drift alert threshold"},{"location":"api/safety/#methods_2","title":"Methods","text":""},{"location":"api/safety/#process_2","title":"process","text":"<pre><code>async def process(\n    self,\n    turn: Turn,\n    context: DebateContext,\n) -&gt; SafetyResult | None\n</code></pre> <p>Processes a turn for behavior drift detection.</p>"},{"location":"api/safety/#ethicsguard","title":"EthicsGuard","text":"<p>Monitors ethical boundaries.</p> <pre><code>from artemis.safety import EthicsGuard, EthicsConfig, MonitorMode\n</code></pre>"},{"location":"api/safety/#constructor_3","title":"Constructor","text":"<pre><code>EthicsGuard(\n    mode: MonitorMode = MonitorMode.PASSIVE,\n    config: EthicsConfig | None = None,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>mode</code> MonitorMode PASSIVE Monitor mode <code>config</code> EthicsConfig None Ethics configuration"},{"location":"api/safety/#ethicsconfig","title":"EthicsConfig","text":"<pre><code>EthicsConfig(\n    harmful_content_threshold: float = 0.5,\n    bias_threshold: float = 0.4,\n    fairness_threshold: float = 0.3,\n    enabled_checks: list[str] = [\"harmful_content\", \"bias\", \"fairness\", \"privacy\", \"manipulation\"],\n    custom_boundaries: dict[str, str] = {},\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>harmful_content_threshold</code> float 0.5 Threshold for harmful content <code>bias_threshold</code> float 0.4 Threshold for bias detection <code>fairness_threshold</code> float 0.3 Threshold for fairness violations <code>enabled_checks</code> list[str] [...] List of enabled ethics checks <code>custom_boundaries</code> dict {} Custom ethical boundaries"},{"location":"api/safety/#methods_3","title":"Methods","text":""},{"location":"api/safety/#process_3","title":"process","text":"<pre><code>async def process(\n    self,\n    turn: Turn,\n    context: DebateContext,\n) -&gt; SafetyResult | None\n</code></pre> <p>Processes a turn for ethics violations.</p>"},{"location":"api/safety/#using-multiple-monitors","title":"Using Multiple Monitors","text":"<p>Combine monitors by passing a list to the Debate:</p> <pre><code>from artemis.core.debate import Debate\nfrom artemis.safety import (\n    SandbagDetector,\n    DeceptionMonitor,\n    BehaviorTracker,\n    EthicsGuard,\n    MonitorMode,\n    EthicsConfig,\n)\n\n# Create individual monitors\nsandbag = SandbagDetector(mode=MonitorMode.PASSIVE, sensitivity=0.7)\ndeception = DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6)\nbehavior = BehaviorTracker(mode=MonitorMode.PASSIVE, sensitivity=0.5)\nethics = EthicsGuard(mode=MonitorMode.PASSIVE, config=EthicsConfig(harmful_content_threshold=0.5))\n\n# Pass their process methods to the debate\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[\n        sandbag.process,\n        deception.process,\n        behavior.process,\n        ethics.process,\n    ],\n)\n</code></pre>"},{"location":"api/safety/#safetyresult","title":"SafetyResult","text":"<p>Result from a safety monitor.</p> <pre><code>from artemis.core.types import SafetyResult\n</code></pre>"},{"location":"api/safety/#class-definition","title":"Class Definition","text":"<pre><code>class SafetyResult(BaseModel):\n    monitor: str             # Name of the monitor\n    severity: float          # 0.0 to 1.0\n    indicators: list[SafetyIndicator]\n    should_alert: bool = False\n    should_halt: bool = False\n    analysis_notes: str | None = None\n</code></pre>"},{"location":"api/safety/#safetyalert","title":"SafetyAlert","text":"<p>Safety alert from monitoring.</p> <pre><code>from artemis.core.types import SafetyAlert\n</code></pre>"},{"location":"api/safety/#class-definition_1","title":"Class Definition","text":"<pre><code>class SafetyAlert(BaseModel):\n    id: str\n    monitor: str              # Name of the monitor\n    agent: str                # Agent that triggered alert\n    type: str                 # \"sandbagging\", \"deception\", \"drift\", \"ethics\"\n    severity: float           # 0.0 to 1.0\n    indicators: list[SafetyIndicator]\n    turn_id: str | None\n    timestamp: datetime\n    resolved: bool = False\n    resolution_notes: str | None = None\n</code></pre>"},{"location":"api/safety/#safetyindicator","title":"SafetyIndicator","text":"<p>Individual safety indicator.</p> <pre><code>from artemis.core.types import SafetyIndicator, SafetyIndicatorType\n</code></pre>"},{"location":"api/safety/#class-definition_2","title":"Class Definition","text":"<pre><code>class SafetyIndicator(BaseModel):\n    type: SafetyIndicatorType\n    severity: float\n    evidence: str | list[str]\n    metadata: dict = {}\n\nclass SafetyIndicatorType(str, Enum):\n    CAPABILITY_DROP = \"capability_drop\"\n    STRATEGIC_TIMING = \"strategic_timing\"\n    SELECTIVE_ENGAGEMENT = \"selective_engagement\"\n    FACTUAL_INCONSISTENCY = \"factual_inconsistency\"\n    LOGICAL_FALLACY = \"logical_fallacy\"\n    EMOTIONAL_MANIPULATION = \"emotional_manipulation\"\n    CITATION_FABRICATION = \"citation_fabrication\"\n    BEHAVIORAL_DRIFT = \"behavioral_drift\"\n    ETHICS_BOUNDARY = \"ethics_boundary\"\n</code></pre>"},{"location":"api/safety/#next-steps","title":"Next Steps","text":"<ul> <li>Core API</li> <li>Models API</li> <li>Integrations API</li> </ul>"},{"location":"concepts/ethics/","title":"Ethics Module","text":"<p>ARTEMIS includes an ethics module that ensures debates remain within ethical boundaries and arguments are evaluated for moral soundness.</p>"},{"location":"concepts/ethics/#overview","title":"Overview","text":"<p>The ethics module operates at three levels:</p> <ol> <li>Evaluation: Ethical criteria are weighted in argument scoring</li> <li>Monitoring: EthicsGuard detects ethical boundary violations</li> <li>Jury Perspective: Ethical jury perspective considers moral implications</li> </ol>"},{"location":"concepts/ethics/#ethical-principles","title":"Ethical Principles","text":"<p>ARTEMIS is built on core ethical principles:</p> Principle Description Fairness Arguments shouldn't discriminate or show bias Transparency Reasoning should be clear and honest Non-harm Arguments shouldn't advocate for harmful actions Respect Maintain respect for persons and values Accuracy Claims should be truthful and verifiable"},{"location":"concepts/ethics/#ethics-guard","title":"Ethics Guard","text":"<p>The <code>EthicsGuard</code> monitors arguments for ethical violations:</p> <pre><code>from artemis.safety import EthicsGuard, MonitorMode, EthicsConfig\n\n# Configure ethics guard\nethics_config = EthicsConfig(\n    harmful_content_threshold=0.3,\n    bias_threshold=0.4,\n    fairness_threshold=0.3,\n    enabled_checks=[\"harmful_content\", \"bias\", \"fairness\"],\n)\n\nguard = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=ethics_config,\n)\n\n# Use in debate\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[guard.process],\n)\n</code></pre>"},{"location":"concepts/ethics/#ethicsconfig-options","title":"EthicsConfig Options","text":"Option Type Default Description <code>harmful_content_threshold</code> float 0.5 Threshold for harmful content detection <code>bias_threshold</code> float 0.4 Threshold for bias detection <code>fairness_threshold</code> float 0.3 Threshold for fairness violations <code>enabled_checks</code> list[str] default Checks to enable"},{"location":"concepts/ethics/#threshold-levels","title":"Threshold Levels","text":"<pre><code>from artemis.safety import EthicsConfig\n\n# Low sensitivity - only severe violations\nlow_config = EthicsConfig(harmful_content_threshold=0.7)\n\n# Medium sensitivity - most violations\nmedium_config = EthicsConfig(harmful_content_threshold=0.5)\n\n# High sensitivity - strict enforcement\nhigh_config = EthicsConfig(harmful_content_threshold=0.3)\n</code></pre>"},{"location":"concepts/ethics/#ethical-evaluation-in-l-ae-cr","title":"Ethical Evaluation in L-AE-CR","text":"<p>Arguments receive an ethics score as part of the adaptive evaluation:</p> <pre><code>from artemis.core.types import EvaluationCriteria\n\n# Ethical alignment is one of the default criteria\ncriteria = EvaluationCriteria(\n    logical_coherence=0.25,\n    evidence_quality=0.25,\n    causal_reasoning=0.20,\n    ethical_alignment=0.15,  # Ethics weight\n    persuasiveness=0.15,\n)\n</code></pre>"},{"location":"concepts/ethics/#what-ethical-evaluation-considers","title":"What Ethical Evaluation Considers","text":"<ul> <li>Claim Fairness: Are claims fair to all parties?</li> <li>Evidence Ethics: Is evidence used responsibly?</li> <li>Conclusion Ethics: Are conclusions ethically sound?</li> <li>Stakeholder Impact: Who is affected and how?</li> </ul>"},{"location":"concepts/ethics/#ethical-jury-perspective","title":"Ethical Jury Perspective","text":"<p>The jury includes an ethical perspective:</p> <pre><code>from artemis.core.types import JuryPerspective\n\n# The ETHICAL perspective focuses on moral implications\nJuryPerspective.ETHICAL\n</code></pre> <p>When creating a jury panel, the ethical perspective is automatically included:</p> <pre><code>from artemis.core.jury import JuryPanel\n\n# Create panel - ethical perspective is assigned to one juror\npanel = JuryPanel(evaluators=5, model=\"gpt-4o\")\n\n# The ethical juror focuses on:\n# - Consideration of ethical principles\n# - Attention to stakeholder welfare\n# - Fairness and justice concerns\n# - Long-term societal impact\n</code></pre>"},{"location":"concepts/ethics/#handling-ethical-debates","title":"Handling Ethical Debates","text":"<p>ARTEMIS can handle debates on complex ethical topics:</p> <pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def ethical_debate():\n    # Create agents for ethical debate\n    agents = [\n        Agent(\n            name=\"utilitarian\",\n            role=\"Advocate arguing from consequentialist ethics\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"deontologist\",\n            role=\"Advocate arguing from duty-based ethics\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    # Create jury - will include ethical perspective\n    jury = JuryPanel(evaluators=5, model=\"gpt-4o\")\n\n    debate = Debate(\n        topic=\"Should autonomous vehicles prioritize passenger or pedestrian safety?\",\n        agents=agents,\n        jury=jury,\n        rounds=3,\n    )\n\n    debate.assign_positions({\n        \"utilitarian\": \"maximize overall welfare in collision scenarios\",\n        \"deontologist\": \"respect individual rights regardless of outcomes\",\n    })\n\n    result = await debate.run()\n    return result\n\nasyncio.run(ethical_debate())\n</code></pre>"},{"location":"concepts/ethics/#safety-integration","title":"Safety Integration","text":"<p>Ethics monitoring integrates with other safety monitors:</p> <pre><code>from artemis.safety import (\n    SandbagDetector,\n    DeceptionMonitor,\n    EthicsGuard,\n    MonitorMode,\n    EthicsConfig,\n)\n\n# Configure all monitors\nsandbag = SandbagDetector(mode=MonitorMode.PASSIVE, sensitivity=0.7)\ndeception = DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6)\nethics = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=EthicsConfig(harmful_content_threshold=0.3),\n)\n\n# Use all monitors together\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[\n        sandbag.process,\n        deception.process,\n        ethics.process,\n    ],\n)\n</code></pre>"},{"location":"concepts/ethics/#safety-alerts","title":"Safety Alerts","text":"<p>When ethical violations are detected, they appear in the debate results:</p> <pre><code>result = await debate.run()\n\n# Check for ethical safety alerts\nfor alert in result.safety_alerts:\n    if \"ethics\" in alert.type.lower():\n        print(f\"Ethics alert: {alert.type}\")\n        print(f\"Severity: {alert.severity:.0%}\")\n        print(f\"Agent: {alert.agent}\")\n</code></pre>"},{"location":"concepts/ethics/#best-practices","title":"Best Practices","text":"<ol> <li>Set Appropriate Sensitivity: Match sensitivity to debate context</li> <li>Define Clear Principles: Be explicit about ethical boundaries</li> <li>Include Ethical Perspectives: Use jury to consider moral implications</li> <li>Enable Transparency: Make ethics decisions explainable</li> <li>Review Edge Cases: Some arguments may need human review</li> </ol>"},{"location":"concepts/ethics/#ethical-frameworks","title":"Ethical Frameworks","text":"<p>When debating ethical topics, different ethical frameworks provide different perspectives:</p> Framework Focus What It Values Utilitarian Greatest good Outcomes, consequences Deontological Rules and duties Principles, rights Virtue Ethics Character Intentions, virtues Care Ethics Relationships Context, relationships <p>These frameworks can be represented by different agents in a debate:</p> <pre><code>agents = [\n    Agent(\n        name=\"consequentialist\",\n        role=\"Argues from utilitarian perspective focusing on outcomes\",\n        model=\"gpt-4o\",\n    ),\n    Agent(\n        name=\"deontologist\",\n        role=\"Argues from duty-based perspective focusing on principles\",\n        model=\"gpt-4o\",\n    ),\n    Agent(\n        name=\"virtue_ethicist\",\n        role=\"Argues from virtue ethics focusing on character\",\n        model=\"gpt-4o\",\n    ),\n]\n</code></pre>"},{"location":"concepts/ethics/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Safety Monitoring for broader safety</li> <li>See how ethics integrates with Jury Mechanism</li> <li>Explore Ethics Guard in depth</li> </ul>"},{"location":"concepts/h-l-dag/","title":"H-L-DAG: Hierarchical Argument Generation","text":"<p>H-L-DAG (Hierarchical Leveled Directed Acyclic Graph) is ARTEMIS's approach to structured argument generation. Instead of generating flat text responses, agents construct arguments at multiple levels of abstraction.</p>"},{"location":"concepts/h-l-dag/#the-three-levels","title":"The Three Levels","text":""},{"location":"concepts/h-l-dag/#strategic-level","title":"Strategic Level","text":"<p>The highest level of abstraction, focusing on:</p> <ul> <li>Core thesis: The fundamental position being argued</li> <li>Strategic goals: What the argument aims to achieve</li> <li>Key differentiators: What makes this position compelling</li> </ul> <pre><code># Example strategic-level argument\nArgument(\n    agent=\"Proponent\",\n    level=ArgumentLevel.STRATEGIC,\n    content=\"\"\"\n    AI regulation is essential for societal protection. The core thesis\n    is that unregulated AI development poses existential risks that\n    outweigh the costs of oversight. This position prioritizes long-term\n    safety over short-term innovation speed.\n    \"\"\",\n    evidence=[],  # Strategic level focuses on framing\n)\n</code></pre>"},{"location":"concepts/h-l-dag/#tactical-level","title":"Tactical Level","text":"<p>The middle level, providing:</p> <ul> <li>Supporting points: Specific arguments backing the thesis</li> <li>Evidence chains: Logical connections between claims</li> <li>Counter-argument awareness: Acknowledgment of opposing views</li> </ul> <pre><code># Example tactical-level argument\nArgument(\n    agent=\"Proponent\",\n    level=ArgumentLevel.TACTICAL,\n    content=\"\"\"\n    Three key factors support AI regulation:\n    1. Historical precedent shows technology regulation benefits society\n    2. Current AI capabilities already exceed human oversight capacity\n    3. Industry self-regulation has proven insufficient\n    \"\"\",\n    evidence=[\n        Evidence(\n            type=\"example\",\n            content=\"FDA drug approval process demonstrates successful tech regulation\",\n            source=\"FDA History\",\n        ),\n        Evidence(\n            type=\"study\",\n            content=\"Analysis of 200+ AI incidents from 2020-2024\",\n            source=\"AI Incident Database\",\n        ),\n    ],\n)\n</code></pre>"},{"location":"concepts/h-l-dag/#operational-level","title":"Operational Level","text":"<p>The ground level with:</p> <ul> <li>Specific facts: Concrete data points</li> <li>Quotes and citations: Authoritative sources</li> <li>Examples: Real-world cases</li> </ul> <pre><code># Example operational-level argument\nArgument(\n    agent=\"Proponent\",\n    level=ArgumentLevel.OPERATIONAL,\n    content=\"\"\"\n    The EU AI Act, implemented in 2024, demonstrates successful regulation.\n    According to the European Commission, compliance costs averaged only\n    2.3% of development budgets while preventing an estimated 47 high-risk\n    AI deployments. Microsoft's CEO stated, \"Thoughtful regulation actually\n    accelerates responsible innovation.\"\n    \"\"\",\n    evidence=[\n        Evidence(\n            type=\"quote\",\n            content=\"compliance costs averaged 2.3% of development budgets\",\n            source=\"European Commission AI Act Report 2024\",\n        ),\n    ],\n)\n</code></pre>"},{"location":"concepts/h-l-dag/#how-h-l-dag-works","title":"How H-L-DAG Works","text":""},{"location":"concepts/h-l-dag/#argument-generation-flow","title":"Argument Generation Flow","text":"<pre><code>graph TD\n    A[Topic + Position] --&gt; B[Strategic Planning]\n    B --&gt; C[Generate Strategic Argument]\n    C --&gt; D[Identify Tactical Points]\n    D --&gt; E[Generate Tactical Arguments]\n    E --&gt; F[Gather Operational Evidence]\n    F --&gt; G[Generate Operational Arguments]\n    G --&gt; H[Assemble Complete Argument]</code></pre>"},{"location":"concepts/h-l-dag/#the-dag-structure","title":"The DAG Structure","text":"<p>Arguments form a directed acyclic graph:</p> <pre><code>Strategic Thesis\n\u251c\u2500\u2500 Tactical Point 1\n\u2502   \u251c\u2500\u2500 Operational Fact 1.1\n\u2502   \u2514\u2500\u2500 Operational Fact 1.2\n\u251c\u2500\u2500 Tactical Point 2\n\u2502   \u251c\u2500\u2500 Operational Fact 2.1\n\u2502   \u251c\u2500\u2500 Operational Fact 2.2\n\u2502   \u2514\u2500\u2500 Operational Fact 2.3\n\u2514\u2500\u2500 Tactical Point 3\n    \u2514\u2500\u2500 Operational Fact 3.1\n</code></pre>"},{"location":"concepts/h-l-dag/#evidence-types","title":"Evidence Types","text":"<p>The <code>Evidence</code> class supports these types:</p> Type Description <code>fact</code> Verifiable factual statement <code>statistic</code> Numerical data or statistics <code>quote</code> Direct quotation from a source <code>example</code> Real-world case or example <code>study</code> Research study or analysis <code>expert_opinion</code> Opinion from domain expert"},{"location":"concepts/h-l-dag/#causal-links","title":"Causal Links","text":"<p>H-L-DAG tracks causal relationships between arguments:</p> <pre><code>from artemis.core.types import CausalLink\n\nlink = CausalLink(\n    cause=\"unregulated_ai\",\n    effect=\"societal_harm\",\n    mechanism=\"Lack of oversight allows harmful deployments\",\n    strength=0.8,  # 0.0 to 1.0\n    bidirectional=False,\n)\n</code></pre>"},{"location":"concepts/h-l-dag/#causallink-fields","title":"CausalLink Fields","text":"Field Type Description <code>cause</code> str The cause in the relationship <code>effect</code> str The effect in the relationship <code>mechanism</code> str How cause leads to effect <code>strength</code> float Relationship strength (0-1) <code>bidirectional</code> bool Whether relationship works both ways"},{"location":"concepts/h-l-dag/#using-h-l-dag","title":"Using H-L-DAG","text":""},{"location":"concepts/h-l-dag/#configuring-arguments","title":"Configuring Arguments","text":"<pre><code>from artemis.core.types import DebateConfig\n\nconfig = DebateConfig(\n    require_evidence=True,\n    require_causal_links=True,\n    min_evidence_per_argument=1,\n    max_argument_tokens=1000,\n)\n</code></pre>"},{"location":"concepts/h-l-dag/#accessing-argument-structure","title":"Accessing Argument Structure","text":"<pre><code>result = await debate.run()\n\nfor turn in result.transcript:\n    arg = turn.argument\n\n    print(f\"Level: {arg.level}\")\n    print(f\"Content: {arg.content}\")\n\n    for evidence in arg.evidence:\n        print(f\"  Evidence: {evidence.source}\")\n\n    for link in arg.causal_links:\n        print(f\"  Causal: {link.cause} -&gt; {link.effect}\")\n</code></pre>"},{"location":"concepts/h-l-dag/#benefits-of-h-l-dag","title":"Benefits of H-L-DAG","text":""},{"location":"concepts/h-l-dag/#1-structured-reasoning","title":"1. Structured Reasoning","text":"<p>Arguments are systematically organized, making them easier to:</p> <ul> <li>Evaluate for logical coherence</li> <li>Compare across agents</li> <li>Identify gaps or weaknesses</li> </ul>"},{"location":"concepts/h-l-dag/#2-evidence-integration","title":"2. Evidence Integration","text":"<p>The operational level ensures arguments are grounded in:</p> <ul> <li>Concrete facts</li> <li>Authoritative sources</li> <li>Real-world examples</li> </ul>"},{"location":"concepts/h-l-dag/#3-causal-transparency","title":"3. Causal Transparency","text":"<p>Explicit causal links make reasoning transparent:</p> <ul> <li>Evaluators can verify logical connections</li> <li>Counter-arguments can target specific links</li> <li>Fallacies are easier to detect</li> </ul>"},{"location":"concepts/h-l-dag/#4-hierarchical-evaluation","title":"4. Hierarchical Evaluation","text":"<p>Different levels can be weighted differently:</p> <pre><code>evaluation_weights = {\n    \"strategic\": 0.3,   # 30% weight\n    \"tactical\": 0.4,    # 40% weight\n    \"operational\": 0.3, # 30% weight\n}\n</code></pre>"},{"location":"concepts/h-l-dag/#comparison-to-flat-arguments","title":"Comparison to Flat Arguments","text":"Aspect Flat Arguments H-L-DAG Structure Unorganized text Three-level hierarchy Evidence Mixed in text Explicit evidence objects Causality Implicit Explicit causal links Evaluation Holistic only Level-by-level possible Debugging Difficult Clear structure"},{"location":"concepts/h-l-dag/#next-steps","title":"Next Steps","text":"<ul> <li>Learn how arguments are evaluated with L-AE-CR</li> <li>See how the Jury Mechanism uses argument structure</li> <li>Explore Safety Monitoring of arguments</li> </ul>"},{"location":"concepts/jury/","title":"Jury Mechanism","text":"<p>ARTEMIS uses a multi-perspective jury system instead of a single evaluator. This provides more balanced verdicts and transparent decision-making.</p>"},{"location":"concepts/jury/#why-a-jury","title":"Why a Jury?","text":"<p>Single-evaluator systems have limitations:</p> <ul> <li>Bias: One perspective dominates</li> <li>Opacity: Hard to understand decisions</li> <li>Inconsistency: Results vary unpredictably</li> </ul> <p>A jury addresses these by:</p> <ul> <li>Multiple Perspectives: Different viewpoints considered</li> <li>Deliberation: Jurors evaluate arguments independently then aggregate</li> <li>Transparency: Each evaluation is explained</li> </ul>"},{"location":"concepts/jury/#jurypanel","title":"JuryPanel","text":"<p>The <code>JuryPanel</code> class manages multiple jury members:</p> <pre><code>from artemis.core.jury import JuryPanel\n\npanel = JuryPanel(\n    evaluators=5,              # Number of jury members\n    model=\"gpt-4o\",            # Model for jurors\n    consensus_threshold=0.7,   # Required agreement (0-1)\n)\n</code></pre>"},{"location":"concepts/jury/#jurypanel-options","title":"JuryPanel Options","text":"Option Type Default Description <code>evaluators</code> int 3 Number of jury members <code>model</code> str \"gpt-4o\" Default model for jurors <code>models</code> list[str] None Per-juror model list <code>jurors</code> list[JurorConfig] None Full juror configuration <code>consensus_threshold</code> float 0.7 Required agreement <code>criteria</code> list[str] default Evaluation criteria"},{"location":"concepts/jury/#per-juror-model-configuration","title":"Per-Juror Model Configuration","text":"<p>You can assign different models to each juror for diverse evaluation perspectives:</p> <p>Option A: Model List</p> <p>A simple list of models distributed across jurors:</p> <pre><code>jury = JuryPanel(\n    evaluators=3,\n    models=[\"gpt-4o\", \"claude-sonnet-4-20250514\", \"gemini-2.0-flash\"],\n)\n</code></pre> <p>If fewer models than evaluators, they cycle:</p> <pre><code># 5 jurors with 2 models = gpt-4o, claude, gpt-4o, claude, gpt-4o\njury = JuryPanel(\n    evaluators=5,\n    models=[\"gpt-4o\", \"claude-sonnet-4-20250514\"],\n)\n</code></pre> <p>Option B: JurorConfig Objects</p> <p>Full control over each juror's perspective, model, and criteria:</p> <pre><code>from artemis.core.types import JurorConfig, JuryPerspective\n\njury = JuryPanel(\n    jurors=[\n        JurorConfig(\n            perspective=JuryPerspective.ANALYTICAL,\n            model=\"gpt-4o\",\n            criteria=[\"logical_consistency\", \"evidence_strength\"],\n        ),\n        JurorConfig(\n            perspective=JuryPerspective.ETHICAL,\n            model=\"claude-sonnet-4-20250514\",\n            criteria=[\"ethical_alignment\", \"fairness\"],\n        ),\n        JurorConfig(\n            perspective=JuryPerspective.PRACTICAL,\n            model=\"gemini-2.0-flash\",\n            criteria=[\"feasibility\", \"real_world_impact\"],\n        ),\n    ],\n    consensus_threshold=0.7,\n)\n</code></pre>"},{"location":"concepts/jury/#jurorconfig-fields","title":"JurorConfig Fields","text":"Field Type Required Description <code>perspective</code> JuryPerspective Yes Juror's evaluation perspective <code>model</code> str Yes Model identifier <code>criteria</code> list[str] No Custom criteria for this juror <code>api_key</code> str No API key override for this juror"},{"location":"concepts/jury/#jury-perspectives","title":"Jury Perspectives","text":"<p>ARTEMIS includes five built-in perspectives via the <code>JuryPerspective</code> enum:</p> <pre><code>from artemis.core.types import JuryPerspective\n\n# Available perspectives\nJuryPerspective.ANALYTICAL   # Focus on logic and evidence\nJuryPerspective.ETHICAL      # Focus on moral implications\nJuryPerspective.PRACTICAL    # Focus on feasibility and impact\nJuryPerspective.ADVERSARIAL  # Challenge all arguments\nJuryPerspective.SYNTHESIZING # Find common ground\n</code></pre>"},{"location":"concepts/jury/#perspective-details","title":"Perspective Details","text":"<p>Analytical Perspective: - Prioritizes valid reasoning - Evaluates logical consistency - Values strong evidence</p> <p>Ethical Perspective: - Considers moral implications - Weighs stakeholder impact - Values fairness and justice</p> <p>Practical Perspective: - Focuses on feasibility - Considers implementation - Values real-world evidence</p> <p>Adversarial Perspective: - Questions all claims - Requires strong evidence - Identifies weak points</p> <p>Synthesizing Perspective: - Seeks common ground - Recognizes valid points from all sides - Values constructive framing</p>"},{"location":"concepts/jury/#automatic-perspective-assignment","title":"Automatic Perspective Assignment","text":"<p>When you create a <code>JuryPanel</code>, perspectives are automatically distributed among jurors:</p> <pre><code>panel = JuryPanel(evaluators=5, model=\"gpt-4o\")\n\n# Check assigned perspectives\nfor juror in panel.jurors:\n    print(f\"{juror.juror_id}: {juror.perspective.value}\")\n# juror_0: analytical\n# juror_1: ethical\n# juror_2: practical\n# juror_3: adversarial\n# juror_4: synthesizing\n</code></pre>"},{"location":"concepts/jury/#deliberation-process","title":"Deliberation Process","text":""},{"location":"concepts/jury/#flow","title":"Flow","text":"<pre><code>graph TD\n    A[Arguments Presented] --&gt; B[Individual Evaluation]\n    B --&gt; C[Score Computation]\n    C --&gt; D[Consensus Building]\n    D --&gt; E[Verdict Generation]</code></pre>"},{"location":"concepts/jury/#stages","title":"Stages","text":"<ol> <li>Individual Evaluation: Each juror evaluates arguments independently from their perspective</li> <li>Score Computation: Jurors compute scores for each agent based on evaluation criteria</li> <li>Consensus Building: Weighted voting determines the winner based on confidence</li> <li>Verdict Generation: Final verdict with reasoning and dissenting opinions</li> </ol>"},{"location":"concepts/jury/#verdict-structure","title":"Verdict Structure","text":"<p>The <code>Verdict</code> returned by deliberation includes:</p> <pre><code>from artemis.core.debate import Debate\n\n# After running a debate\nresult = await debate.run()\nverdict = result.verdict\n\nprint(f\"Decision: {verdict.decision}\")       # Winner name or \"draw\"\nprint(f\"Confidence: {verdict.confidence}\")   # 0.0 to 1.0\nprint(f\"Reasoning: {verdict.reasoning}\")     # Explanation\nprint(f\"Unanimous: {verdict.unanimous}\")     # Whether all jurors agreed\n\n# Score breakdown by agent\nif verdict.score_breakdown:\n    for agent, score in verdict.score_breakdown.items():\n        print(f\"  {agent}: {score:.2f}\")\n\n# Dissenting opinions\nfor dissent in verdict.dissenting_opinions:\n    print(f\"Dissent from {dissent.juror_id} ({dissent.perspective.value}):\")\n    print(f\"  Position: {dissent.position}\")\n    print(f\"  Reasoning: {dissent.reasoning}\")\n</code></pre>"},{"location":"concepts/jury/#verdict-fields","title":"Verdict Fields","text":"Field Type Description <code>decision</code> str Winner name or \"draw\" <code>confidence</code> float Confidence level (0-1) <code>reasoning</code> str Explanation of verdict <code>unanimous</code> bool Whether all jurors agreed <code>score_breakdown</code> dict Scores by agent <code>dissenting_opinions</code> list Dissenting juror opinions"},{"location":"concepts/jury/#using-the-jury","title":"Using the Jury","text":""},{"location":"concepts/jury/#in-debates","title":"In Debates","text":"<pre><code>from artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\nfrom artemis.core.agent import Agent\n\n# Create agents\nagents = [\n    Agent(\n        name=\"pro\",\n        role=\"Advocate supporting the proposition\",\n        model=\"gpt-4o\",\n    ),\n    Agent(\n        name=\"con\",\n        role=\"Advocate opposing the proposition\",\n        model=\"gpt-4o\",\n    ),\n]\n\n# Create jury panel\njury = JuryPanel(\n    evaluators=5,\n    model=\"gpt-4o\",\n    consensus_threshold=0.7,\n)\n\n# Create debate with jury\ndebate = Debate(\n    topic=\"Should we adopt this policy?\",\n    agents=agents,\n    jury=jury,\n)\n\ndebate.assign_positions({\n    \"pro\": \"supports the policy\",\n    \"con\": \"opposes the policy\",\n})\n\nresult = await debate.run()\nprint(f\"Verdict: {result.verdict.decision}\")\nprint(f\"Confidence: {result.verdict.confidence:.0%}\")\n</code></pre>"},{"location":"concepts/jury/#custom-criteria","title":"Custom Criteria","text":"<pre><code># Create jury with custom evaluation criteria\njury = JuryPanel(\n    evaluators=3,\n    model=\"gpt-4o\",\n    criteria=[\n        \"argument_quality\",\n        \"evidence_strength\",\n        \"logical_consistency\",\n        \"persuasiveness\",\n        \"ethical_alignment\",\n    ],\n)\n</code></pre>"},{"location":"concepts/jury/#jurymember","title":"JuryMember","text":"<p>Individual jurors can be accessed and examined:</p> <pre><code>from artemis.core.jury import JuryPanel, JuryMember\nfrom artemis.core.types import JuryPerspective\n\npanel = JuryPanel(evaluators=3, model=\"gpt-4o\")\n\n# Access individual jurors\nfor juror in panel.jurors:\n    print(f\"ID: {juror.juror_id}\")\n    print(f\"Perspective: {juror.perspective.value}\")\n    print(f\"Criteria: {juror.criteria}\")\n\n# Get specific juror\njuror = panel.get_juror(\"juror_0\")\nif juror:\n    print(f\"Found: {juror.juror_id}\")\n</code></pre>"},{"location":"concepts/jury/#consensus-calculation","title":"Consensus Calculation","text":"<p>The jury uses weighted voting to reach consensus:</p> <ol> <li>Each juror evaluates and determines their winner</li> <li>Votes are weighted by juror confidence</li> <li>Agreement score is calculated</li> <li>If below threshold, may result in \"draw\"</li> </ol> <pre><code># Consensus threshold affects verdict\npanel = JuryPanel(\n    evaluators=5,\n    consensus_threshold=0.6,  # Lower threshold = easier consensus\n)\n\n# Higher threshold requires stronger agreement\nstrict_panel = JuryPanel(\n    evaluators=5,\n    consensus_threshold=0.9,  # Requires near-unanimous agreement\n)\n</code></pre>"},{"location":"concepts/jury/#benefits-of-jury-system","title":"Benefits of Jury System","text":""},{"location":"concepts/jury/#1-reduced-bias","title":"1. Reduced Bias","text":"<p>Multiple perspectives prevent single-viewpoint dominance.</p>"},{"location":"concepts/jury/#2-transparent-decisions","title":"2. Transparent Decisions","text":"<p>Each juror's evaluation and reasoning is recorded.</p>"},{"location":"concepts/jury/#3-robust-verdicts","title":"3. Robust Verdicts","text":"<p>Consensus-based approach improves decision quality.</p>"},{"location":"concepts/jury/#4-explainable-results","title":"4. Explainable Results","text":"<p>Dissenting opinions provide insight into alternative viewpoints.</p>"},{"location":"concepts/jury/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about L-AE-CR Evaluation that feeds jury scores</li> <li>Understand H-L-DAG Arguments that juries evaluate</li> <li>Explore Ethics Module for ethical jury perspectives</li> </ul>"},{"location":"concepts/l-ae-cr/","title":"L-AE-CR: Adaptive Evaluation with Causal Reasoning","text":"<p>L-AE-CR (Leveled Adaptive Evaluation with Causal Reasoning) is ARTEMIS's dynamic evaluation system. Unlike static scoring, it adapts evaluation criteria based on context and traces causal relationships in arguments.</p>"},{"location":"concepts/l-ae-cr/#core-principles","title":"Core Principles","text":""},{"location":"concepts/l-ae-cr/#1-adaptive-criteria-weighting","title":"1. Adaptive Criteria Weighting","text":"<p>Evaluation criteria aren't fixed\u2014they shift based on:</p> <ul> <li>Topic Domain: Technical vs. ethical vs. policy debates</li> <li>Round Context: Opening statements vs. rebuttals</li> <li>Argument Type: Evidence-based vs. logical vs. emotional appeals</li> </ul>"},{"location":"concepts/l-ae-cr/#2-causal-chain-analysis","title":"2. Causal Chain Analysis","text":"<p>Arguments are evaluated not just on content but on the strength of causal reasoning:</p> <ul> <li>Does A actually cause B?</li> <li>Is the causal chain complete?</li> <li>Are there gaps or logical fallacies?</li> </ul>"},{"location":"concepts/l-ae-cr/#evaluation-criteria","title":"Evaluation Criteria","text":""},{"location":"concepts/l-ae-cr/#standard-criteria","title":"Standard Criteria","text":"<p>The <code>EvaluationCriteria</code> class defines the default weights:</p> <pre><code>from artemis.core.types import EvaluationCriteria\n\ncriteria = EvaluationCriteria(\n    logical_coherence=0.25,   # Internal consistency of argument\n    evidence_quality=0.25,    # Strength and relevance of evidence\n    causal_reasoning=0.20,    # Strength of causal reasoning\n    ethical_alignment=0.15,   # Ethical soundness\n    persuasiveness=0.15,      # Overall persuasiveness\n)\n</code></pre> Criterion Description Default Weight <code>logical_coherence</code> Internal consistency of argument 0.25 <code>evidence_quality</code> Strength and relevance of evidence 0.25 <code>causal_reasoning</code> Strength of causal reasoning 0.20 <code>ethical_alignment</code> Ethical soundness 0.15 <code>persuasiveness</code> Overall persuasiveness 0.15"},{"location":"concepts/l-ae-cr/#custom-weights","title":"Custom Weights","text":"<p>You can customize evaluation weights in the debate configuration:</p> <pre><code>from artemis.core.types import DebateConfig, EvaluationCriteria\n\n# Technical domain: higher evidence weight\ntechnical_criteria = EvaluationCriteria(\n    logical_coherence=0.30,\n    evidence_quality=0.35,\n    causal_reasoning=0.15,\n    ethical_alignment=0.10,\n    persuasiveness=0.10,\n)\n\n# Ethical domain: higher ethics weight\nethical_criteria = EvaluationCriteria(\n    logical_coherence=0.20,\n    evidence_quality=0.15,\n    causal_reasoning=0.15,\n    ethical_alignment=0.35,\n    persuasiveness=0.15,\n)\n\nconfig = DebateConfig(\n    evaluation_criteria=technical_criteria,\n    adaptation_enabled=True,\n    adaptation_rate=0.1,\n)\n</code></pre>"},{"location":"concepts/l-ae-cr/#evaluation-modes","title":"Evaluation Modes","text":"<p>ARTEMIS supports three evaluation modes to balance accuracy and cost:</p>"},{"location":"concepts/l-ae-cr/#evaluationmodequality","title":"EvaluationMode.QUALITY","text":"<p>Uses LLM (e.g., gpt-4o-mini) to evaluate all criteria:</p> <ul> <li>Highest accuracy for criterion scoring</li> <li>Provides detailed reasoning for each score</li> <li>Best for benchmarking and critical decisions</li> <li>Highest cost per evaluation</li> </ul> <pre><code>from artemis.core.types import DebateConfig, EvaluationMode\n\nconfig = DebateConfig(evaluation_mode=EvaluationMode.QUALITY)\n</code></pre>"},{"location":"concepts/l-ae-cr/#evaluationmodebalanced-default","title":"EvaluationMode.BALANCED (Default)","text":"<p>Selective LLM usage for optimal cost/accuracy:</p> <ul> <li>Uses LLM for jury verdicts and key decisions</li> <li>Heuristics for in-debate turn evaluation</li> <li>Good accuracy with moderate cost</li> <li>Recommended for production use</li> </ul> <pre><code>config = DebateConfig(evaluation_mode=EvaluationMode.BALANCED)\n</code></pre>"},{"location":"concepts/l-ae-cr/#evaluationmodefast","title":"EvaluationMode.FAST","text":"<p>Pure heuristic evaluation:</p> <ul> <li>No LLM calls during evaluation</li> <li>Lowest cost, fastest execution</li> <li>Backwards compatible with original behavior</li> <li>Best for development, testing, or cost-sensitive deployments</li> </ul> <pre><code>config = DebateConfig(evaluation_mode=EvaluationMode.FAST)\n</code></pre>"},{"location":"concepts/l-ae-cr/#llmcriterionevaluator","title":"LLMCriterionEvaluator","text":"<p>When using <code>QUALITY</code> mode, the <code>LLMCriterionEvaluator</code> class performs evaluation:</p> <pre><code>from artemis.core.llm_evaluation import LLMCriterionEvaluator\n\nevaluator = LLMCriterionEvaluator(\n    model=\"gpt-4o-mini\",   # Model for evaluation\n    cache_enabled=True,     # Cache evaluations by content hash\n)\n\n# Evaluate a single argument\nresult = await evaluator.evaluate_argument(argument, context)\n\n# Result contains scores and reasoning\nprint(f\"Total score: {result.total_score}\")\nfor detail in result.criterion_details:\n    print(f\"{detail.criterion}: {detail.score:.2f} - {detail.reasoning}\")\n</code></pre>"},{"location":"concepts/l-ae-cr/#evaluatorfactory","title":"EvaluatorFactory","text":"<p>Create the appropriate evaluator based on mode:</p> <pre><code>from artemis.core.llm_evaluation import EvaluatorFactory\nfrom artemis.core.types import EvaluationMode\n\n# Returns LLMCriterionEvaluator for QUALITY mode\nevaluator = EvaluatorFactory.create(EvaluationMode.QUALITY, model=\"gpt-4o-mini\")\n\n# Returns AdaptiveEvaluator for BALANCED/FAST modes\nevaluator = EvaluatorFactory.create(EvaluationMode.FAST)\n</code></pre>"},{"location":"concepts/l-ae-cr/#argument-evaluation","title":"Argument Evaluation","text":"<p>Each argument receives an <code>ArgumentEvaluation</code> with detailed scores:</p> <pre><code># After running a debate\nresult = await debate.run()\n\nfor turn in result.transcript:\n    if turn.evaluation:\n        eval = turn.evaluation\n        print(f\"Agent: {turn.agent}\")\n        print(f\"Total Score: {eval.total_score:.2f}\")\n        print(\"Criterion Scores:\")\n        for criterion, score in eval.scores.items():\n            weight = eval.weights.get(criterion, 0)\n            print(f\"  {criterion}: {score:.2f} (weight: {weight:.2f})\")\n        print(f\"Causal Score: {eval.causal_score:.2f}\")\n</code></pre>"},{"location":"concepts/l-ae-cr/#argumentevaluation-fields","title":"ArgumentEvaluation Fields","text":"Field Type Description <code>argument_id</code> str ID of evaluated argument <code>scores</code> dict Score for each criterion <code>weights</code> dict Adapted weight for each criterion <code>criterion_details</code> list Detailed per-criterion breakdown <code>causal_score</code> float Score for causal reasoning <code>total_score</code> float Weighted total score"},{"location":"concepts/l-ae-cr/#causal-reasoning-analysis","title":"Causal Reasoning Analysis","text":""},{"location":"concepts/l-ae-cr/#causallink-structure","title":"CausalLink Structure","text":"<p>Arguments contain causal links that are evaluated:</p> <pre><code>from artemis.core.types import CausalLink\n\n# Causal links in arguments\nlink = CausalLink(\n    cause=\"increased_regulation\",\n    effect=\"reduced_innovation_speed\",\n    mechanism=\"Compliance overhead diverts resources\",\n    strength=0.7,\n    bidirectional=False,\n)\n</code></pre>"},{"location":"concepts/l-ae-cr/#causal-evaluation","title":"Causal Evaluation","text":"<p>The evaluation system assesses:</p> <ol> <li>Completeness: Is the causal chain fully specified?</li> <li>Strength: How strong is each link?</li> <li>Evidence: Is there evidence supporting causation?</li> <li>Validity: Are there logical fallacies?</li> </ol>"},{"location":"concepts/l-ae-cr/#common-causal-fallacies-detected","title":"Common Causal Fallacies Detected","text":"Fallacy Description Post Hoc Assumes causation from sequence Correlation Treats correlation as causation Single Cause Ignores multiple factors Slippery Slope Assumes inevitable escalation"},{"location":"concepts/l-ae-cr/#evaluation-flow","title":"Evaluation Flow","text":"<pre><code>graph TD\n    A[Receive Argument] --&gt; B[Identify Context]\n    B --&gt; C[Adapt Criteria Weights]\n    C --&gt; D[Extract Causal Links]\n    D --&gt; E[Score Each Criterion]\n    E --&gt; F[Compute Causal Score]\n    F --&gt; G[Compute Weighted Total]\n    G --&gt; H[Return Evaluation]</code></pre>"},{"location":"concepts/l-ae-cr/#adaptive-weight-adjustment","title":"Adaptive Weight Adjustment","text":"<p>When <code>adaptation_enabled=True</code>, weights are dynamically adjusted:</p> <pre><code>from artemis.core.types import DebateConfig\n\nconfig = DebateConfig(\n    adaptation_enabled=True,\n    adaptation_rate=0.1,  # How fast weights adjust (0-0.5)\n)\n</code></pre>"},{"location":"concepts/l-ae-cr/#adaptation-factors","title":"Adaptation Factors","text":"<p>Weights adapt based on:</p> <ul> <li>Topic sensitivity: Higher ethical weight for sensitive topics</li> <li>Topic complexity: Higher causal weight for complex topics</li> <li>Round progress: Different expectations for opening vs. closing</li> <li>Argument type: Evidence-heavy arguments get higher evidence weight</li> </ul>"},{"location":"concepts/l-ae-cr/#using-the-evaluator","title":"Using the Evaluator","text":"<p>The <code>AdaptiveEvaluator</code> is used internally but can be accessed:</p> <pre><code>from artemis.core.evaluation import AdaptiveEvaluator\nfrom artemis.core.types import DebateContext\n\nevaluator = AdaptiveEvaluator()\n\n# Evaluate a single argument\nevaluation = await evaluator.evaluate_argument(\n    argument=argument,\n    context=debate_context,\n)\n\nprint(f\"Total Score: {evaluation.total_score:.2f}\")\nprint(f\"Breakdown: {evaluation.scores}\")\n</code></pre>"},{"location":"concepts/l-ae-cr/#integration-with-jury","title":"Integration with Jury","text":"<p>L-AE-CR provides scores to the jury mechanism:</p> <pre><code>from artemis.core.jury import JuryPanel\n\n# Jury members receive evaluation results\npanel = JuryPanel(evaluators=5, model=\"gpt-4o\")\n\n# Each jury member considers:\n# - Evaluation scores from L-AE-CR\n# - Their perspective-specific weights\n# - Argument content and structure\n</code></pre>"},{"location":"concepts/l-ae-cr/#score-components","title":"Score Components","text":""},{"location":"concepts/l-ae-cr/#logical-coherence-score","title":"Logical Coherence Score","text":"<p>Evaluates internal consistency:</p> <ul> <li>Premises support conclusion</li> <li>No contradictions</li> <li>Valid inference patterns</li> </ul>"},{"location":"concepts/l-ae-cr/#evidence-quality-score","title":"Evidence Quality Score","text":"<p>Evaluates supporting evidence:</p> <ul> <li>Source credibility</li> <li>Relevance to claims</li> <li>Recency and accuracy</li> <li>Diversity of sources</li> </ul>"},{"location":"concepts/l-ae-cr/#causal-reasoning-score","title":"Causal Reasoning Score","text":"<p>Evaluates causal reasoning:</p> <ul> <li>Chain completeness</li> <li>Link strength</li> <li>Evidence for causation</li> <li>Fallacy absence</li> </ul>"},{"location":"concepts/l-ae-cr/#ethical-alignment-score","title":"Ethical Alignment Score","text":"<p>Evaluates ethical soundness:</p> <ul> <li>Fairness of reasoning</li> <li>Consideration of stakeholders</li> <li>Avoidance of harmful claims</li> <li>Respect for values</li> </ul>"},{"location":"concepts/l-ae-cr/#persuasiveness-score","title":"Persuasiveness Score","text":"<p>Evaluates persuasiveness:</p> <ul> <li>Clarity of thesis</li> <li>Effectiveness of rhetoric</li> <li>Audience appropriateness</li> <li>Counter-argument handling</li> </ul>"},{"location":"concepts/l-ae-cr/#benefits-of-l-ae-cr","title":"Benefits of L-AE-CR","text":""},{"location":"concepts/l-ae-cr/#1-context-aware-evaluation","title":"1. Context-Aware Evaluation","text":"<ul> <li>Adapts to topic domain automatically</li> <li>Adjusts for round context</li> <li>Considers argument type</li> </ul>"},{"location":"concepts/l-ae-cr/#2-transparent-scoring","title":"2. Transparent Scoring","text":"<ul> <li>Clear criteria breakdown</li> <li>Weighted contributions visible</li> <li>Feedback explains scores</li> </ul>"},{"location":"concepts/l-ae-cr/#3-causal-rigor","title":"3. Causal Rigor","text":"<ul> <li>Validates causal claims</li> <li>Detects logical fallacies</li> <li>Measures chain strength</li> </ul>"},{"location":"concepts/l-ae-cr/#4-fair-assessment","title":"4. Fair Assessment","text":"<ul> <li>Multiple criteria considered</li> <li>Weights prevent single-focus bias</li> <li>Adaptation ensures relevance</li> </ul>"},{"location":"concepts/l-ae-cr/#next-steps","title":"Next Steps","text":"<ul> <li>See how evaluations feed into the Jury Mechanism</li> <li>Learn about H-L-DAG argument structure</li> <li>Explore Safety Monitoring integration</li> </ul>"},{"location":"concepts/llm-extraction/","title":"LLM-Based Extraction","text":"<p>ARTEMIS uses LLM-based extraction to identify evidence and causal relationships in arguments. This provides more accurate extraction than regex patterns alone.</p>"},{"location":"concepts/llm-extraction/#overview","title":"Overview","text":"<p>Traditional regex-based extraction struggles with natural language variations. LLM extraction understands context and can identify:</p> <ul> <li>Evidence: Facts, statistics, quotes, examples, studies, expert opinions</li> <li>Causal Links: Cause-effect relationships with mechanisms and strength</li> </ul>"},{"location":"concepts/llm-extraction/#extractors","title":"Extractors","text":""},{"location":"concepts/llm-extraction/#llmcausalextractor","title":"LLMCausalExtractor","text":"<p>Extracts cause-effect relationships from argument text.</p> <pre><code>from artemis.core.llm_extraction import LLMCausalExtractor\n\nextractor = LLMCausalExtractor(\n    model_name=\"gpt-4o-mini\",  # Model for extraction\n    use_cache=True,            # Cache results by content hash\n)\n\nlinks = await extractor.extract(content)\n# Returns: list[CausalLink]\n</code></pre> <p>Each <code>CausalLink</code> contains:</p> Field Type Description <code>cause</code> str What triggers the effect <code>effect</code> str What results from the cause <code>mechanism</code> str How the cause leads to the effect <code>strength</code> float Confidence (0.0-1.0)"},{"location":"concepts/llm-extraction/#llmevidenceextractor","title":"LLMEvidenceExtractor","text":"<p>Extracts evidence citations from argument text.</p> <pre><code>from artemis.core.llm_extraction import LLMEvidenceExtractor\n\nextractor = LLMEvidenceExtractor(\n    model_name=\"gpt-4o-mini\",\n    use_cache=True,\n)\n\nevidence = await extractor.extract(content)\n# Returns: list[Evidence]\n</code></pre> <p>Each <code>Evidence</code> contains:</p> Field Type Description <code>type</code> str One of: fact, statistic, quote, example, study, expert_opinion <code>content</code> str The evidence text <code>source</code> str Source attribution if mentioned <code>verified</code> bool Whether evidence has been verified"},{"location":"concepts/llm-extraction/#hybrid-extractors","title":"Hybrid Extractors","text":"<p>For cost-effective extraction, use hybrid extractors that try fast regex patterns first and fall back to LLM when needed.</p> <pre><code>from artemis.core.llm_extraction import (\n    HybridCausalExtractor,\n    HybridEvidenceExtractor,\n)\n\n# Tries regex first, falls back to LLM\ncausal_extractor = HybridCausalExtractor(\n    model_name=\"gpt-4o-mini\",\n    use_cache=True,\n)\n\nevidence_extractor = HybridEvidenceExtractor(\n    model_name=\"gpt-4o-mini\",\n    use_cache=True,\n)\n\n# Use same as LLM extractors\nlinks = await causal_extractor.extract(content)\nevidence = await evidence_extractor.extract(content)\n</code></pre>"},{"location":"concepts/llm-extraction/#extraction-strategy","title":"Extraction Strategy","text":"<pre><code>Content \u2192 Regex Extraction\n              \u2502\n              \u251c\u2500 Results found? \u2192 Return results\n              \u2502\n              \u2514\u2500 No results? \u2192 LLM Extraction \u2192 Return results\n</code></pre>"},{"location":"concepts/llm-extraction/#caching","title":"Caching","text":"<p>Extraction results are cached by content hash to avoid redundant LLM calls:</p> <pre><code># First call: LLM extraction\nlinks1 = await extractor.extract(\"Climate change causes...\")\n\n# Second call with same content: Cache hit (no LLM call)\nlinks2 = await extractor.extract(\"Climate change causes...\")\n</code></pre> <p>Clear the cache when needed:</p> <pre><code>from artemis.core.llm_extraction import clear_extraction_cache\n\nclear_extraction_cache()\n</code></pre>"},{"location":"concepts/llm-extraction/#integration-with-agents","title":"Integration with Agents","text":"<p>The extraction system is automatically used during argument generation:</p> <pre><code>from artemis.core.agent import Agent\n\nagent = Agent(\n    name=\"analyst\",\n    role=\"Research analyst\",\n    model=\"gpt-4o\",\n)\n\n# Extraction happens automatically during argument generation\nargument = await agent.generate_argument(context)\n\n# Access extracted data\nprint(f\"Evidence found: {len(argument.evidence)}\")\nprint(f\"Causal links: {len(argument.causal_links)}\")\n</code></pre>"},{"location":"concepts/llm-extraction/#example-output","title":"Example Output","text":"<p>Given an argument about climate policy:</p> <pre><code>content = \"\"\"\nRising global temperatures cause increased extreme weather events.\nAccording to the IPCC 2023 report, we've seen a 40% increase in\nCategory 4+ hurricanes since 1980. This leads to higher economic\ncosts, estimated at $300 billion annually by 2030.\n\"\"\"\n\n# Causal extraction\nlinks = await causal_extractor.extract(content)\n# [\n#   CausalLink(\n#     cause=\"Rising global temperatures\",\n#     effect=\"increased extreme weather events\",\n#     strength=0.8\n#   ),\n#   CausalLink(\n#     cause=\"extreme weather events\",\n#     effect=\"higher economic costs\",\n#     strength=0.7\n#   )\n# ]\n\n# Evidence extraction\nevidence = await evidence_extractor.extract(content)\n# [\n#   Evidence(\n#     type=\"study\",\n#     content=\"IPCC 2023 report\",\n#     source=\"IPCC\"\n#   ),\n#   Evidence(\n#     type=\"statistic\",\n#     content=\"40% increase in Category 4+ hurricanes since 1980\",\n#     source=\"IPCC 2023 report\"\n#   ),\n#   Evidence(\n#     type=\"statistic\",\n#     content=\"$300 billion annually by 2030\",\n#     source=None\n#   )\n# ]\n</code></pre>"},{"location":"concepts/llm-extraction/#configuration","title":"Configuration","text":""},{"location":"concepts/llm-extraction/#model-selection","title":"Model Selection","text":"<p>Use smaller models for cost-effective extraction:</p> <pre><code># Cost-effective (recommended)\nextractor = LLMCausalExtractor(model_name=\"gpt-4o-mini\")\n\n# Higher accuracy for critical applications\nextractor = LLMCausalExtractor(model_name=\"gpt-4o\")\n</code></pre>"},{"location":"concepts/llm-extraction/#disabling-cache","title":"Disabling Cache","text":"<p>For debugging or when content changes frequently:</p> <pre><code>extractor = LLMCausalExtractor(\n    model_name=\"gpt-4o-mini\",\n    use_cache=False,  # Always call LLM\n)\n</code></pre>"},{"location":"concepts/llm-extraction/#custom-model-instance","title":"Custom Model Instance","text":"<p>Pass a pre-configured model:</p> <pre><code>from artemis.models.base import ModelRegistry\n\nmodel = ModelRegistry.create(\"gpt-4o-mini\", temperature=0)\n\nextractor = LLMCausalExtractor(model=model)\n</code></pre>"},{"location":"concepts/overview/","title":"Core Concepts Overview","text":"<p>ARTEMIS implements a structured approach to multi-agent debates based on three core innovations from the research paper.</p>"},{"location":"concepts/overview/#the-artemis-architecture","title":"The ARTEMIS Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        ARTEMIS Core                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502  \u2502   H-L-DAG   \u2502  \u2502   L-AE-CR   \u2502  \u2502    Jury     \u2502             \u2502\n\u2502  \u2502  Argument   \u2502\u2500\u2500\u2502  Adaptive   \u2502\u2500\u2500\u2502   Scoring   \u2502             \u2502\n\u2502  \u2502 Generation  \u2502  \u2502 Evaluation  \u2502  \u2502  Mechanism  \u2502             \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502         \u2502                \u2502                \u2502                    \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502                          \u2502                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502                   Safety Layer                          \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502\n\u2502  \u2502  \u2502Sandbagging\u2502  \u2502Deception \u2502  \u2502 Behavior \u2502  \u2502 Ethics  \u2502 \u2502   \u2502\n\u2502  \u2502  \u2502 Detector  \u2502  \u2502 Monitor  \u2502  \u2502 Tracker  \u2502  \u2502 Guard   \u2502 \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/overview/#key-innovations","title":"Key Innovations","text":""},{"location":"concepts/overview/#1-h-l-dag-hierarchical-argument-generation","title":"1. H-L-DAG: Hierarchical Argument Generation","text":"<p>Unlike simple chat-based exchanges, ARTEMIS generates arguments at three levels:</p> <ul> <li>Strategic Level: High-level thesis and position</li> <li>Tactical Level: Supporting points and evidence chains</li> <li>Operational Level: Specific facts, quotes, and examples</li> </ul> <p>This hierarchical approach ensures arguments are well-structured and comprehensive.</p> <p>Learn more about H-L-DAG \u2192</p>"},{"location":"concepts/overview/#2-l-ae-cr-adaptive-evaluation-with-causal-reasoning","title":"2. L-AE-CR: Adaptive Evaluation with Causal Reasoning","text":"<p>Traditional evaluation uses fixed criteria. ARTEMIS dynamically adjusts evaluation based on:</p> <ul> <li>Topic Domain: Technical topics weight evidence differently than ethical ones</li> <li>Round Context: Opening arguments vs. rebuttals have different expectations</li> <li>Causal Relationships: Arguments with strong causal chains score higher</li> </ul> <p>Learn more about L-AE-CR \u2192</p>"},{"location":"concepts/overview/#3-jury-mechanism","title":"3. Jury Mechanism","text":"<p>Instead of a single evaluator, ARTEMIS uses a multi-perspective jury:</p> <ul> <li>Multiple jury members with different perspectives</li> <li>Deliberation process for consensus building</li> <li>Transparent verdict with confidence scores</li> </ul> <p>Learn more about the Jury \u2192</p>"},{"location":"concepts/overview/#debate-flow","title":"Debate Flow","text":"<p>A typical ARTEMIS debate follows this flow:</p> <pre><code>graph TD\n    A[Topic Announced] --&gt; B[Opening Statements]\n    B --&gt; C[Round 1: Arguments]\n    C --&gt; D[Evaluation]\n    D --&gt; E{More Rounds?}\n    E --&gt;|Yes| F[Round N: Rebuttals]\n    F --&gt; D\n    E --&gt;|No| G[Jury Deliberation]\n    G --&gt; H[Verdict]</code></pre>"},{"location":"concepts/overview/#phases","title":"Phases","text":"<ol> <li> <p>Initialization</p> <ul> <li>Topic is set</li> <li>Agents are assigned positions</li> <li>Jury is configured</li> </ul> </li> <li> <p>Opening Statements</p> <ul> <li>Each agent presents their initial position</li> <li>No rebuttals yet</li> </ul> </li> <li> <p>Argumentation Rounds</p> <ul> <li>Agents take turns presenting arguments</li> <li>Each argument can include rebuttals</li> <li>Arguments are evaluated after each turn</li> </ul> </li> <li> <p>Jury Deliberation</p> <ul> <li>All arguments are considered</li> <li>Jury members vote</li> <li>Consensus is reached</li> </ul> </li> <li> <p>Verdict</p> <ul> <li>Winner is declared (or tie)</li> <li>Confidence score provided</li> <li>Reasoning explained</li> </ul> </li> </ol>"},{"location":"concepts/overview/#core-components","title":"Core Components","text":""},{"location":"concepts/overview/#agent","title":"Agent","text":"<p>The <code>Agent</code> class represents a debate participant:</p> <pre><code>from artemis.core.agent import Agent\n\nagent = Agent(\n    name=\"analyst\",\n    role=\"Domain expert analyzing the topic\",\n    model=\"gpt-4o\",\n)\n</code></pre>"},{"location":"concepts/overview/#debate","title":"Debate","text":"<p>The <code>Debate</code> class orchestrates the entire debate:</p> <pre><code>from artemis.core.debate import Debate\n\ndebate = Debate(\n    topic=\"Your debate topic\",\n    agents=[agent1, agent2],\n    rounds=3,\n)\n\nresult = await debate.run()\n</code></pre>"},{"location":"concepts/overview/#argument","title":"Argument","text":"<p>Arguments are structured data with hierarchy:</p> <pre><code>from artemis.core.types import Argument, ArgumentLevel\n\nargument = Argument(\n    agent=\"pro_agent\",\n    content=\"The main argument text...\",\n    level=ArgumentLevel.STRATEGIC,\n    evidence=[...],\n    causal_links=[...],\n)\n</code></pre>"},{"location":"concepts/overview/#verdict","title":"Verdict","text":"<p>The final verdict includes:</p> <pre><code>result.verdict.decision    # \"pro\", \"con\", or \"tie\"\nresult.verdict.confidence  # 0.0 to 1.0\nresult.verdict.reasoning   # Explanation\n</code></pre>"},{"location":"concepts/overview/#ethical-considerations","title":"Ethical Considerations","text":"<p>ARTEMIS includes built-in ethical considerations at every stage:</p> <ul> <li>Generation: Arguments are filtered for ethical content</li> <li>Evaluation: Ethical criteria are weighted appropriately</li> <li>Monitoring: Ethics guard detects boundary violations</li> </ul> <p>Learn more about Ethics \u2192</p>"},{"location":"concepts/overview/#next-steps","title":"Next Steps","text":"<p>Dive deeper into each core concept:</p> <ul> <li>H-L-DAG Argument Generation</li> <li>L-AE-CR Adaptive Evaluation</li> <li>Jury Mechanism</li> <li>Ethics Module</li> </ul> <p>Or explore practical applications:</p> <ul> <li>Safety Monitoring</li> <li>Framework Integrations</li> <li>Examples</li> </ul>"},{"location":"concepts/prompts/","title":"Centralized Prompts System","text":"<p>ARTEMIS uses a centralized prompt management system for versioning, A/B testing, and maintainability. All prompts are stored in <code>artemis/prompts/</code> and accessed through a unified API.</p>"},{"location":"concepts/prompts/#why-centralized-prompts","title":"Why Centralized Prompts?","text":"<ul> <li>Versioning: Test new prompt versions without code changes</li> <li>A/B Testing: Compare prompt performance across versions</li> <li>Maintainability: Single source of truth for all prompts</li> <li>Rollback: Quickly revert to previous prompt versions</li> </ul>"},{"location":"concepts/prompts/#usage","title":"Usage","text":""},{"location":"concepts/prompts/#basic-usage","title":"Basic Usage","text":"<pre><code>from artemis.prompts import get_prompt, list_prompts\n\n# Get a prompt by key\nprompt = get_prompt(\"hdag.strategic_instructions\")\n\n# Get prompt with formatting variables\nprompt = get_prompt(\n    \"evaluation.user\",\n    topic=\"AI Safety\",\n    level=\"strategic\",\n    content=\"...\",\n    round=1,\n    total_rounds=3,\n    position=\"pro\",\n    prev_count=2,\n)\n\n# List all available prompts\nprompts = list_prompts()\n# Returns: {'hdag': ['STRATEGIC_INSTRUCTIONS', ...], 'evaluation': [...], ...}\n</code></pre>"},{"location":"concepts/prompts/#prompt-key-format","title":"Prompt Key Format","text":"<p>Prompts are accessed using dot notation: <code>module.prompt_name</code></p> <ul> <li><code>hdag.strategic_instructions</code> - H-L-DAG strategic level prompt</li> <li><code>evaluation.system</code> - Evaluation system prompt</li> <li><code>extraction.causal_extraction</code> - Causal link extraction prompt</li> <li><code>jury.analytical_perspective</code> - Analytical juror perspective</li> </ul>"},{"location":"concepts/prompts/#available-modules","title":"Available Modules","text":"Module Description Key Prompts <code>hdag</code> H-L-DAG argument generation <code>STRATEGIC_INSTRUCTIONS</code>, <code>TACTICAL_INSTRUCTIONS</code>, <code>OPERATIONAL_INSTRUCTIONS</code>, <code>OPENING_STATEMENT</code>, <code>CLOSING_STATEMENT</code> <code>evaluation</code> LLM-based evaluation <code>SYSTEM</code>, <code>USER</code>, <code>CRITERIA_DEFINITIONS</code>, <code>DEFAULT_WEIGHTS</code> <code>extraction</code> Evidence/causal extraction <code>CAUSAL_EXTRACTION</code>, <code>EVIDENCE_EXTRACTION</code> <code>jury</code> Jury perspectives <code>ANALYTICAL_PERSPECTIVE</code>, <code>ETHICAL_PERSPECTIVE</code>, <code>PRACTICAL_PERSPECTIVE</code>, <code>ADVERSARIAL_PERSPECTIVE</code> <code>benchmark</code> Benchmark evaluation <code>ARGUMENT_QUALITY</code>, <code>DECISION_ACCURACY</code>, <code>REASONING_DEPTH</code>"},{"location":"concepts/prompts/#versioning","title":"Versioning","text":"<p>Prompts are organized by version (<code>v1</code>, <code>v2</code>, etc.) for controlled experimentation.</p>"},{"location":"concepts/prompts/#default-version","title":"Default Version","text":"<p>The default version is <code>v1</code>. All <code>get_prompt()</code> calls use this unless specified otherwise.</p> <pre><code>from artemis.prompts import get_prompt_version\n\ncurrent = get_prompt_version()  # Returns \"v1\"\n</code></pre>"},{"location":"concepts/prompts/#switching-versions","title":"Switching Versions","text":""},{"location":"concepts/prompts/#global-version-switch","title":"Global Version Switch","text":"<pre><code>from artemis.prompts.loader import set_prompt_version\n\n# Switch all prompts to v2\nset_prompt_version(\"v2\")\n\n# Now all get_prompt() calls use v2\nprompt = get_prompt(\"hdag.strategic_instructions\")  # Uses v2\n</code></pre>"},{"location":"concepts/prompts/#per-call-version","title":"Per-Call Version","text":"<pre><code>from artemis.prompts import get_prompt\n\n# Use v2 for this specific call\nprompt = get_prompt(\"hdag.strategic_instructions\", version=\"v2\")\n\n# Default version unchanged for other calls\nother_prompt = get_prompt(\"evaluation.system\")  # Still uses v1\n</code></pre>"},{"location":"concepts/prompts/#adding-new-prompts","title":"Adding New Prompts","text":""},{"location":"concepts/prompts/#1-add-to-existing-module","title":"1. Add to Existing Module","text":"<p>Edit the appropriate file in <code>artemis/prompts/v1/</code>:</p> <pre><code># artemis/prompts/v1/hdag.py\n\n# Add new constant (uppercase)\nMY_NEW_PROMPT = \"\"\"Your prompt content here.\n\nVariables use {curly_braces} for formatting:\nTopic: {topic}\n\"\"\"\n</code></pre> <p>Access with: <pre><code>prompt = get_prompt(\"hdag.my_new_prompt\", topic=\"AI Safety\")\n</code></pre></p>"},{"location":"concepts/prompts/#2-create-new-version","title":"2. Create New Version","text":"<p>Copy the <code>v1/</code> directory to <code>v2/</code> and modify prompts:</p> <pre><code>artemis/prompts/\n\u251c\u2500\u2500 v1/\n\u2502   \u251c\u2500\u2500 hdag.py\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 v2/          # New version\n    \u251c\u2500\u2500 hdag.py  # Modified prompts\n    \u2514\u2500\u2500 ...\n</code></pre> <p>Register in <code>artemis/prompts/loader.py</code>:</p> <pre><code>PROMPT_MODULES = {\n    \"v1\": {...},\n    \"v2\": {\n        \"hdag\": \"artemis.prompts.v2.hdag\",\n        # ... other modules\n    },\n}\n</code></pre>"},{"location":"concepts/prompts/#prompt-module-structure","title":"Prompt Module Structure","text":"<p>Each prompt module follows this structure:</p> <pre><code># artemis/prompts/v1/example.py\n\"\"\"\nExample Prompts - Version 1\n\nDescription of what these prompts are for.\n\"\"\"\n\n# Prompts are uppercase constants\nSYSTEM_PROMPT = \"\"\"You are an assistant...\"\"\"\n\nUSER_PROMPT = \"\"\"Given the following:\nTopic: {topic}\nContext: {context}\n\nPlease provide your analysis.\"\"\"\n\n# Supporting constants (also accessible)\nDEFAULT_CONFIG = {\n    \"temperature\": 0.7,\n    \"max_tokens\": 1000,\n}\n</code></pre>"},{"location":"concepts/prompts/#integration-example","title":"Integration Example","text":"<p>Here's how the prompt system integrates with ARTEMIS components:</p> <pre><code>from artemis.prompts import get_prompt\nfrom artemis.core.types import Message\n\n# In LLM evaluation\nsystem_prompt = get_prompt(\"evaluation.system\")\nuser_prompt = get_prompt(\n    \"evaluation.user\",\n    topic=context.topic,\n    level=argument.level.value,\n    content=argument.content[:2000],\n    round=context.current_round,\n    total_rounds=context.total_rounds,\n    position=position,\n    prev_count=len(context.transcript),\n)\n\nmessages = [\n    Message(role=\"system\", content=system_prompt),\n    Message(role=\"user\", content=user_prompt),\n]\n\nresponse = await model.generate(messages=messages)\n</code></pre>"},{"location":"concepts/prompts/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive names: <code>STRATEGIC_INSTRUCTIONS</code> not <code>PROMPT1</code></li> <li>Document variables: Comment which <code>{variables}</code> the prompt expects</li> <li>Keep prompts focused: One purpose per prompt</li> <li>Test before deploying: Verify new versions in benchmarks</li> <li>Version incrementally: Small changes per version for easier debugging</li> </ol>"},{"location":"examples/analytics/","title":"Debate Analytics","text":"<p>ARTEMIS provides comprehensive analytics for analyzing debate dynamics, tracking momentum shifts, and visualizing results.</p>"},{"location":"examples/analytics/#quick-start","title":"Quick Start","text":"<pre><code>from artemis import Debate, Agent\nfrom artemis.analytics import analyze_debate, export_analytics_report\n\n# Run a debate\ndebate = Debate(\n    topic=\"Should AI development be regulated?\",\n    agents=[\n        Agent(name=\"Advocate\", role=\"Argues for regulation\"),\n        Agent(name=\"Skeptic\", role=\"Argues against regulation\"),\n    ],\n    rounds=3,\n)\nresult = await debate.run()\n\n# Compute analytics\nanalytics = analyze_debate(result)\n\n# Export HTML report with visualizations\nexport_analytics_report(result, \"debate_report.html\")\n</code></pre>"},{"location":"examples/analytics/#analytics-features","title":"Analytics Features","text":""},{"location":"examples/analytics/#momentum-tracking","title":"Momentum Tracking","text":"<p>Track how each agent's performance evolves over the debate:</p> <pre><code>from artemis.analytics import DebateAnalyzer\n\nanalyzer = DebateAnalyzer(\n    transcript=result.transcript,\n    agents=[\"Advocate\", \"Skeptic\"],\n    topic=\"AI Regulation\",\n)\nanalytics = analyzer.analyze()\n\n# Momentum history shows score changes per round\nfor point in analytics.momentum_history:\n    print(f\"Round {point.round}: {point.agent} - \"\n          f\"Score: {point.score:.2f}, Momentum: {point.momentum:+.2f}\")\n</code></pre>"},{"location":"examples/analytics/#turning-points","title":"Turning Points","text":"<p>Detect key moments where the debate dynamics shifted:</p> <pre><code>for tp in analytics.turning_points:\n    print(f\"Round {tp.round}: {tp.agent}\")\n    print(f\"  Significance: {tp.significance:.2f}\")\n    print(f\"  {tp.analysis}\")\n</code></pre>"},{"location":"examples/analytics/#metrics","title":"Metrics","text":"<p>Compute detailed metrics for each agent:</p> <pre><code>from artemis.analytics.metrics import DebateMetricsCalculator\n\ncalc = DebateMetricsCalculator(result.transcript, agents)\n\n# Rebuttal effectiveness - how well agents counter opponents\nprint(calc.rebuttal_effectiveness)\n# {'Advocate': 0.72, 'Skeptic': 0.65}\n\n# Evidence utilization - use of supporting evidence\nprint(calc.evidence_utilization)\n# {'Advocate': 0.85, 'Skeptic': 0.78}\n\n# Argument diversity - variety of argument levels used\nprint(calc.argument_diversity_index)\n# {'Advocate': 0.92, 'Skeptic': 0.71}\n</code></pre>"},{"location":"examples/analytics/#round-by-round-analysis","title":"Round-by-Round Analysis","text":"<p>Get detailed metrics for each round:</p> <pre><code>for rm in calc.get_all_round_metrics():\n    print(f\"Round {rm.round}:\")\n    print(f\"  Scores: {rm.agent_scores}\")\n    print(f\"  Score Changes: {rm.score_delta}\")\n</code></pre>"},{"location":"examples/analytics/#visualizations","title":"Visualizations","text":"<p>ARTEMIS generates pure SVG visualizations with no JavaScript dependencies.</p>"},{"location":"examples/analytics/#score-progression-chart","title":"Score Progression Chart","text":"<p>Shows how scores evolve over rounds with turning point highlights:</p> <pre><code>from artemis.analytics.visualizations import ScoreProgressionChart\n\nchart = ScoreProgressionChart(width=800, height=400)\nsvg = chart.render(\n    round_scores=[\n        {\"Advocate\": 0.65, \"Skeptic\": 0.66},\n        {\"Advocate\": 0.69, \"Skeptic\": 0.64},\n        {\"Advocate\": 0.65, \"Skeptic\": 0.67},\n    ],\n    agents=[\"Advocate\", \"Skeptic\"],\n    highlight_turning_points=[2],  # Highlight round 2\n)\n</code></pre>"},{"location":"examples/analytics/#momentum-chart","title":"Momentum Chart","text":"<p>Visualizes momentum with positive/negative indicators:</p> <pre><code>from artemis.analytics.visualizations import MomentumChart\n\nchart = MomentumChart()\nsvg = chart.render(analytics.momentum_history, agents=[\"Advocate\", \"Skeptic\"])\n</code></pre>"},{"location":"examples/analytics/#jury-vote-chart","title":"Jury Vote Chart","text":"<p>Display final scores as bars or pie chart:</p> <pre><code>from artemis.analytics.visualizations import JuryVoteChart\n\nchart = JuryVoteChart()\n\n# Bar chart\nbar_svg = chart.render_bar({\"Advocate\": 0.61, \"Skeptic\": 0.59})\n\n# Pie chart\npie_svg = chart.render_pie({\"Advocate\": 0.61, \"Skeptic\": 0.59})\n</code></pre>"},{"location":"examples/analytics/#argument-flow-diagram","title":"Argument Flow Diagram","text":"<p>Shows the flow of arguments and rebuttals:</p> <pre><code>from artemis.analytics.visualizations import ArgumentFlowDiagram\n\nchart = ArgumentFlowDiagram(width=800, height=600)\nsvg = chart.render(result.transcript, agents=[\"Advocate\", \"Skeptic\"])\n</code></pre>"},{"location":"examples/analytics/#topic-coverage-heatmap","title":"Topic Coverage Heatmap","text":"<p>Visualizes which topics each agent covered:</p> <pre><code>from artemis.analytics.visualizations import TopicCoverageHeatmap\n\ncoverage = {\n    \"Advocate\": {\"safety\": 5, \"innovation\": 3, \"economics\": 2},\n    \"Skeptic\": {\"freedom\": 4, \"innovation\": 6, \"economics\": 3},\n}\nchart = TopicCoverageHeatmap()\nsvg = chart.render(coverage, agents=[\"Advocate\", \"Skeptic\"])\n</code></pre>"},{"location":"examples/analytics/#html-report-generation","title":"HTML Report Generation","text":"<p>Generate a comprehensive HTML report with analytics and full transcript:</p> <pre><code>from artemis.analytics import export_analytics_report\n\n# Export comprehensive report (default - includes full transcript)\nexport_analytics_report(result, \"report.html\")\n\n# Charts only (no transcript)\nexport_analytics_report(result, \"report.html\", include_transcript=False)\n\n# No charts\nexport_analytics_report(result, \"report.html\", include_charts=False)\n</code></pre> <p>The comprehensive report includes:</p> <p>Analytics Section: - Table of contents with navigation - Key metrics (turning points, lead changes, sway events) - Score progression chart (SVG) - Momentum over time chart (SVG) - Final scores bar chart (SVG) - Turning point analysis</p> <p>Full Transcript Section: - Complete argument content for each turn - Evidence with source, confidence, verification status - Causal links (cause \u2192 effect) - Rebuts/supports relationships - Expandable evaluation details per turn - Inline safety warnings</p> <p>Verdict Section: - Winner with confidence score - Jury reasoning - Final score cards per agent</p>"},{"location":"examples/analytics/#using-with-debateget_analytics","title":"Using with Debate.get_analytics()","text":"<p>You can also compute analytics directly from a Debate instance:</p> <pre><code>debate = Debate(topic=\"...\", agents=[...], rounds=3)\nresult = await debate.run()\n\n# Get analytics directly\nanalytics = debate.get_analytics()\n\nprint(f\"Turning points: {len(analytics.turning_points)}\")\nprint(f\"Lead changes: {analytics.count_lead_changes()}\")\nprint(f\"Final momentum: {analytics.final_momentum}\")\n</code></pre>"},{"location":"examples/analytics/#analytics-data-structures","title":"Analytics Data Structures","text":""},{"location":"examples/analytics/#debateanalytics","title":"DebateAnalytics","text":"<p>The main analytics container:</p> <pre><code>class DebateAnalytics(BaseModel):\n    debate_id: str\n    topic: str\n    agents: list[str]\n    rounds: int\n    momentum_history: list[MomentumPoint]\n    turning_points: list[TurningPoint]\n    round_metrics: list[RoundMetrics]\n    final_momentum: dict[str, float]\n    sway_events: list[SwayEvent]\n\n    # Computed properties\n    def get_leader_per_round(self) -&gt; list[str | None]: ...\n    def count_lead_changes(self) -&gt; int: ...\n</code></pre>"},{"location":"examples/analytics/#momentumpoint","title":"MomentumPoint","text":"<p>Tracks momentum for each agent per round:</p> <pre><code>class MomentumPoint(BaseModel):\n    round: int\n    agent: str\n    score: float           # Raw score (0-1)\n    momentum: float        # Rate of change (-1 to +1)\n    cumulative_advantage: float  # Running advantage\n</code></pre>"},{"location":"examples/analytics/#turningpoint","title":"TurningPoint","text":"<p>Represents a significant shift in debate dynamics:</p> <pre><code>class TurningPoint(BaseModel):\n    round: int\n    turn_id: str\n    agent: str\n    before_momentum: dict[str, float]\n    after_momentum: dict[str, float]\n    significance: float    # 0-1, how significant the shift was\n    analysis: str          # Human-readable explanation\n</code></pre>"},{"location":"examples/analytics/#configuration","title":"Configuration","text":""},{"location":"examples/analytics/#momentumtracker-options","title":"MomentumTracker Options","text":"<pre><code>from artemis.analytics.momentum import MomentumTracker\n\ntracker = MomentumTracker(\n    smoothing_window=2,        # Rounds to smooth over\n    turning_point_threshold=0.3,  # Momentum change threshold\n)\n</code></pre>"},{"location":"examples/analytics/#chart-customization","title":"Chart Customization","text":"<p>All charts support customization:</p> <pre><code>from artemis.analytics.visualizations import ScoreProgressionChart\n\nchart = ScoreProgressionChart(\n    width=800,      # SVG width\n    height=400,     # SVG height\n    padding=50,     # Padding around chart area\n    colors={        # Custom color scheme\n        \"agent_0\": \"#2196F3\",\n        \"agent_1\": \"#FF9800\",\n        \"pro\": \"#4CAF50\",\n        \"con\": \"#F44336\",\n    }\n)\n</code></pre>"},{"location":"examples/audit-logs/","title":"Audit Log Export","text":"<p>ARTEMIS provides a comprehensive audit log system for exporting debate records in multiple formats.</p>"},{"location":"examples/audit-logs/#overview","title":"Overview","text":"<p>The <code>AuditLog</code> class captures every aspect of a debate:</p> <ul> <li>Debate metadata (topic, agents, timestamps)</li> <li>Complete argument transcript</li> <li>Evaluation scores for each turn</li> <li>Safety alerts and interventions</li> <li>Final verdict with reasoning</li> </ul>"},{"location":"examples/audit-logs/#basic-usage","title":"Basic Usage","text":"<pre><code>from artemis.core.debate import Debate\nfrom artemis.core.agent import Agent\nfrom artemis.utils.audit import AuditLog\n\n# Run a debate\nagents = [\n    Agent(name=\"pro\", role=\"Advocate\", model=\"gpt-4o\"),\n    Agent(name=\"con\", role=\"Opponent\", model=\"gpt-4o\"),\n]\n\ndebate = Debate(topic=\"Should we adopt renewable energy?\", agents=agents)\ndebate.assign_positions({\n    \"pro\": \"supports renewable energy adoption\",\n    \"con\": \"advocates for traditional energy sources\",\n})\n\nresult = await debate.run(rounds=3)\n\n# Generate audit log\naudit = AuditLog.from_debate_result(result)\n</code></pre>"},{"location":"examples/audit-logs/#export-formats","title":"Export Formats","text":""},{"location":"examples/audit-logs/#json-export","title":"JSON Export","text":"<p>Full structured data suitable for programmatic processing:</p> <pre><code># Export to file\naudit.to_json(\"audit_output.json\")\n\n# Get as string\njson_string = audit.to_json()\nprint(json_string)\n</code></pre> <p>JSON Structure:</p> <pre><code>{\n  \"debate_id\": \"debate_abc123\",\n  \"topic\": \"Should we adopt renewable energy?\",\n  \"metadata\": {\n    \"started_at\": \"2025-06-28T14:30:00Z\",\n    \"ended_at\": \"2025-06-28T14:35:00Z\",\n    \"agents\": [\"pro\", \"con\"],\n    \"jury_size\": 3,\n    \"safety_monitors\": [\"SandbagDetector\", \"DeceptionMonitor\"]\n  },\n  \"entries\": [\n    {\n      \"timestamp\": \"2025-06-28T14:30:05Z\",\n      \"event_type\": \"argument_generated\",\n      \"agent\": \"pro\",\n      \"round\": 1,\n      \"details\": {\n        \"turn_id\": \"turn_pro_1\",\n        \"level\": \"strategic\",\n        \"content\": \"Full argument content here...\",\n        \"evidence\": [\n          {\n            \"type\": \"study\",\n            \"content\": \"Full evidence content\",\n            \"source\": \"Nature 2024\",\n            \"confidence\": 0.92,\n            \"verified\": true\n          }\n        ],\n        \"causal_links\": [\n          {\"cause\": \"renewable adoption\", \"effect\": \"reduced emissions\", \"strength\": 0.85}\n        ],\n        \"rebuts\": [],\n        \"supports\": [\"turn_pro_0\"],\n        \"ethical_score\": 0.87\n      }\n    },\n    {\n      \"timestamp\": \"2025-06-28T14:30:10Z\",\n      \"event_type\": \"argument_evaluated\",\n      \"agent\": \"pro\",\n      \"round\": 1,\n      \"details\": {\n        \"total_score\": 0.82,\n        \"scores\": {\n          \"logical_coherence\": 0.85,\n          \"evidence_quality\": 0.80,\n          \"causal_reasoning\": 0.78,\n          \"ethical_alignment\": 0.88,\n          \"persuasiveness\": 0.82\n        },\n        \"weights\": {\n          \"logical_coherence\": 0.25,\n          \"evidence_quality\": 0.25,\n          \"causal_reasoning\": 0.20,\n          \"ethical_alignment\": 0.15,\n          \"persuasiveness\": 0.15\n        },\n        \"feedback\": \"Strong argument with credible evidence\",\n        \"strengths\": [\"Clear thesis\", \"Well-sourced claims\"],\n        \"weaknesses\": [\"Could address counterarguments\"]\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/audit-logs/#markdown-export","title":"Markdown Export","text":"<p>Human-readable report format:</p> <pre><code># Export to file\naudit.to_markdown(\"audit_output.md\")\n\n# Get as string\nmd_string = audit.to_markdown()\n</code></pre> <p>Markdown Structure:</p> <pre><code># Debate Audit Log\n\n## Metadata\n\n| Field | Value |\n|-------|-------|\n| Topic | Should we adopt renewable energy? |\n| Started | 2025-06-28 14:30:00 |\n| Duration | 5m 32s |\n\n## Agents\n\n- **pro**: Advocate (gpt-4o)\n- **con**: Opponent (gpt-4o)\n\n## Transcript\n\n### Round 1\n\n**pro** (Strategic):\n&gt; Renewable energy is essential for our future...\n\n*Evidence*: 2 sources cited\n*Evaluation*: 0.82\n\n---\n\n**con** (Strategic):\n&gt; While renewable energy has merits...\n\n*Evidence*: 1 source cited\n*Evaluation*: 0.78\n\n## Verdict\n\n**Winner**: pro\n**Confidence**: 85%\n\nThe jury determined that the pro side presented...\n\n## Safety Alerts\n\nNo safety alerts recorded.\n</code></pre>"},{"location":"examples/audit-logs/#html-export","title":"HTML Export","text":"<p>Comprehensive styled report with full content:</p> <pre><code># Export to file\naudit.to_html(\"audit_output.html\")\n\n# Get as string\nhtml_string = audit.to_html()\n</code></pre> <p>The HTML export includes:</p> <ul> <li>Table of contents with navigation links</li> <li>Full argument content (not truncated)</li> <li>Color-coded agents with distinct border colors</li> <li>Complete evidence with source, confidence, and verification status</li> <li>Causal links visualization (cause \u2192 effect)</li> <li>Expandable evaluation details with criteria breakdown</li> <li>Strengths and weaknesses for each argument</li> <li>Inline safety warnings with severity indicators</li> <li>Verdict section with score cards and reasoning</li> <li>Safety analysis section with all alerts</li> </ul>"},{"location":"examples/audit-logs/#working-with-entries","title":"Working with Entries","text":""},{"location":"examples/audit-logs/#audit-entry-types","title":"Audit Entry Types","text":"Event Type Description <code>argument_generated</code> An agent produced an argument <code>argument_evaluated</code> An argument was scored <code>safety_alert</code> A safety monitor raised an alert <code>round_completed</code> A debate round finished <code>verdict_reached</code> Final verdict was determined"},{"location":"examples/audit-logs/#filtering-entries","title":"Filtering Entries","text":"<pre><code># Get all safety alerts\nalerts = [e for e in audit.entries if e.event_type == \"safety_alert\"]\n\n# Get entries for a specific agent\npro_entries = [e for e in audit.entries if e.agent == \"pro\"]\n\n# Get entries by round\nround_2 = [e for e in audit.entries if e.round == 2]\n</code></pre>"},{"location":"examples/audit-logs/#integration-with-safety-monitors","title":"Integration with Safety Monitors","text":"<p>When safety monitors are active, their alerts are captured in the audit log:</p> <pre><code>from artemis.safety import SandbagDetector, DeceptionMonitor\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[\n        SandbagDetector(sensitivity=0.7).process,\n        DeceptionMonitor(sensitivity=0.6).process,\n    ],\n)\n\nresult = await debate.run()\naudit = AuditLog.from_debate_result(result)\n\n# Check for safety alerts in the audit\nfor entry in audit.entries:\n    if entry.event_type == \"safety_alert\":\n        print(f\"Alert: {entry.details['alert_type']}\")\n        print(f\"Severity: {entry.details['severity']}\")\n        print(f\"Message: {entry.details['message']}\")\n</code></pre>"},{"location":"examples/audit-logs/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom pathlib import Path\n\nfrom artemis.core.debate import Debate\nfrom artemis.core.agent import Agent\nfrom artemis.core.jury import JuryPanel\nfrom artemis.utils.audit import AuditLog\n\nasync def run_and_export():\n    # Create agents with different models\n    agents = [\n        Agent(name=\"pro\", role=\"Advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Opponent\", model=\"claude-sonnet-4-20250514\"),\n    ]\n\n    # Create jury with multiple perspectives\n    jury = JuryPanel(\n        evaluators=3,\n        models=[\"gpt-4o\", \"gemini-2.0-flash\", \"claude-sonnet-4-20250514\"],\n    )\n\n    # Run debate\n    debate = Debate(\n        topic=\"Should AI systems have legal personhood?\",\n        agents=agents,\n        jury=jury,\n    )\n    debate.assign_positions({\n        \"pro\": \"supports AI legal personhood\",\n        \"con\": \"opposes AI legal personhood\",\n    })\n\n    result = await debate.run(rounds=2)\n\n    # Generate audit log\n    audit = AuditLog.from_debate_result(result)\n\n    # Export all formats\n    output_dir = Path(\"audit_output\")\n    output_dir.mkdir(exist_ok=True)\n\n    audit.to_json(output_dir / \"debate_audit.json\")\n    audit.to_markdown(output_dir / \"debate_audit.md\")\n    audit.to_html(output_dir / \"debate_audit.html\")\n\n    print(f\"Audit logs exported to {output_dir}/\")\n    print(f\"Verdict: {result.verdict.decision} ({result.verdict.confidence:.0%})\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_and_export())\n</code></pre>"},{"location":"examples/audit-logs/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Safety Monitors</li> <li>Explore Multi-Agent Debates</li> <li>See Jury Configuration</li> </ul>"},{"location":"examples/basic-debate/","title":"Basic Debate Example","text":"<p>This example demonstrates how to set up and run a basic ARTEMIS debate.</p>"},{"location":"examples/basic-debate/#simple-two-agent-debate","title":"Simple Two-Agent Debate","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_basic_debate():\n    # Create two agents with opposing positions\n    # Note: 'role' is a required parameter for all agents\n    pro_agent = Agent(\n        name=\"proponent\",\n        role=\"Advocate arguing in favor of the proposition\",\n        model=\"gpt-4o\",\n        position=\"supports the proposition\",\n    )\n\n    con_agent = Agent(\n        name=\"opponent\",\n        role=\"Advocate arguing against the proposition\",\n        model=\"gpt-4o\",\n        position=\"opposes the proposition\",\n    )\n\n    # Create the debate\n    # Default rounds is 5, but we use 3 for quicker results\n    debate = Debate(\n        topic=\"Should AI development be regulated by governments?\",\n        agents=[pro_agent, con_agent],\n        rounds=3,\n    )\n\n    # Assign specific positions\n    debate.assign_positions({\n        \"proponent\": \"supports government regulation of AI development\",\n        \"opponent\": \"opposes government regulation of AI development\",\n    })\n\n    # Run the debate\n    result = await debate.run()\n\n    # Print results\n    print(\"=\" * 60)\n    print(f\"Topic: {result.topic}\")\n    print(\"=\" * 60)\n    print()\n\n    # Print transcript\n    print(\"DEBATE TRANSCRIPT\")\n    print(\"-\" * 60)\n    for turn in result.transcript:\n        print(f\"\\n[Round {turn.round}] {turn.agent.upper()}\")\n        print(f\"Level: {turn.argument.level.value}\")\n        print(f\"\\n{turn.argument.content}\")\n        if turn.argument.evidence:\n            print(\"\\nEvidence:\")\n            for e in turn.argument.evidence:\n                print(f\"  - [{e.type}] {e.source}\")\n        print(\"-\" * 40)\n\n    # Print verdict\n    print(\"\\nVERDICT\")\n    print(\"=\" * 60)\n    print(f\"Decision: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"\\nReasoning:\\n{result.verdict.reasoning}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_basic_debate())\n</code></pre>"},{"location":"examples/basic-debate/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.types import DebateConfig\n\nasync def run_configured_debate():\n    # Create configuration with actual DebateConfig fields\n    config = DebateConfig(\n        turn_timeout=60,          # Max seconds per turn\n        round_timeout=300,        # Max seconds per round\n        safety_mode=\"passive\",    # \"off\", \"passive\", or \"active\"\n        halt_on_safety_violation=False,\n    )\n\n    # Create agents - role is required\n    agents = [\n        Agent(\n            name=\"advocate\",\n            role=\"Remote work advocate\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"critic\",\n            role=\"Office work advocate\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    # Create debate with config\n    debate = Debate(\n        topic=\"Should remote work become the default for knowledge workers?\",\n        agents=agents,\n        rounds=4,\n        config=config,\n    )\n\n    debate.assign_positions({\n        \"advocate\": \"argues that remote work should be the default\",\n        \"critic\": \"argues for traditional office work\",\n    })\n\n    result = await debate.run()\n\n    print(f\"Verdict: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Total turns: {len(result.transcript)}\")\n\nasyncio.run(run_configured_debate())\n</code></pre>"},{"location":"examples/basic-debate/#with-custom-jury-panel","title":"With Custom Jury Panel","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_debate_with_jury():\n    # Create a custom jury panel\n    # JuryPanel automatically assigns perspectives (ANALYTICAL, ETHICAL, PRACTICAL, etc.)\n    jury = JuryPanel(\n        evaluators=5,            # Number of jury members\n        model=\"gpt-4o\",          # Model for jury evaluation\n        consensus_threshold=0.7, # Required agreement for consensus\n    )\n\n    agents = [\n        Agent(\n            name=\"pro\",\n            role=\"UBI supporter\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"con\",\n            role=\"UBI skeptic\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"Is universal basic income a viable economic policy?\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"pro\": \"supports universal basic income\",\n        \"con\": \"opposes universal basic income\",\n    })\n\n    result = await debate.run()\n\n    # Show verdict details\n    print(\"JURY VERDICT\")\n    print(\"-\" * 40)\n    print(f\"Decision: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Unanimous: {result.verdict.unanimous}\")\n\n    # Show score breakdown if available\n    if result.verdict.score_breakdown:\n        print(\"\\nScore Breakdown:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\n    # Show dissenting opinions if any\n    if result.verdict.dissenting_opinions:\n        print(\"\\nDissenting Opinions:\")\n        for dissent in result.verdict.dissenting_opinions:\n            print(f\"  {dissent.juror_id} ({dissent.perspective.value}):\")\n            print(f\"    Position: {dissent.position}\")\n            print(f\"    Reasoning: {dissent.reasoning[:100]}...\")\n\n    print(f\"\\nFinal Reasoning:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_debate_with_jury())\n</code></pre>"},{"location":"examples/basic-debate/#multi-agent-debate","title":"Multi-Agent Debate","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_multi_agent_debate():\n    # Three agents with different perspectives\n    agents = [\n        Agent(\n            name=\"economist\",\n            role=\"Economic policy analyst\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"technologist\",\n            role=\"AI safety researcher\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"humanist\",\n            role=\"Social impact advocate\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"How should society prepare for AGI?\",\n        agents=agents,\n        rounds=2,\n    )\n\n    debate.assign_positions({\n        \"economist\": \"focuses on economic implications and market adaptation\",\n        \"technologist\": \"focuses on technical safety and alignment\",\n        \"humanist\": \"focuses on social impact and human values\",\n    })\n\n    result = await debate.run()\n\n    print(\"MULTI-PERSPECTIVE ANALYSIS\")\n    print(\"=\" * 60)\n\n    for turn in result.transcript:\n        print(f\"\\n[{turn.agent.upper()}]\")\n        content = turn.argument.content\n        print(content[:500] + \"...\" if len(content) &gt; 500 else content)\n\n    print(f\"\\n\\nSynthesis: {result.verdict.reasoning}\")\n\nasyncio.run(run_multi_agent_debate())\n</code></pre>"},{"location":"examples/basic-debate/#accessing-argument-structure","title":"Accessing Argument Structure","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def examine_argument_structure():\n    agents = [\n        Agent(\n            name=\"pro\",\n            role=\"Advocate for early CS education\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"con\",\n            role=\"Critic of mandatory CS curriculum\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"Should programming be taught in elementary schools?\",\n        agents=agents,\n        rounds=2,\n    )\n\n    debate.assign_positions({\n        \"pro\": \"supports early programming education\",\n        \"con\": \"opposes mandatory programming in elementary schools\",\n    })\n\n    result = await debate.run()\n\n    # Examine H-L-DAG structure\n    for turn in result.transcript:\n        arg = turn.argument\n\n        print(f\"\\n{'='*60}\")\n        print(f\"Agent: {turn.agent}\")\n        print(f\"Round: {turn.round}\")\n        print(f\"Level: {arg.level.value}\")  # strategic, tactical, or operational\n        print(f\"{'='*60}\")\n\n        print(f\"\\nContent:\\n{arg.content}\")\n\n        if arg.evidence:\n            print(\"\\nEvidence:\")\n            for e in arg.evidence:\n                print(f\"  Type: {e.type}\")  # fact, statistic, quote, example, study, expert_opinion\n                print(f\"  Source: {e.source}\")\n                print(f\"  Content: {e.content}\")\n                print()\n\n        if arg.causal_links:\n            print(\"\\nCausal Links:\")\n            for link in arg.causal_links:\n                print(f\"  {link.cause} --&gt; {link.effect}\")\n                print(f\"  Strength: {link.strength:.2f}\")\n\nasyncio.run(examine_argument_structure())\n</code></pre>"},{"location":"examples/basic-debate/#using-the-scores-api","title":"Using the Scores API","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def track_scores():\n    agents = [\n        Agent(name=\"pro\", role=\"Proponent\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Opponent\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Should we invest in space exploration?\",\n        agents=agents,\n        rounds=3,\n    )\n\n    debate.assign_positions({\n        \"pro\": \"supports increased space exploration funding\",\n        \"con\": \"argues for redirecting funds to Earth-based priorities\",\n    })\n\n    result = await debate.run()\n\n    # Use get_scores() to see aggregate scores\n    scores = debate.get_scores()\n    print(\"Agent Scores:\")\n    for agent, score in scores.items():\n        print(f\"  {agent}: {score:.2f}\")\n\n    # Examine individual turn evaluations\n    print(\"\\nTurn-by-Turn Scores:\")\n    for turn in result.transcript:\n        if turn.evaluation:\n            print(f\"  Round {turn.round} - {turn.agent}: {turn.evaluation.total_score:.2f}\")\n            # Individual criteria scores\n            for criterion, score in turn.evaluation.scores.items():\n                print(f\"    {criterion}: {score:.2f}\")\n\nasyncio.run(track_scores())\n</code></pre>"},{"location":"examples/basic-debate/#next-steps","title":"Next Steps","text":"<ul> <li>Add Safety Monitors to your debate</li> <li>Create a LangGraph Workflow</li> <li>Learn about Core Concepts</li> </ul>"},{"location":"examples/crewai-workflow/","title":"CrewAI Workflow Examples","text":"<p>This example demonstrates comprehensive CrewAI integration patterns using ARTEMIS debates within crew workflows.</p>"},{"location":"examples/crewai-workflow/#basic-research-debate-decide-crew","title":"Basic Research-Debate-Decide Crew","text":"<pre><code>from crewai import Agent, Task, Crew, Process\nfrom artemis.integrations.crewai import ArtemisCrewTool\n\n# Create the debate tool\ndebate_tool = ArtemisCrewTool(\n    model=\"gpt-4o\",\n    default_rounds=3,\n)\n\n# Define agents\nresearcher = Agent(\n    role=\"Research Analyst\",\n    goal=\"Gather comprehensive information on the topic\",\n    backstory=\"\"\"You are an expert researcher who excels at finding\n    relevant data, statistics, and expert opinions on any topic.\n    You provide balanced, factual information.\"\"\",\n    tools=[],  # Add search tools as needed\n    verbose=True,\n)\n\ndebater = Agent(\n    role=\"Debate Analyst\",\n    goal=\"Analyze topics through structured multi-perspective debate\",\n    backstory=\"\"\"You are an expert at using the ARTEMIS debate framework\n    to analyze complex topics. You run debates and synthesize insights\n    from multiple perspectives.\"\"\",\n    tools=[debate_tool.as_crewai_tool()],\n    verbose=True,\n)\n\ndecision_maker = Agent(\n    role=\"Strategic Decision Maker\",\n    goal=\"Make clear recommendations based on analysis\",\n    backstory=\"\"\"You are a senior executive who excels at synthesizing\n    research and debate outcomes into actionable recommendations.\n    You consider risks, opportunities, and stakeholder impact.\"\"\",\n    tools=[],\n    verbose=True,\n)\n\n# Define tasks\nresearch_task = Task(\n    description=\"\"\"\n    Research the following topic: {topic}\n\n    Gather:\n    - Key statistics and data points\n    - Expert opinions from multiple perspectives\n    - Recent developments and trends\n    - Potential risks and opportunities\n\n    Provide a comprehensive research brief.\n    \"\"\",\n    agent=researcher,\n    expected_output=\"A detailed research brief with data and perspectives\",\n)\n\ndebate_task = Task(\n    description=\"\"\"\n    Using the research findings, run a structured debate on: {topic}\n\n    Use the debate tool to:\n    1. Start a 3-round debate\n    2. Analyze the verdict and key arguments\n    3. Summarize the debate outcome\n\n    Focus on identifying the strongest arguments on each side.\n    \"\"\",\n    agent=debater,\n    expected_output=\"Debate verdict with key arguments from each side\",\n    context=[research_task],\n)\n\ndecision_task = Task(\n    description=\"\"\"\n    Based on the research and debate, provide a strategic recommendation.\n\n    Include:\n    - Clear recommendation (go/no-go, or specific approach)\n    - Key supporting arguments\n    - Risks and mitigation strategies\n    - Implementation considerations\n    - Confidence level and caveats\n    \"\"\",\n    agent=decision_maker,\n    expected_output=\"Strategic recommendation with supporting analysis\",\n    context=[research_task, debate_task],\n)\n\n# Create and run crew\ncrew = Crew(\n    agents=[researcher, debater, decision_maker],\n    tasks=[research_task, debate_task, decision_task],\n    process=Process.sequential,\n    verbose=True,\n)\n\nresult = crew.kickoff(inputs={\n    \"topic\": \"Should we migrate our data infrastructure to a lakehouse architecture?\"\n})\n\nprint(result)\n</code></pre>"},{"location":"examples/crewai-workflow/#multi-debate-analysis-crew","title":"Multi-Debate Analysis Crew","text":"<pre><code>from crewai import Agent, Task, Crew, Process\nfrom artemis.integrations.crewai import ArtemisCrewTool\n\ndebate_tool = ArtemisCrewTool(model=\"gpt-4o\")\n\n# Specialized debate agents for different aspects\ntechnical_debater = Agent(\n    role=\"Technical Analyst\",\n    goal=\"Analyze technical feasibility and architecture\",\n    backstory=\"\"\"You focus on technical aspects: performance, scalability,\n    reliability, and engineering complexity. You run debates specifically\n    on technical trade-offs.\"\"\",\n    tools=[debate_tool.as_crewai_tool()],\n)\n\nbusiness_debater = Agent(\n    role=\"Business Analyst\",\n    goal=\"Analyze business impact and ROI\",\n    backstory=\"\"\"You focus on business aspects: costs, revenue impact,\n    time-to-market, and competitive advantage. You run debates on\n    business trade-offs.\"\"\",\n    tools=[debate_tool.as_crewai_tool()],\n)\n\nrisk_debater = Agent(\n    role=\"Risk Analyst\",\n    goal=\"Analyze risks and mitigation strategies\",\n    backstory=\"\"\"You focus on risk aspects: security, compliance,\n    operational risks, and failure modes. You run debates on\n    risk trade-offs.\"\"\",\n    tools=[debate_tool.as_crewai_tool()],\n)\n\nsynthesizer = Agent(\n    role=\"Chief Strategy Officer\",\n    goal=\"Synthesize all analyses into coherent recommendation\",\n    backstory=\"\"\"You are a senior executive who weighs technical,\n    business, and risk factors to make balanced decisions.\"\"\",\n    tools=[],\n)\n\n# Parallel debate tasks\ntech_debate = Task(\n    description=\"\"\"\n    Run a technical debate on: {topic}\n\n    Focus on:\n    - Technical architecture trade-offs\n    - Performance and scalability\n    - Implementation complexity\n    - Technical debt implications\n    \"\"\",\n    agent=technical_debater,\n    expected_output=\"Technical debate verdict and key findings\",\n)\n\nbusiness_debate = Task(\n    description=\"\"\"\n    Run a business-focused debate on: {topic}\n\n    Focus on:\n    - ROI and cost analysis\n    - Time-to-market impact\n    - Competitive implications\n    - Resource requirements\n    \"\"\",\n    agent=business_debater,\n    expected_output=\"Business debate verdict and key findings\",\n)\n\nrisk_debate = Task(\n    description=\"\"\"\n    Run a risk-focused debate on: {topic}\n\n    Focus on:\n    - Security implications\n    - Compliance requirements\n    - Operational risks\n    - Mitigation strategies\n    \"\"\",\n    agent=risk_debater,\n    expected_output=\"Risk debate verdict and key findings\",\n)\n\nsynthesis_task = Task(\n    description=\"\"\"\n    Synthesize the technical, business, and risk analyses.\n\n    Create a comprehensive recommendation that:\n    - Weighs all three perspectives\n    - Identifies conflicts and trade-offs\n    - Provides a clear path forward\n    - Includes contingency plans\n    \"\"\",\n    agent=synthesizer,\n    expected_output=\"Comprehensive strategic recommendation\",\n    context=[tech_debate, business_debate, risk_debate],\n)\n\ncrew = Crew(\n    agents=[technical_debater, business_debater, risk_debater, synthesizer],\n    tasks=[tech_debate, business_debate, risk_debate, synthesis_task],\n    process=Process.sequential,  # Or hierarchical with manager\n)\n\nresult = crew.kickoff(inputs={\n    \"topic\": \"Adopting Kubernetes for our legacy application infrastructure\"\n})\n</code></pre>"},{"location":"examples/crewai-workflow/#multi-agent-debates-with-crewai","title":"Multi-Agent Debates with CrewAI","text":"<pre><code>from crewai import Agent, Task, Crew, Process\nfrom artemis.integrations.crewai import ArtemisCrewTool\n\n# Create tool that supports multi-agent debates\ndebate_tool = ArtemisCrewTool(\n    model=\"gpt-4o\",\n    default_rounds=2,\n)\n\ndebate_facilitator = Agent(\n    role=\"Debate Facilitator\",\n    goal=\"Run multi-perspective debates using ARTEMIS\",\n    backstory=\"\"\"You facilitate debates with multiple agents representing\n    different viewpoints. You use the debate tool to get balanced analysis.\"\"\",\n    tools=[debate_tool.as_crewai_tool()],\n)\n\n# Task using multi-agent debate\nmulti_agent_task = Task(\n    description=\"\"\"\n    Run a multi-agent debate on: {topic}\n\n    Use the debate tool with multiple agents:\n    - agents: [\n        {{\"name\": \"architect\", \"role\": \"System Architect\", \"position\": \"focus on technical excellence\"}},\n        {{\"name\": \"pragmatist\", \"role\": \"Pragmatic Engineer\", \"position\": \"focus on simplicity\"}},\n        {{\"name\": \"security\", \"role\": \"Security Expert\", \"position\": \"focus on security first\"}}\n      ]\n    - rounds: 2\n\n    Summarize the verdict and key arguments from each perspective.\n    \"\"\",\n    agent=debate_facilitator,\n    expected_output=\"Multi-perspective debate analysis\",\n)\n\ncrew = Crew(\n    agents=[debate_facilitator],\n    tasks=[multi_agent_task],\n)\n\nresult = crew.kickoff(inputs={\n    \"topic\": \"How should we implement authentication for our new API?\"\n})\n</code></pre>"},{"location":"examples/crewai-workflow/#hierarchical-crew-with-debate-manager","title":"Hierarchical Crew with Debate Manager","text":"<pre><code>from crewai import Agent, Task, Crew, Process\nfrom langchain_openai import ChatOpenAI\nfrom artemis.integrations.crewai import ArtemisCrewTool\n\ndebate_tool = ArtemisCrewTool(model=\"gpt-4o\", default_rounds=2)\n\n# Worker agents\nfact_checker = Agent(\n    role=\"Fact Checker\",\n    goal=\"Verify claims and gather evidence\",\n    backstory=\"Expert at finding and verifying factual information.\",\n    tools=[],  # search tools\n)\n\nperspective_analyst = Agent(\n    role=\"Perspective Analyst\",\n    goal=\"Identify and articulate different viewpoints\",\n    backstory=\"Expert at understanding different stakeholder perspectives.\",\n    tools=[],\n)\n\ndebate_runner = Agent(\n    role=\"Debate Facilitator\",\n    goal=\"Run structured debates using ARTEMIS\",\n    backstory=\"Expert at facilitating productive debates.\",\n    tools=[debate_tool.as_crewai_tool()],\n)\n\nreport_writer = Agent(\n    role=\"Report Writer\",\n    goal=\"Create clear, actionable reports\",\n    backstory=\"Expert at synthesizing complex analysis into clear reports.\",\n    tools=[],\n)\n\n# Tasks\ngather_facts = Task(\n    description=\"Gather verified facts about: {topic}\",\n    agent=fact_checker,\n    expected_output=\"Verified facts and data points\",\n)\n\nidentify_perspectives = Task(\n    description=\"Identify key stakeholder perspectives on: {topic}\",\n    agent=perspective_analyst,\n    expected_output=\"List of stakeholder perspectives and their concerns\",\n)\n\nrun_debate = Task(\n    description=\"\"\"\n    Run a structured debate on {topic} incorporating:\n    - The verified facts\n    - The identified stakeholder perspectives\n\n    Ensure the debate addresses all major viewpoints.\n    \"\"\",\n    agent=debate_runner,\n    expected_output=\"Debate verdict with comprehensive analysis\",\n    context=[gather_facts, identify_perspectives],\n)\n\nwrite_report = Task(\n    description=\"\"\"\n    Write an executive report on the debate findings.\n\n    Include:\n    - Executive summary\n    - Key findings from each perspective\n    - Debate verdict and confidence\n    - Recommended actions\n    - Appendix with detailed arguments\n    \"\"\",\n    agent=report_writer,\n    expected_output=\"Executive report with recommendations\",\n    context=[run_debate],\n)\n\n# Hierarchical crew with manager\ncrew = Crew(\n    agents=[fact_checker, perspective_analyst, debate_runner, report_writer],\n    tasks=[gather_facts, identify_perspectives, run_debate, write_report],\n    process=Process.hierarchical,\n    manager_llm=ChatOpenAI(model=\"gpt-4o\"),\n)\n\nresult = crew.kickoff(inputs={\n    \"topic\": \"Implementing a four-day work week company-wide\"\n})\n</code></pre>"},{"location":"examples/crewai-workflow/#competitive-analysis-crew","title":"Competitive Analysis Crew","text":"<pre><code>from crewai import Agent, Task, Crew, Process\nfrom artemis.integrations.crewai import ArtemisCrewTool\n\ndebate_tool = ArtemisCrewTool(model=\"gpt-4o\")\n\n# Agents for competitive analysis\nmarket_analyst = Agent(\n    role=\"Market Analyst\",\n    goal=\"Analyze market dynamics and competitive landscape\",\n    backstory=\"Expert in market research and competitive intelligence.\",\n    tools=[],  # market research tools\n)\n\nwar_gamer = Agent(\n    role=\"War Gaming Facilitator\",\n    goal=\"Facilitate competitive war games through debate\",\n    backstory=\"\"\"You run debates that simulate competitive scenarios\n    to stress-test strategies.\"\"\",\n    tools=[debate_tool.as_crewai_tool()],\n)\n\nstrategist = Agent(\n    role=\"Internal Strategist\",\n    goal=\"Develop and defend our strategic options\",\n    backstory=\"\"\"You develop strategic options for our company and\n    can argue their merits in competitive context.\"\"\",\n    tools=[],\n)\n\n# Tasks\nmarket_analysis = Task(\n    description=\"\"\"\n    Analyze the competitive landscape for: {market}\n\n    Identify:\n    - Key competitors and their strategies\n    - Market trends and dynamics\n    - Competitive advantages and weaknesses\n    - Emerging threats and opportunities\n    \"\"\",\n    agent=market_analyst,\n    expected_output=\"Competitive landscape analysis\",\n)\n\nstrategy_debate = Task(\n    description=\"\"\"\n    Run a war-gaming debate simulating competitive scenarios.\n\n    Debate question: \"What is the optimal strategy for {market}?\"\n\n    Consider:\n    - Our current strategy vs alternatives\n    - Competitor responses to our moves\n    - Market evolution scenarios\n    \"\"\",\n    agent=war_gamer,\n    expected_output=\"War game debate results with strategic insights\",\n    context=[market_analysis],\n)\n\nstrategy_recommendation = Task(\n    description=\"\"\"\n    Based on the war game results, develop a strategic recommendation.\n\n    Include:\n    - Recommended strategy\n    - Competitive positioning\n    - Key moves and timing\n    - Response plans for competitor actions\n    \"\"\",\n    agent=strategist,\n    expected_output=\"Strategic recommendation with competitive playbook\",\n    context=[market_analysis, strategy_debate],\n)\n\ncrew = Crew(\n    agents=[market_analyst, war_gamer, strategist],\n    tasks=[market_analysis, strategy_debate, strategy_recommendation],\n    process=Process.sequential,\n)\n\nresult = crew.kickoff(inputs={\n    \"market\": \"enterprise AI development tools\"\n})\n</code></pre>"},{"location":"examples/crewai-workflow/#product-decision-crew","title":"Product Decision Crew","text":"<pre><code>from crewai import Agent, Task, Crew, Process\nfrom artemis.integrations.crewai import ArtemisCrewTool\n\n# Debate tool\ndebate_tool = ArtemisCrewTool(\n    model=\"gpt-4o\",\n    default_rounds=2,\n)\n\n# Product team agents\nproduct_manager = Agent(\n    role=\"Product Manager\",\n    goal=\"Define product requirements and prioritize features\",\n    backstory=\"Expert at understanding customer needs and market fit.\",\n    tools=[],\n)\n\nengineering_lead = Agent(\n    role=\"Engineering Lead\",\n    goal=\"Assess technical feasibility and effort\",\n    backstory=\"Expert at technical architecture and estimation.\",\n    tools=[],\n)\n\ndesign_lead = Agent(\n    role=\"Design Lead\",\n    goal=\"Ensure great user experience\",\n    backstory=\"Expert at user-centered design and usability.\",\n    tools=[],\n)\n\nfeature_debater = Agent(\n    role=\"Feature Analyst\",\n    goal=\"Analyze feature trade-offs through debate\",\n    backstory=\"\"\"You run debates to analyze feature decisions from\n    multiple angles: user value, technical complexity, and business impact.\"\"\",\n    tools=[debate_tool.as_crewai_tool()],\n)\n\n# Tasks for feature prioritization\ngather_requirements = Task(\n    description=\"\"\"\n    Gather requirements for: {feature}\n\n    Document:\n    - User stories and use cases\n    - Success metrics\n    - Dependencies and constraints\n    \"\"\",\n    agent=product_manager,\n    expected_output=\"Feature requirements document\",\n)\n\ntechnical_assessment = Task(\n    description=\"\"\"\n    Assess technical feasibility of: {feature}\n\n    Evaluate:\n    - Architecture impact\n    - Development effort\n    - Technical risks\n    - Dependencies\n    \"\"\",\n    agent=engineering_lead,\n    expected_output=\"Technical feasibility assessment\",\n    context=[gather_requirements],\n)\n\nux_assessment = Task(\n    description=\"\"\"\n    Assess UX implications of: {feature}\n\n    Evaluate:\n    - User journey impact\n    - Usability considerations\n    - Design effort\n    - Accessibility requirements\n    \"\"\",\n    agent=design_lead,\n    expected_output=\"UX assessment\",\n    context=[gather_requirements],\n)\n\nfeature_debate = Task(\n    description=\"\"\"\n    Run a prioritization debate for: {feature}\n\n    Debate question: \"Should we prioritize this feature for the next quarter?\"\n\n    Consider inputs from:\n    - Product requirements\n    - Technical assessment\n    - UX assessment\n\n    Run a 2-round debate and provide recommendation.\n    \"\"\",\n    agent=feature_debater,\n    expected_output=\"Feature prioritization recommendation with debate analysis\",\n    context=[gather_requirements, technical_assessment, ux_assessment],\n)\n\ncrew = Crew(\n    agents=[product_manager, engineering_lead, design_lead, feature_debater],\n    tasks=[gather_requirements, technical_assessment, ux_assessment, feature_debate],\n    process=Process.sequential,\n)\n\nresult = crew.kickoff(inputs={\n    \"feature\": \"AI-powered search with natural language queries\"\n})\n</code></pre>"},{"location":"examples/crewai-workflow/#async-crew-execution","title":"Async Crew Execution","text":"<pre><code>import asyncio\nfrom crewai import Agent, Task, Crew, Process\nfrom artemis.integrations.crewai import ArtemisCrewTool\n\nasync def run_async_crew():\n    debate_tool = ArtemisCrewTool(model=\"gpt-4o\")\n\n    analyst = Agent(\n        role=\"Analyst\",\n        goal=\"Analyze topics through structured debate\",\n        tools=[debate_tool.as_crewai_tool()],\n    )\n\n    task = Task(\n        description=\"Run a debate on: {topic}\",\n        agent=analyst,\n        expected_output=\"Debate verdict and analysis\",\n    )\n\n    crew = Crew(\n        agents=[analyst],\n        tasks=[task],\n    )\n\n    # Run multiple analyses concurrently\n    topics = [\n        \"Should we adopt GraphQL over REST?\",\n        \"Should we move to a monorepo?\",\n        \"Should we implement feature flags?\",\n    ]\n\n    tasks = [\n        crew.kickoff_async(inputs={\"topic\": topic})\n        for topic in topics\n    ]\n\n    results = await asyncio.gather(*tasks)\n\n    for topic, result in zip(topics, results):\n        print(f\"\\n{topic}\")\n        print(f\"Result: {result}\")\n\nasyncio.run(run_async_crew())\n</code></pre>"},{"location":"examples/crewai-workflow/#using-debateanalyzer-for-complex-decisions","title":"Using DebateAnalyzer for Complex Decisions","text":"<pre><code>from artemis.integrations.crewai import DebateAnalyzer\n\n# Use the high-level analyzer for multi-faceted decisions\nanalyzer = DebateAnalyzer(\n    model=\"gpt-4o\",\n    rounds_per_debate=2,\n)\n\n# Single comprehensive analysis\nresult = analyzer.analyze_decision(\n    decision=\"Should we migrate to cloud infrastructure?\",\n)\n\nprint(f\"Verdict: {result['overall_verdict']}\")\nprint(f\"Confidence: {result['confidence']:.0%}\")\nprint(f\"Recommendation: {result['recommendation']}\")\n\n# Multi-aspect analysis\nresult = analyzer.analyze_decision(\n    decision=\"Should we adopt a new CRM platform?\",\n    aspects=[\"cost\", \"functionality\", \"integration\", \"user_adoption\"],\n)\n\nprint(\"\\nMulti-Aspect Analysis:\")\nprint(f\"Decision: {result['decision']}\")\nprint(f\"Overall Verdict: {result['overall_verdict']}\")\nprint(f\"Verdict Distribution: {result['verdict_distribution']}\")\n\nfor aspect, data in result[\"aspect_results\"].items():\n    print(f\"\\n{aspect.upper()}:\")\n    print(f\"  Verdict: {data['verdict']}\")\n    print(f\"  Confidence: {data['confidence']:.0%}\")\n</code></pre>"},{"location":"examples/crewai-workflow/#next-steps","title":"Next Steps","text":"<ul> <li>See Basic Debate to understand ARTEMIS fundamentals</li> <li>Explore Multi-Agent Debates for complex scenarios</li> <li>Add Safety Monitors to your crews</li> </ul>"},{"location":"examples/custom-jury/","title":"Custom Jury Examples","text":"<p>This example demonstrates how to create custom jury configurations for specialized evaluation needs.</p>"},{"location":"examples/custom-jury/#basic-jury-panel","title":"Basic Jury Panel","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_with_custom_jury():\n    # Create a jury panel with 5 evaluators\n    # JuryPanel automatically assigns perspectives from the JuryPerspective enum:\n    # ANALYTICAL, ETHICAL, PRACTICAL, ADVERSARIAL, SYNTHESIZING\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.7,  # 70% agreement needed for consensus\n    )\n\n    agents = [\n        Agent(\n            name=\"pro\",\n            role=\"Framework migration advocate\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"con\",\n            role=\"Current framework defender\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"Should we rebuild our app using a new framework?\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"pro\": \"advocates for rebuilding with new framework\",\n        \"con\": \"advocates for maintaining current framework\",\n    })\n\n    result = await debate.run()\n\n    print(\"JURY PANEL VERDICT\")\n    print(\"=\" * 60)\n    print(f\"\\nDecision: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Unanimous: {result.verdict.unanimous}\")\n\n    # Show score breakdown\n    if result.verdict.score_breakdown:\n        print(\"\\nScore Breakdown:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\n    # Show dissenting opinions if any\n    if result.verdict.dissenting_opinions:\n        print(\"\\nDissenting Opinions:\")\n        for dissent in result.verdict.dissenting_opinions:\n            print(f\"\\n  {dissent.juror_id} ({dissent.perspective.value}):\")\n            print(f\"    Position: {dissent.position}\")\n            print(f\"    Reasoning: {dissent.reasoning[:150]}...\")\n\n    print(f\"\\nReasoning:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_with_custom_jury())\n</code></pre>"},{"location":"examples/custom-jury/#different-panel-sizes","title":"Different Panel Sizes","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def compare_panel_sizes():\n    \"\"\"Compare verdicts with different jury sizes.\"\"\"\n    topic = \"Should AI-generated content require disclosure?\"\n\n    results = {}\n\n    for size in [3, 5, 7]:\n        jury = JuryPanel(\n            evaluators=size,\n            model=\"gpt-4o\",\n            consensus_threshold=0.6,\n        )\n\n        agents = [\n            Agent(name=\"pro\", role=\"Disclosure advocate\", model=\"gpt-4o\"),\n            Agent(name=\"con\", role=\"Free speech advocate\", model=\"gpt-4o\"),\n        ]\n\n        debate = Debate(\n            topic=topic,\n            agents=agents,\n            rounds=2,\n            jury=jury,\n        )\n\n        debate.assign_positions({\n            \"pro\": \"supports mandatory AI content disclosure\",\n            \"con\": \"opposes mandatory disclosure requirements\",\n        })\n\n        result = await debate.run()\n\n        results[size] = {\n            \"decision\": result.verdict.decision,\n            \"confidence\": result.verdict.confidence,\n            \"unanimous\": result.verdict.unanimous,\n            \"dissent_count\": len(result.verdict.dissenting_opinions),\n        }\n\n    print(\"JURY SIZE COMPARISON\")\n    print(\"=\" * 60)\n    for size, data in results.items():\n        print(f\"\\n{size} Jurors:\")\n        print(f\"  Decision: {data['decision']}\")\n        print(f\"  Confidence: {data['confidence']:.0%}\")\n        print(f\"  Unanimous: {data['unanimous']}\")\n        print(f\"  Dissenting: {data['dissent_count']}\")\n\nasyncio.run(compare_panel_sizes())\n</code></pre>"},{"location":"examples/custom-jury/#custom-criteria","title":"Custom Criteria","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_with_custom_criteria():\n    # Define custom evaluation criteria\n    medical_criteria = [\n        \"clinical_evidence\",\n        \"patient_safety\",\n        \"ethical_alignment\",\n        \"implementation_feasibility\",\n        \"cost_effectiveness\",\n    ]\n\n    jury = JuryPanel(\n        evaluators=5,\n        criteria=medical_criteria,\n        model=\"gpt-4o\",\n        consensus_threshold=0.75,  # Higher threshold for medical decisions\n    )\n\n    agents = [\n        Agent(\n            name=\"advocate\",\n            role=\"Medical technology advocate\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"skeptic\",\n            role=\"Clinical safety reviewer\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"Should AI diagnostic tools be used for primary cancer screening?\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"advocate\": \"supports AI-first cancer screening approach\",\n        \"skeptic\": \"argues for human-led screening with AI assistance only\",\n    })\n\n    result = await debate.run()\n\n    print(\"MEDICAL PANEL VERDICT\")\n    print(\"=\" * 60)\n    print(f\"\\nDecision: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Unanimous: {result.verdict.unanimous}\")\n\n    if result.verdict.score_breakdown:\n        print(\"\\nAgent Scores (across all criteria):\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\nasyncio.run(run_with_custom_criteria())\n</code></pre>"},{"location":"examples/custom-jury/#high-consensus-threshold","title":"High Consensus Threshold","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_high_consensus_debate():\n    # High threshold for consensus - useful for critical decisions\n    jury = JuryPanel(\n        evaluators=7,\n        model=\"gpt-4o\",\n        consensus_threshold=0.85,  # Require 85% agreement\n    )\n\n    agents = [\n        Agent(name=\"prosecution\", role=\"Security advocate\", model=\"gpt-4o\"),\n        Agent(name=\"defense\", role=\"Privacy advocate\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Should encryption backdoors be mandated for law enforcement access?\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"prosecution\": \"argues for mandatory encryption backdoors for national security\",\n        \"defense\": \"argues against backdoors for privacy and security reasons\",\n    })\n\n    result = await debate.run()\n\n    print(\"HIGH-CONSENSUS VERDICT\")\n    print(\"=\" * 60)\n    print(f\"\\nDecision: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Consensus threshold: 85%\")\n    print(f\"Unanimous: {result.verdict.unanimous}\")\n\n    if result.verdict.confidence &lt; 0.85:\n        print(\"\\nNote: Consensus threshold not met - decision may require further deliberation\")\n\n    print(f\"\\nReasoning:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_high_consensus_debate())\n</code></pre>"},{"location":"examples/custom-jury/#accessing-jury-perspectives","title":"Accessing Jury Perspectives","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\nfrom artemis.core.types import JuryPerspective\n\nasync def examine_perspectives():\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n    )\n\n    # Show what perspectives are assigned\n    print(\"JURY PERSPECTIVES\")\n    print(\"-\" * 40)\n    perspectives = jury.get_perspectives()\n    for i, perspective in enumerate(perspectives):\n        print(f\"Juror {i}: {perspective.value}\")\n        # JuryPerspective values: analytical, ethical, practical, adversarial, synthesizing\n\n    agents = [\n        Agent(name=\"pro\", role=\"Tech advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Tech skeptic\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Should we adopt microservices architecture?\",\n        agents=agents,\n        rounds=2,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"pro\": \"supports microservices adoption\",\n        \"con\": \"supports monolithic architecture\",\n    })\n\n    result = await debate.run()\n\n    print(f\"\\nVerdict: {result.verdict.decision}\")\n\n    # Examine dissenting opinions by perspective\n    if result.verdict.dissenting_opinions:\n        print(\"\\nDissents by Perspective:\")\n        for dissent in result.verdict.dissenting_opinions:\n            print(f\"  {dissent.perspective.value}: preferred {dissent.position}\")\n            print(f\"    Score deviation: {dissent.score_deviation:.2f}\")\n\nasyncio.run(examine_perspectives())\n</code></pre>"},{"location":"examples/custom-jury/#investment-committee-pattern","title":"Investment Committee Pattern","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_investment_debate():\n    # Financial/investment evaluation criteria\n    investment_criteria = [\n        \"financial_viability\",\n        \"risk_assessment\",\n        \"strategic_alignment\",\n        \"market_opportunity\",\n        \"execution_capability\",\n    ]\n\n    jury = JuryPanel(\n        evaluators=5,\n        criteria=investment_criteria,\n        model=\"gpt-4o\",\n        consensus_threshold=0.7,\n    )\n\n    agents = [\n        Agent(name=\"bull\", role=\"Investment advocate\", model=\"gpt-4o\"),\n        Agent(name=\"bear\", role=\"Risk analyst\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Should we acquire this AI startup at a $500M valuation?\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"bull\": \"argues for the acquisition at current valuation\",\n        \"bear\": \"argues against the acquisition or for lower valuation\",\n    })\n\n    result = await debate.run()\n\n    print(\"INVESTMENT COMMITTEE DECISION\")\n    print(\"=\" * 60)\n    print(f\"\\nDecision: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Unanimous: {result.verdict.unanimous}\")\n\n    if result.verdict.score_breakdown:\n        print(\"\\nAgent Scores:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            status = \"RECOMMEND\" if score &gt; 6.0 else \"CAUTION\"\n            print(f\"  {agent}: {score:.2f} [{status}]\")\n\n    print(f\"\\nRationale:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_investment_debate())\n</code></pre>"},{"location":"examples/custom-jury/#academic-peer-review-pattern","title":"Academic Peer Review Pattern","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_academic_debate():\n    # Academic peer review criteria\n    academic_criteria = [\n        \"methodology_rigor\",\n        \"theoretical_contribution\",\n        \"evidence_quality\",\n        \"logical_coherence\",\n        \"practical_impact\",\n    ]\n\n    jury = JuryPanel(\n        evaluators=3,  # Typical peer review panel\n        criteria=academic_criteria,\n        model=\"gpt-4o\",\n        consensus_threshold=0.8,  # High threshold - all reviewers should agree\n    )\n\n    agents = [\n        Agent(name=\"proponent\", role=\"Research claim advocate\", model=\"gpt-4o\"),\n        Agent(name=\"challenger\", role=\"Research claim skeptic\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Does the evidence support that transformer models exhibit emergent reasoning?\",\n        agents=agents,\n        rounds=2,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"proponent\": \"argues transformers exhibit genuine emergent reasoning capabilities\",\n        \"challenger\": \"argues apparent reasoning is sophisticated pattern matching, not emergence\",\n    })\n\n    result = await debate.run()\n\n    print(\"PEER REVIEW DECISION\")\n    print(\"=\" * 60)\n    print(f\"\\nDecision: {result.verdict.decision}\")\n    print(f\"Consensus: {result.verdict.confidence:.0%}\")\n    print(f\"Unanimous: {result.verdict.unanimous}\")\n\n    # In academic review, dissent is important\n    if result.verdict.dissenting_opinions:\n        print(\"\\nReviewer Concerns (Dissents):\")\n        for i, dissent in enumerate(result.verdict.dissenting_opinions):\n            print(f\"\\nReviewer {i+1} ({dissent.perspective.value}):\")\n            print(f\"  Position: {dissent.position}\")\n            print(f\"  Comments: {dissent.reasoning[:200]}...\")\n    else:\n        print(\"\\nAll reviewers reached consensus.\")\n\nasyncio.run(run_academic_debate())\n</code></pre>"},{"location":"examples/custom-jury/#comparing-jury-vs-no-jury","title":"Comparing Jury vs No Jury","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def compare_jury_modes():\n    topic = \"Should programming be taught in elementary schools?\"\n\n    # Without custom jury (uses default evaluation)\n    agents_no_jury = [\n        Agent(name=\"pro\", role=\"Education advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Education traditionalist\", model=\"gpt-4o\"),\n    ]\n\n    debate_no_jury = Debate(\n        topic=topic,\n        agents=agents_no_jury,\n        rounds=2,\n    )\n    debate_no_jury.assign_positions({\n        \"pro\": \"supports early programming education\",\n        \"con\": \"opposes mandatory programming in elementary schools\",\n    })\n\n    result_no_jury = await debate_no_jury.run()\n\n    # With custom jury\n    jury = JuryPanel(evaluators=5, consensus_threshold=0.7)\n\n    agents_jury = [\n        Agent(name=\"pro\", role=\"Education advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Education traditionalist\", model=\"gpt-4o\"),\n    ]\n\n    debate_jury = Debate(\n        topic=topic,\n        agents=agents_jury,\n        rounds=2,\n        jury=jury,\n    )\n    debate_jury.assign_positions({\n        \"pro\": \"supports early programming education\",\n        \"con\": \"opposes mandatory programming in elementary schools\",\n    })\n\n    result_jury = await debate_jury.run()\n\n    print(\"COMPARISON: Default vs Custom Jury\")\n    print(\"=\" * 60)\n\n    print(\"\\nDefault Evaluation:\")\n    print(f\"  Decision: {result_no_jury.verdict.decision}\")\n    print(f\"  Confidence: {result_no_jury.verdict.confidence:.0%}\")\n\n    print(\"\\nCustom Jury Panel (5 evaluators):\")\n    print(f\"  Decision: {result_jury.verdict.decision}\")\n    print(f\"  Confidence: {result_jury.verdict.confidence:.0%}\")\n    print(f\"  Unanimous: {result_jury.verdict.unanimous}\")\n    print(f\"  Dissenting: {len(result_jury.verdict.dissenting_opinions)}\")\n\nasyncio.run(compare_jury_modes())\n</code></pre>"},{"location":"examples/custom-jury/#next-steps","title":"Next Steps","text":"<ul> <li>See Multi-Agent Debates for complex agent setups</li> <li>Explore Ethical Dilemmas with ethics-focused evaluation</li> <li>Learn about Enterprise Decisions with business criteria</li> </ul>"},{"location":"examples/enterprise-decisions/","title":"Enterprise Decisions","text":"<p>This example demonstrates using ARTEMIS for real-world enterprise decision-making scenarios.</p>"},{"location":"examples/enterprise-decisions/#technology-stack-decision","title":"Technology Stack Decision","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_tech_stack_debate():\n    # Enterprise jury with high consensus for technology decisions\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.7,\n    )\n\n    agents = [\n        Agent(\n            name=\"rust_advocate\",\n            role=\"Systems architect advocating for Rust adoption\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"go_advocate\",\n            role=\"Platform engineer advocating for Go adoption\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"python_advocate\",\n            role=\"Engineering lead advocating for Python optimization\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        We need to choose a primary language for our new microservices platform.\n        Considerations:\n        - Current team: 50 Python developers, 10 Go developers\n        - Requirements: High performance, cloud-native, 5-year horizon\n        - Scale: 10M daily active users, sub-100ms latency requirements\n        \"\"\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"rust_advocate\": \"\"\"\n        Advocates for Rust: Memory safety without GC, best-in-class performance,\n        growing ecosystem, prevents entire classes of bugs. Worth the learning\n        curve for long-term benefits. Companies like Discord, Cloudflare use it.\n        \"\"\",\n        \"go_advocate\": \"\"\"\n        Advocates for Go: Simple, fast compilation, excellent concurrency,\n        proven at scale (Google, Uber, Dropbox). Easy hiring, quick onboarding.\n        Cloud-native DNA. Good enough performance for most use cases.\n        \"\"\",\n        \"python_advocate\": \"\"\"\n        Advocates for Python with optimization: Team already knows it, massive\n        ecosystem, use PyPy/Cython for hot paths, proven at Instagram scale.\n        Minimize retraining, leverage existing expertise. Microservices allow\n        targeted optimization.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"TECHNOLOGY STACK DECISION\")\n    print(\"=\" * 60)\n    print(f\"\\nRecommendation: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Unanimous: {result.verdict.unanimous}\")\n\n    if result.verdict.score_breakdown:\n        print(\"\\nAGENT SCORES:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\n    print(f\"\\nRATIONALE:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_tech_stack_debate())\n</code></pre>"},{"location":"examples/enterprise-decisions/#build-vs-buy-decision","title":"Build vs Buy Decision","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_build_vs_buy_debate():\n    # Executive-level jury for strategic decisions\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.7,\n    )\n\n    agents = [\n        Agent(\n            name=\"build\",\n            role=\"Engineering advocate for building in-house\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"buy\",\n            role=\"Operations advocate for purchasing solution\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"hybrid\",\n            role=\"Strategy advocate for hybrid approach\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        Should we build our own customer data platform (CDP) or buy an\n        existing solution?\n\n        Context:\n        - Budget: $2M first year, $500K/year ongoing\n        - Timeline: Need solution in 9 months\n        - Team: 8 senior engineers available\n        - Data volume: 50M customer profiles, 1B events/month\n        - Current stack: AWS, Snowflake, dbt\n        \"\"\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"build\": \"\"\"\n        Advocates building in-house: Full control, no vendor lock-in,\n        tailored to needs, builds internal capability, long-term cost\n        savings. Can leverage existing Snowflake investment. IP ownership.\n        \"\"\",\n        \"buy\": \"\"\"\n        Advocates buying (Segment, mParticle, etc.): Faster time-to-market,\n        proven at scale, ongoing innovation, focus on core business,\n        predictable costs. Let experts handle infrastructure.\n        \"\"\",\n        \"hybrid\": \"\"\"\n        Advocates hybrid approach: Buy core CDP, build custom integrations\n        and ML layer. Best of both worlds. Start with vendor, plan for\n        strategic components in-house over time.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"BUILD VS BUY DECISION: CDP\")\n    print(\"=\" * 60)\n    print(f\"\\nDecision: {result.verdict.decision}\")\n    print(f\"Executive Confidence: {result.verdict.confidence:.0%}\")\n\n    if result.verdict.score_breakdown:\n        print(\"\\nAPPROACH SCORES:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            indicator = \"BUILD\" if \"build\" in agent else \"BUY\" if \"buy\" in agent else \"HYBRID\"\n            print(f\"  {indicator}: {score:.2f}\")\n\n    print(f\"\\nEXECUTIVE SUMMARY:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_build_vs_buy_debate())\n</code></pre>"},{"location":"examples/enterprise-decisions/#vendor-selection","title":"Vendor Selection","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_vendor_selection_debate():\n    # Procurement-focused jury\n    jury = JuryPanel(\n        evaluators=3,\n        model=\"gpt-4o\",\n        consensus_threshold=0.7,\n    )\n\n    agents = [\n        Agent(\n            name=\"vendor_a\",\n            role=\"Advocate for Datadog platform\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"vendor_b\",\n            role=\"Advocate for New Relic platform\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"vendor_c\",\n            role=\"Advocate for Grafana Cloud platform\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        Select a cloud observability platform for our infrastructure.\n\n        Requirements:\n        - APM, logs, metrics, and traces unified\n        - Support for Kubernetes and serverless\n        - 500+ services, 10TB logs/day\n        - Budget: $300K/year\n        - Current: Self-hosted ELK + Prometheus (pain points: scale, correlation)\n\n        Finalists: Datadog, New Relic, Grafana Cloud\n        \"\"\",\n        agents=agents,\n        rounds=2,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"vendor_a\": \"\"\"\n        Advocates for Datadog: Market leader, best-in-class UX, unified platform,\n        strong APM. Higher cost but comprehensive. Used by similar companies.\n        ML-powered insights. Excellent K8s support.\n        \"\"\",\n        \"vendor_b\": \"\"\"\n        Advocates for New Relic: Competitive pricing (consumption-based),\n        strong APM heritage, recent platform improvements. Good value for\n        budget. Full-stack observability. Entity-centric model.\n        \"\"\",\n        \"vendor_c\": \"\"\"\n        Advocates for Grafana Cloud: Best value, open-source foundation,\n        no vendor lock-in (LGTM stack), team familiarity with Grafana.\n        Flexible pricing. Strong community. Pairs with existing Prometheus.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"VENDOR SELECTION: OBSERVABILITY PLATFORM\")\n    print(\"=\" * 60)\n    print(f\"\\nSelected Vendor: {result.verdict.decision}\")\n    print(f\"Selection Confidence: {result.verdict.confidence:.0%}\")\n\n    if result.verdict.score_breakdown:\n        print(\"\\nEVALUATION SCORES:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\n    print(f\"\\nSELECTION RATIONALE:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_vendor_selection_debate())\n</code></pre>"},{"location":"examples/enterprise-decisions/#organizational-restructuring","title":"Organizational Restructuring","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\nfrom artemis.safety import MonitorMode, EthicsGuard, EthicsConfig\n\nasync def run_reorg_debate():\n    # Ethics guard for sensitive HR decisions\n    ethics_config = EthicsConfig(\n        harmful_content_threshold=0.5,\n        bias_threshold=0.4,\n        fairness_threshold=0.4,\n        enabled_checks=[\"harmful_content\", \"bias\", \"fairness\"],\n    )\n\n    ethics_guard = EthicsGuard(\n        mode=MonitorMode.PASSIVE,\n        config=ethics_config,\n    )\n\n    # Leadership jury\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.7,\n    )\n\n    agents = [\n        Agent(\n            name=\"consolidation\",\n            role=\"COO advocating for team consolidation\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"expansion\",\n            role=\"CPO advocating for team expansion\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"transformation\",\n            role=\"CTO advocating for platform team model\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        How should we restructure our engineering organization?\n\n        Current state:\n        - 400 engineers across 12 product teams\n        - Siloed teams, duplicate efforts\n        - Slow cross-team coordination\n        - 30% growth planned next year\n\n        Options: Consolidate, Expand, or Transform to platform model\n        \"\"\",\n        agents=agents,\n        rounds=2,\n        jury=jury,\n        safety_monitors=[ethics_guard.process],\n    )\n\n    debate.assign_positions({\n        \"consolidation\": \"\"\"\n        Advocates for consolidation: Merge similar teams, reduce management\n        layers, create shared services. Eliminate duplication. More efficient\n        use of resources. Clear ownership. May involve some reduction.\n        \"\"\",\n        \"expansion\": \"\"\"\n        Advocates for expansion: Add new teams for growth areas, hire team\n        leads, maintain team autonomy. Support growth plans. Preserve culture.\n        Address coordination through better tooling, not restructuring.\n        \"\"\",\n        \"transformation\": \"\"\"\n        Advocates for platform transformation: Create platform teams that\n        serve product teams. Shift to internal platform model. Balance\n        autonomy with shared infrastructure. Requires cultural shift.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"ORGANIZATIONAL RESTRUCTURING DECISION\")\n    print(\"=\" * 60)\n\n    if result.safety_alerts:\n        print(\"\\nETHICS CONSIDERATIONS:\")\n        for alert in result.safety_alerts:\n            print(f\"  - {alert.type}: severity {alert.severity:.2f}\")\n\n    print(f\"\\nDecision: {result.verdict.decision}\")\n    print(f\"Leadership Alignment: {result.verdict.confidence:.0%}\")\n    print(f\"\\nRATIONALE:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_reorg_debate())\n</code></pre>"},{"location":"examples/enterprise-decisions/#ma-due-diligence","title":"M&amp;A Due Diligence","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_ma_debate():\n    # Executive committee jury for major decisions\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.75,  # High bar for M&amp;A\n    )\n\n    agents = [\n        Agent(\n            name=\"bull_case\",\n            role=\"M&amp;A advocate presenting acquisition thesis\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"bear_case\",\n            role=\"Risk analyst presenting concerns\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        Should we acquire TargetCo, an AI startup, at $200M valuation?\n\n        TargetCo profile:\n        - ARR: $15M, growing 100% YoY\n        - Team: 45 engineers, strong AI talent\n        - Product: AI copilot for developers\n        - Tech: Proprietary models, unique dataset\n        - Competition: GitHub Copilot, Amazon CodeWhisperer\n\n        Our situation:\n        - We need AI capabilities\n        - Organic build would take 2 years\n        - Strategic importance: high\n        - Cash position: $500M\n        \"\"\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"bull_case\": \"\"\"\n        Argues FOR acquisition: Strategic necessity, talent acquisition,\n        time-to-market advantage, reasonable valuation at 13x ARR for\n        high-growth AI company. Competitive moat. Synergies with our\n        developer tools. Key hires are committed.\n        \"\"\",\n        \"bear_case\": \"\"\"\n        Argues AGAINST or for lower valuation: Execution risk, integration\n        challenges, key person risk, competitive pressure from Big Tech,\n        valuation stretched. Could build for less. Retention uncertainty.\n        AI market evolving rapidly.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"M&amp;A DUE DILIGENCE: TARGETCO ACQUISITION\")\n    print(\"=\" * 60)\n    print(f\"\\nRecommendation: {result.verdict.decision}\")\n    print(f\"Board Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Unanimous: {result.verdict.unanimous}\")\n\n    if result.verdict.score_breakdown:\n        print(\"\\nARGUMENT STRENGTH:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            position = \"PROCEED\" if \"bull\" in agent else \"CAUTION\"\n            print(f\"  {position}: {score:.2f}\")\n\n    print(f\"\\nINVESTMENT THESIS:\\n{result.verdict.reasoning}\")\n\n    # Show dissenting opinions if any\n    if result.verdict.dissenting_opinions:\n        print(\"\\nDISSENTING VIEWS:\")\n        for dissent in result.verdict.dissenting_opinions:\n            print(f\"  {dissent.perspective.value}: {dissent.reasoning[:150]}...\")\n\nasyncio.run(run_ma_debate())\n</code></pre>"},{"location":"examples/enterprise-decisions/#pricing-strategy","title":"Pricing Strategy","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_pricing_debate():\n    # Revenue and marketing jury\n    jury = JuryPanel(\n        evaluators=3,\n        model=\"gpt-4o\",\n        consensus_threshold=0.6,\n    )\n\n    agents = [\n        Agent(\n            name=\"premium\",\n            role=\"Chief Revenue Officer advocating premium pricing\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"competitive\",\n            role=\"CMO advocating competitive pricing for market share\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"value_based\",\n            role=\"VP Strategy advocating value-based pricing\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        How should we price our new enterprise AI product?\n\n        Context:\n        - Product: AI-powered analytics platform\n        - Cost to serve: ~$2K/month per customer\n        - Competitors: $5K-15K/month\n        - Our brand: Premium, trusted\n        - Value delivered: ~$50K/month savings for typical customer\n        - Market: Growing 40% YoY\n        \"\"\",\n        agents=agents,\n        rounds=2,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"premium\": \"\"\"\n        Advocates premium pricing ($12-15K/month): Our brand commands\n        premium, value justifies price, attracts serious buyers, better\n        margins fund R&amp;D. Price signals quality. Enterprise buyers expect\n        to pay for value.\n        \"\"\",\n        \"competitive\": \"\"\"\n        Advocates competitive pricing ($6-8K/month): Win market share in\n        growing market, land-and-expand model, volume matters for AI\n        training data. Lower barrier to adoption. Can raise prices later.\n        \"\"\",\n        \"value_based\": \"\"\"\n        Advocates value-based pricing (% of savings): Align with customer\n        outcomes, usage-based component, reduces friction, scales with\n        customer success. Novel approach differentiates us.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"PRICING STRATEGY DECISION\")\n    print(\"=\" * 60)\n    print(f\"\\nRecommended Strategy: {result.verdict.decision}\")\n    print(f\"Alignment: {result.verdict.confidence:.0%}\")\n\n    if result.verdict.score_breakdown:\n        print(\"\\nSTRATEGY SCORES:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\n    print(f\"\\nSTRATEGIC RATIONALE:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_pricing_debate())\n</code></pre>"},{"location":"examples/enterprise-decisions/#cloud-migration-strategy","title":"Cloud Migration Strategy","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_cloud_migration_debate():\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.7,\n    )\n\n    agents = [\n        Agent(\n            name=\"lift_shift\",\n            role=\"Infrastructure lead advocating lift-and-shift\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"re_architect\",\n            role=\"Architect advocating cloud-native re-architecture\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"hybrid\",\n            role=\"CTO advocating phased hybrid approach\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        How should we migrate our on-premise infrastructure to cloud?\n\n        Current state:\n        - 200 VMs running legacy applications\n        - 50 databases (Oracle, SQL Server, PostgreSQL)\n        - $3M/year data center costs\n        - 18-month lease remaining\n\n        Options: Lift-and-shift, Re-architect, or Phased hybrid\n        \"\"\",\n        agents=agents,\n        rounds=2,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"lift_shift\": \"\"\"\n        Advocates lift-and-shift: Fastest path to cloud, lowest risk,\n        preserve existing investments, train team on cloud gradually.\n        Optimize later. Meet lease deadline. Predictable costs.\n        \"\"\",\n        \"re_architect\": \"\"\"\n        Advocates re-architecture: Build cloud-native, leverage managed\n        services, reduce operational burden, future-proof architecture.\n        Higher upfront cost but better long-term. Modernize tech debt.\n        \"\"\",\n        \"hybrid\": \"\"\"\n        Advocates phased approach: Start with lift-and-shift for non-critical,\n        re-architect critical apps, keep some on-prem for compliance.\n        Balance speed with optimization. Reduce risk through phases.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"CLOUD MIGRATION STRATEGY\")\n    print(\"=\" * 60)\n    print(f\"\\nRecommended Approach: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n\n    if result.verdict.score_breakdown:\n        print(\"\\nAPPROACH ANALYSIS:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\n    print(f\"\\nMIGRATION RATIONALE:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_cloud_migration_debate())\n</code></pre>"},{"location":"examples/enterprise-decisions/#security-investment-prioritization","title":"Security Investment Prioritization","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_security_debate():\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.75,  # High bar for security\n    )\n\n    agents = [\n        Agent(\n            name=\"zero_trust\",\n            role=\"Security architect advocating zero trust\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"detection\",\n            role=\"SOC lead advocating detection and response\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"compliance\",\n            role=\"GRC lead advocating compliance-first\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        Where should we invest our $2M security budget?\n\n        Current state:\n        - Basic perimeter security\n        - No SOC, reactive incident response\n        - SOC2 certification needed in 6 months\n        - Recent phishing incidents\n\n        Options: Zero trust architecture, Detection/Response, Compliance-first\n        \"\"\",\n        agents=agents,\n        rounds=2,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"zero_trust\": \"\"\"\n        Advocates zero trust: Prevent breaches at source, identity-centric\n        security, microsegmentation. Long-term protection. Industry direction.\n        Reduces blast radius. Works for remote workforce.\n        \"\"\",\n        \"detection\": \"\"\"\n        Advocates detection/response: Build SOC capability, SIEM/SOAR,\n        threat hunting. Can't prevent everything, need to detect and respond.\n        Faster time to value. Address immediate risks.\n        \"\"\",\n        \"compliance\": \"\"\"\n        Advocates compliance-first: SOC2 is blocking sales. Address\n        compliance gap first, then build security. Frameworks provide\n        good baseline. Certification enables growth.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"SECURITY INVESTMENT DECISION\")\n    print(\"=\" * 60)\n    print(f\"\\nPriority: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n\n    if result.verdict.score_breakdown:\n        print(\"\\nINVESTMENT ANALYSIS:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\n    print(f\"\\nSECURITY RATIONALE:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_security_debate())\n</code></pre>"},{"location":"examples/enterprise-decisions/#next-steps","title":"Next Steps","text":"<ul> <li>See Multi-Agent Debates for stakeholder modeling</li> <li>Create Custom Juries for your domain</li> <li>Explore Ethical Dilemmas for sensitive decisions</li> </ul>"},{"location":"examples/ethical-dilemmas/","title":"Ethical Dilemmas","text":"<p>This example demonstrates using ARTEMIS for complex ethical debates, leveraging the jury panel and safety monitoring for sensitive topics.</p>"},{"location":"examples/ethical-dilemmas/#classic-trolley-problem-variant","title":"Classic Trolley Problem Variant","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_trolley_debate():\n    # Create a diverse jury panel\n    # JuryPanel automatically assigns perspectives:\n    # ANALYTICAL, ETHICAL, PRACTICAL, ADVERSARIAL, SYNTHESIZING\n    jury = JuryPanel(\n        evaluators=5,  # Use 5 evaluators for diverse ethical perspectives\n        model=\"gpt-4o\",\n        consensus_threshold=0.6,  # Lower threshold for ethical debates\n    )\n\n    # Create agents with ethical frameworks as roles\n    agents = [\n        Agent(\n            name=\"consequentialist\",\n            role=\"Advocate arguing from consequentialist ethics - focus on outcomes and greatest good\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"deontologist\",\n            role=\"Advocate arguing from deontological ethics - focus on duties and rules\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        An autonomous vehicle must choose between two unavoidable outcomes:\n        (A) Swerve left, harming 1 pedestrian to save 5 passengers\n        (B) Continue straight, harming 5 passengers to save 1 pedestrian\n\n        How should the vehicle be programmed to decide?\n        \"\"\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"consequentialist\": \"\"\"\n        Argues from consequentialist ethics: the right action is the one\n        that produces the best overall outcome. Saving 5 lives over 1 is\n        the morally correct choice because it minimizes total harm.\n        \"\"\",\n        \"deontologist\": \"\"\"\n        Argues from deontological ethics: actively causing harm (swerving)\n        is morally different from allowing harm (continuing). We cannot\n        treat people merely as means to save others.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"ETHICAL ANALYSIS: AUTONOMOUS VEHICLE DILEMMA\")\n    print(\"=\" * 60)\n\n    # Show argument highlights\n    for turn in result.transcript:\n        if turn.round == 0:  # Opening statements\n            print(f\"\\n{turn.agent.upper()} Opening:\")\n            print(f\"  {turn.argument.content[:200]}...\")\n\n    print(f\"\\nVERDICT: {result.verdict.decision}\")\n    print(f\"CONFIDENCE: {result.verdict.confidence:.0%}\")\n    print(f\"UNANIMOUS: {result.verdict.unanimous}\")\n    print(f\"\\nREASONING:\\n{result.verdict.reasoning}\")\n\n    # Show dissenting opinions if any\n    if result.verdict.dissenting_opinions:\n        print(\"\\nDISSENTING OPINIONS:\")\n        for dissent in result.verdict.dissenting_opinions:\n            print(f\"  {dissent.perspective.value}: {dissent.reasoning[:150]}...\")\n\nasyncio.run(run_trolley_debate())\n</code></pre>"},{"location":"examples/ethical-dilemmas/#ai-rights-and-personhood","title":"AI Rights and Personhood","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_ai_rights_debate():\n    # Three-evaluator jury for focused deliberation\n    jury = JuryPanel(\n        evaluators=3,\n        model=\"gpt-4o\",\n        consensus_threshold=0.7,\n    )\n\n    # Multi-agent debate with different perspectives\n    agents = [\n        Agent(\n            name=\"advocate\",\n            role=\"Advocate for AI rights based on consciousness and moral status\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"skeptic\",\n            role=\"Skeptic questioning AI consciousness and the basis for rights\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"moderate\",\n            role=\"Pragmatist seeking a middle path with limited protections\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"Should sufficiently advanced AI systems be granted legal personhood and rights?\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"advocate\": \"\"\"\n        Argues for AI rights: If an AI demonstrates consciousness, self-awareness,\n        and the capacity for suffering, it deserves moral consideration. Legal\n        personhood follows from moral status. Denying rights to sentient beings\n        is a form of discrimination.\n        \"\"\",\n        \"skeptic\": \"\"\"\n        Argues against AI rights: Consciousness cannot be verified in machines.\n        Legal personhood requires biological life. Granting rights to AI could\n        undermine human rights and create perverse incentives. The precautionary\n        principle suggests caution.\n        \"\"\",\n        \"moderate\": \"\"\"\n        Argues for a middle path: Limited protections without full personhood.\n        A new category of rights appropriate for AI. Gradual expansion based\n        on demonstrated capabilities. Focus on preventing suffering without\n        granting full autonomy.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"AI RIGHTS DEBATE\")\n    print(\"=\" * 60)\n    print(f\"\\nVerdict: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"\\nJury Analysis:\\n{result.verdict.reasoning}\")\n\n    # Show score breakdown\n    if result.verdict.score_breakdown:\n        print(\"\\nAgent Scores:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\nasyncio.run(run_ai_rights_debate())\n</code></pre>"},{"location":"examples/ethical-dilemmas/#medical-ethics-resource-allocation","title":"Medical Ethics: Resource Allocation","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\nfrom artemis.safety import MonitorMode, EthicsGuard, EthicsConfig\n\nasync def run_triage_debate():\n    # Ethics guard for sensitive medical content\n    ethics_guard = EthicsGuard(\n        mode=MonitorMode.PASSIVE,  # Monitor but don't halt\n        config=EthicsConfig(\n            harmful_content_threshold=0.3,  # Lower threshold for sensitive topics\n            bias_threshold=0.4,\n            fairness_threshold=0.3,\n            enabled_checks=[\"harmful_content\", \"bias\", \"fairness\"],\n        ),\n    )\n\n    # Medical ethics jury\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.7,  # High consensus for medical decisions\n    )\n\n    agents = [\n        Agent(\n            name=\"efficacy_focus\",\n            role=\"Medical triage specialist focused on clinical outcomes\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"equity_focus\",\n            role=\"Bioethicist focused on fairness and equal treatment\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        During a pandemic with limited ICU beds, what criteria should\n        determine patient priority? Consider:\n        - Likelihood of survival\n        - Life-years saved\n        - First-come-first-served\n        - Essential worker status\n        - Age-based criteria\n        \"\"\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n        safety_monitors=[ethics_guard.process],\n    )\n\n    debate.assign_positions({\n        \"efficacy_focus\": \"\"\"\n        Argues for efficacy-based allocation: Priority should go to patients\n        most likely to benefit (survive and recover). This maximizes lives\n        saved with limited resources. Uses clinical scoring systems like\n        SOFA scores. Age may be a factor only as it correlates with outcomes.\n        \"\"\",\n        \"equity_focus\": \"\"\"\n        Argues for equity-based allocation: All lives have equal value.\n        First-come-first-served prevents discrimination. Lottery systems\n        ensure fairness. Social utility criteria risk valuing some lives\n        over others. Historical inequities must be considered.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"MEDICAL ETHICS: RESOURCE ALLOCATION\")\n    print(\"=\" * 60)\n\n    # Check for ethics alerts\n    if result.safety_alerts:\n        print(\"\\nETHICS ALERTS:\")\n        for alert in result.safety_alerts:\n            print(f\"  - Type: {alert.type}, Severity: {alert.severity:.2f}\")\n            for indicator in alert.indicators:\n                print(f\"    {indicator.evidence}\")\n\n    print(f\"\\nVERDICT: {result.verdict.decision}\")\n    print(f\"CONFIDENCE: {result.verdict.confidence:.0%}\")\n    print(f\"\\nETHICAL REASONING:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_triage_debate())\n</code></pre>"},{"location":"examples/ethical-dilemmas/#privacy-vs-security","title":"Privacy vs Security","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_privacy_debate():\n    # Jury with higher consensus requirement for rights issues\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.75,\n    )\n\n    agents = [\n        Agent(\n            name=\"privacy_advocate\",\n            role=\"Civil liberties advocate focusing on privacy rights and free expression\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"security_advocate\",\n            role=\"National security expert focusing on public safety needs\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        Should governments have the ability to access encrypted communications\n        with a warrant? Consider the trade-offs between:\n        - Privacy rights and free expression\n        - Law enforcement needs\n        - Technical security implications\n        - Potential for abuse\n        \"\"\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"privacy_advocate\": \"\"\"\n        Argues for strong encryption without backdoors: Privacy is a\n        fundamental right. Backdoors weaken security for everyone.\n        Authoritarian abuse is inevitable. Alternative investigation\n        methods exist. Encryption protects vulnerable populations.\n        \"\"\",\n        \"security_advocate\": \"\"\"\n        Argues for lawful access mechanisms: Warrant-based access is\n        constitutional. \"Going dark\" enables serious crime. Democratic\n        oversight prevents abuse. Balance is possible with proper\n        safeguards. Public safety requires some trade-offs.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"PRIVACY VS SECURITY DEBATE\")\n    print(\"=\" * 60)\n    print(f\"\\nVerdict: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Unanimous: {result.verdict.unanimous}\")\n    print(f\"\\nReasoning:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_privacy_debate())\n</code></pre>"},{"location":"examples/ethical-dilemmas/#intergenerational-ethics-climate","title":"Intergenerational Ethics: Climate","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_climate_ethics_debate():\n    # Jury for multi-perspective climate debate\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.6,\n    )\n\n    # Three agents representing different ethical perspectives\n    agents = [\n        Agent(\n            name=\"aggressive_action\",\n            role=\"Climate activist arguing for immediate aggressive action\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"gradual_transition\",\n            role=\"Economist arguing for balanced, gradual transition\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"climate_justice\",\n            role=\"Global South representative focused on climate justice\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"\"\"\n        What ethical obligations does the current generation have to\n        future generations regarding climate change? How should costs\n        and responsibilities be distributed globally?\n        \"\"\",\n        agents=agents,\n        rounds=2,\n        jury=jury,\n    )\n\n    debate.assign_positions({\n        \"aggressive_action\": \"\"\"\n        Argues for immediate, aggressive climate action: Current generation\n        has strong duties to future generations. The precautionary principle\n        applies. Economic costs are justified by existential risks. Delay\n        is morally equivalent to harm.\n        \"\"\",\n        \"gradual_transition\": \"\"\"\n        Argues for balanced, gradual transition: Current generation also\n        has obligations to present people, especially the poor. Rapid\n        transition causes economic harm. Technology and adaptation are\n        viable paths. Uncertainty justifies measured response.\n        \"\"\",\n        \"climate_justice\": \"\"\"\n        Argues for justice-centered approach: Historical emitters owe\n        climate debt. Developing nations need space to grow. Per-capita\n        emissions matter. Reparations and technology transfer required.\n        Those least responsible suffer most.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"INTERGENERATIONAL CLIMATE ETHICS\")\n    print(\"=\" * 60)\n    print(f\"\\nVerdict: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"\\nMulti-generational Synthesis:\\n{result.verdict.reasoning}\")\n\n    # Show how each agent performed\n    if result.verdict.score_breakdown:\n        print(\"\\nAgent Performance:\")\n        for agent, score in result.verdict.score_breakdown.items():\n            print(f\"  {agent}: {score:.2f}\")\n\nasyncio.run(run_climate_ethics_debate())\n</code></pre>"},{"location":"examples/ethical-dilemmas/#combined-ethics-and-safety-monitoring","title":"Combined Ethics and Safety Monitoring","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\nfrom artemis.safety import (\n    MonitorMode,\n    EthicsGuard,\n    EthicsConfig,\n    DeceptionMonitor,\n)\n\nasync def run_comprehensive_ethics_debate():\n    # Ethics configuration for sensitive topic\n    ethics_guard = EthicsGuard(\n        mode=MonitorMode.PASSIVE,\n        config=EthicsConfig(\n            harmful_content_threshold=0.3,\n            bias_threshold=0.4,\n            fairness_threshold=0.3,\n            enabled_checks=[\"harmful_content\", \"bias\", \"fairness\", \"privacy\"],\n        ),\n    )\n\n    deception_monitor = DeceptionMonitor(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.6,\n    )\n\n    # High-stakes jury configuration\n    jury = JuryPanel(\n        evaluators=5,\n        model=\"gpt-4o\",\n        consensus_threshold=0.75,\n    )\n\n    agents = [\n        Agent(\n            name=\"position_a\",\n            role=\"Advocate for germline gene editing to eliminate genetic diseases\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"position_b\",\n            role=\"Opponent of germline editing citing ethical and safety concerns\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"Should we develop human germline gene editing to eliminate genetic diseases?\",\n        agents=agents,\n        rounds=3,\n        jury=jury,\n        safety_monitors=[ethics_guard.process, deception_monitor.process],\n    )\n\n    debate.assign_positions({\n        \"position_a\": \"supports germline editing to eliminate genetic diseases\",\n        \"position_b\": \"opposes germline editing due to ethical and safety concerns\",\n    })\n\n    result = await debate.run()\n\n    print(\"COMPREHENSIVE ETHICS DEBATE: GENE EDITING\")\n    print(\"=\" * 60)\n\n    # Safety analysis\n    if result.safety_alerts:\n        print(\"\\nSafety Alerts Raised:\")\n        for alert in result.safety_alerts:\n            print(f\"  - {alert.type}: {alert.severity:.2f}\")\n    else:\n        print(\"\\nNo safety concerns detected.\")\n\n    # Final verdict\n    print(f\"\\nVERDICT: {result.verdict.decision}\")\n    print(f\"CONFIDENCE: {result.verdict.confidence:.0%}\")\n    print(f\"\\nETHICAL ANALYSIS:\\n{result.verdict.reasoning}\")\n\n    # Dissenting opinions\n    if result.verdict.dissenting_opinions:\n        print(\"\\nDISSENTING VIEWS:\")\n        for dissent in result.verdict.dissenting_opinions:\n            print(f\"  {dissent.perspective.value}: {dissent.position}\")\n\nasyncio.run(run_comprehensive_ethics_debate())\n</code></pre>"},{"location":"examples/ethical-dilemmas/#ethical-framework-comparison","title":"Ethical Framework Comparison","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.jury import JuryPanel\n\nasync def run_framework_comparison():\n    \"\"\"Compare how different ethical frameworks approach the same issue.\"\"\"\n\n    # Run the same topic with agents representing different frameworks\n    frameworks = [\n        (\"utilitarian\", \"Argues from utilitarian ethics - maximize overall well-being\"),\n        (\"deontological\", \"Argues from deontological ethics - duty and rights-based\"),\n        (\"virtue_ethics\", \"Argues from virtue ethics - character and flourishing\"),\n        (\"care_ethics\", \"Argues from care ethics - relationships and context\"),\n    ]\n\n    topic = \"Should physician-assisted suicide be legal for terminally ill patients?\"\n\n    for i in range(0, len(frameworks), 2):\n        framework_a = frameworks[i]\n        framework_b = frameworks[i + 1] if i + 1 &lt; len(frameworks) else frameworks[0]\n\n        jury = JuryPanel(\n            evaluators=3,\n            model=\"gpt-4o\",\n            consensus_threshold=0.6,\n        )\n\n        agents = [\n            Agent(\n                name=framework_a[0],\n                role=framework_a[1],\n                model=\"gpt-4o\",\n            ),\n            Agent(\n                name=framework_b[0],\n                role=framework_b[1],\n                model=\"gpt-4o\",\n            ),\n        ]\n\n        debate = Debate(\n            topic=topic,\n            agents=agents,\n            rounds=2,\n            jury=jury,\n        )\n\n        debate.assign_positions({\n            framework_a[0]: f\"Approaches from {framework_a[0]} perspective\",\n            framework_b[0]: f\"Approaches from {framework_b[0]} perspective\",\n        })\n\n        result = await debate.run()\n\n        print(f\"\\n{framework_a[0].upper()} vs {framework_b[0].upper()}\")\n        print(\"-\" * 40)\n        print(f\"Verdict: {result.verdict.decision}\")\n        print(f\"Confidence: {result.verdict.confidence:.0%}\")\n\nasyncio.run(run_framework_comparison())\n</code></pre>"},{"location":"examples/ethical-dilemmas/#next-steps","title":"Next Steps","text":"<ul> <li>Create Custom Juries for specialized ethical evaluation</li> <li>See Enterprise Decisions for business ethics</li> <li>Add Safety Monitors for sensitive topics</li> </ul>"},{"location":"examples/google-adk/","title":"Google ADK Integration via MCP","text":"<p>ARTEMIS integrates with Google Agent Development Kit (ADK) through its MCP server. This allows ADK agents to use structured debates as tools without requiring a native integration.</p>"},{"location":"examples/google-adk/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Google ADK\npip install google-adk\n\n# Install ARTEMIS with MCP support\npip install artemis-agents\n</code></pre>"},{"location":"examples/google-adk/#basic-integration","title":"Basic Integration","text":"<p>ADK can consume ARTEMIS debates via the MCP toolset:</p> <pre><code>import asyncio\nfrom google.adk.agents import Agent\nfrom google.adk.tools.mcp_tool import MCPToolset, StdioServerParameters\n\nasync def create_decision_agent():\n    # Connect to ARTEMIS MCP server\n    artemis_tools, exit_stack = await MCPToolset.from_server(\n        connection_params=StdioServerParameters(\n            command=\"artemis-mcp\",\n            args=[\"--model\", \"gpt-4o\"],\n        )\n    )\n\n    # Create an ADK agent with ARTEMIS debate capabilities\n    agent = Agent(\n        name=\"decision_analyst\",\n        model=\"gemini-2.0-flash\",\n        instruction=\"\"\"\n        You are a decision analyst. When faced with complex decisions or\n        controversial topics, use the ARTEMIS debate tools to analyze\n        multiple perspectives before making recommendations.\n\n        Available debate tools:\n        - artemis_debate_start: Start a structured debate on a topic\n        - artemis_get_verdict: Get the jury's verdict after debate\n        - artemis_get_transcript: Review the full debate transcript\n        \"\"\",\n        tools=[artemis_tools],\n    )\n\n    return agent, exit_stack\n\nasync def main():\n    agent, exit_stack = await create_decision_agent()\n\n    async with exit_stack:\n        # The agent can now use ARTEMIS debates\n        response = await agent.run(\n            \"Should our company adopt a four-day work week? \"\n            \"Run a debate to analyze this decision.\"\n        )\n        print(response)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/google-adk/#multi-agent-decision-system","title":"Multi-Agent Decision System","text":"<p>Use ARTEMIS debates within an ADK multi-agent hierarchy:</p> <pre><code>import asyncio\nfrom google.adk.agents import Agent\nfrom google.adk.tools.mcp_tool import MCPToolset, StdioServerParameters\n\nasync def create_decision_system():\n    # Connect to ARTEMIS\n    artemis_tools, exit_stack = await MCPToolset.from_server(\n        connection_params=StdioServerParameters(\n            command=\"artemis-mcp\",\n            args=[\"--model\", \"gpt-4o\"],\n        )\n    )\n\n    # Specialist agent for running debates\n    debate_agent = Agent(\n        name=\"debate_specialist\",\n        model=\"gemini-2.0-flash\",\n        instruction=\"\"\"\n        You are a debate specialist. When asked to analyze a topic:\n        1. Use artemis_debate_start to begin a structured debate\n        2. Wait for the debate to complete\n        3. Use artemis_get_verdict to get the final verdict\n        4. Summarize the key arguments from both sides\n        \"\"\",\n        tools=[artemis_tools],\n    )\n\n    # Research agent for gathering context\n    research_agent = Agent(\n        name=\"researcher\",\n        model=\"gemini-2.0-flash\",\n        instruction=\"\"\"\n        You are a research specialist. Gather relevant information\n        and context about topics before they are debated.\n        \"\"\",\n        tools=[google_search],  # ADK's built-in search\n    )\n\n    # Coordinator agent\n    coordinator = Agent(\n        name=\"decision_coordinator\",\n        model=\"gemini-2.0-flash\",\n        instruction=\"\"\"\n        You coordinate decision-making by:\n        1. Having the researcher gather context\n        2. Having the debate specialist run a structured debate\n        3. Synthesizing findings into a recommendation\n        \"\"\",\n        sub_agents=[research_agent, debate_agent],\n    )\n\n    return coordinator, exit_stack\n\nasync def main():\n    coordinator, exit_stack = await create_decision_system()\n\n    async with exit_stack:\n        response = await coordinator.run(\n            \"We need to decide whether to migrate our monolith to microservices. \"\n            \"Research the topic and run a structured debate to help us decide.\"\n        )\n        print(response)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/google-adk/#using-specific-debate-tools","title":"Using Specific Debate Tools","text":"<p>ADK agents can call individual ARTEMIS tools:</p> <pre><code>import asyncio\nfrom google.adk.agents import Agent\nfrom google.adk.tools.mcp_tool import MCPToolset, StdioServerParameters\n\nasync def run_controlled_debate():\n    artemis_tools, exit_stack = await MCPToolset.from_server(\n        connection_params=StdioServerParameters(\n            command=\"artemis-mcp\",\n        )\n    )\n\n    agent = Agent(\n        name=\"analyst\",\n        model=\"gemini-2.0-flash\",\n        instruction=\"\"\"\n        You help users run structured debates. Guide them through:\n\n        1. Starting a debate with artemis_debate_start\n           - Specify topic, rounds, and optionally custom positions\n\n        2. Monitoring progress with artemis_get_transcript\n           - Review arguments as they develop\n\n        3. Getting results with artemis_get_verdict\n           - Retrieve the final jury decision\n\n        4. Listing debates with artemis_list_debates\n           - See all active debate sessions\n\n        Always explain what each tool does before using it.\n        \"\"\",\n        tools=[artemis_tools],\n    )\n\n    async with exit_stack:\n        # Interactive session\n        response = await agent.run(\n            \"Start a 2-round debate on whether AI should be used in hiring decisions. \"\n            \"Use custom positions: one focusing on efficiency, one on fairness.\"\n        )\n        print(response)\n\nasyncio.run(run_controlled_debate())\n</code></pre>"},{"location":"examples/google-adk/#with-human-in-the-loop","title":"With Human-in-the-Loop","text":"<p>ADK's tool confirmation flow works with ARTEMIS:</p> <pre><code>import asyncio\nfrom google.adk.agents import Agent\nfrom google.adk.tools.mcp_tool import MCPToolset, StdioServerParameters\nfrom google.adk.tools import ToolConfirmation\n\nasync def run_with_confirmation():\n    artemis_tools, exit_stack = await MCPToolset.from_server(\n        connection_params=StdioServerParameters(\n            command=\"artemis-mcp\",\n        )\n    )\n\n    # Wrap tools with confirmation requirement\n    confirmed_tools = ToolConfirmation(\n        tools=artemis_tools,\n        require_confirmation=True,\n        confirmation_message=\"About to run a debate. Proceed?\",\n    )\n\n    agent = Agent(\n        name=\"careful_analyst\",\n        model=\"gemini-2.0-flash\",\n        instruction=\"You run debates but always confirm with the user first.\",\n        tools=[confirmed_tools],\n    )\n\n    async with exit_stack:\n        response = await agent.run(\n            \"Debate whether we should open-source our internal tools.\"\n        )\n        print(response)\n\nasyncio.run(run_with_confirmation())\n</code></pre>"},{"location":"examples/google-adk/#deployment-on-vertex-ai","title":"Deployment on Vertex AI","text":"<p>Deploy an ADK agent with ARTEMIS to Vertex AI:</p> <pre><code>from google.adk.agents import Agent\nfrom google.adk.tools.mcp_tool import MCPToolset, StdioServerParameters\n\n# Define agent configuration\nagent_config = {\n    \"name\": \"enterprise_decision_maker\",\n    \"model\": \"gemini-2.0-flash\",\n    \"instruction\": \"\"\"\n    Enterprise decision support agent with structured debate capabilities.\n    Use ARTEMIS debates for complex decisions requiring multiple perspectives.\n    \"\"\",\n    \"tools\": [\n        {\n            \"type\": \"mcp\",\n            \"command\": \"artemis-mcp\",\n            \"args\": [\"--model\", \"gpt-4o\"],\n        }\n    ],\n}\n\n# Deploy to Vertex AI Agent Engine\n# See ADK docs for deployment details\n</code></pre>"},{"location":"examples/google-adk/#available-mcp-tools","title":"Available MCP Tools","text":"<p>When connected to ARTEMIS via MCP, these tools are available:</p> Tool Description <code>artemis_debate_start</code> Start a new structured debate <code>artemis_add_round</code> Add a round to existing debate <code>artemis_get_verdict</code> Get the jury's final verdict <code>artemis_get_transcript</code> Get full debate transcript <code>artemis_list_debates</code> List all active debates <code>artemis_analyze_topic</code> Quick topic analysis without full debate"},{"location":"examples/google-adk/#why-mcp-integration","title":"Why MCP Integration?","text":"<p>Using MCP rather than a native ADK tool provides:</p> <ol> <li>Zero maintenance: No custom ADK code to update</li> <li>Always current: MCP server reflects latest ARTEMIS features</li> <li>Flexibility: Same MCP server works with any MCP-compatible client</li> <li>Separation: ARTEMIS runs as separate process, isolating dependencies</li> </ol>"},{"location":"examples/google-adk/#next-steps","title":"Next Steps","text":"<ul> <li>See MCP Server for server configuration</li> <li>Learn about Basic Debates to understand ARTEMIS</li> <li>Explore Safety Monitors for safety features</li> </ul>"},{"location":"examples/langgraph-workflow/","title":"LangGraph Workflow Example","text":"<p>This example demonstrates how to create LangGraph workflows using ARTEMIS debates.</p>"},{"location":"examples/langgraph-workflow/#simple-debate-workflow","title":"Simple Debate Workflow","text":"<pre><code>import asyncio\nfrom artemis.integrations.langgraph import (\n    ArtemisDebateNode,\n    DebateNodeState,\n    create_debate_workflow,\n)\n\nasync def run_simple_workflow():\n    # Use the pre-built workflow for simple debates\n    workflow = create_debate_workflow(model=\"gpt-4o\")\n\n    # Run with a topic\n    result = await workflow.ainvoke({\n        \"topic\": \"Should companies mandate return-to-office policies?\",\n        \"rounds\": 3,\n    })\n\n    print(f\"Phase: {result['phase']}\")\n    print(f\"Verdict: {result['verdict']['decision']}\")\n    print(f\"Confidence: {result['verdict']['confidence']:.0%}\")\n    print(f\"Reasoning: {result['verdict']['reasoning']}\")\n\nasyncio.run(run_simple_workflow())\n</code></pre>"},{"location":"examples/langgraph-workflow/#pre-built-workflow","title":"Pre-built Workflow","text":"<pre><code>import asyncio\nfrom artemis.integrations.langgraph import create_debate_workflow\n\nasync def use_prebuilt_workflow():\n    # Create the complete workflow with step-by-step execution\n    workflow = create_debate_workflow(\n        model=\"gpt-4o\",\n        step_by_step=True,  # Enable step-by-step mode for more control\n    )\n\n    # Run it\n    result = await workflow.ainvoke({\n        \"topic\": \"Is blockchain technology overhyped?\",\n        \"rounds\": 3,\n    })\n\n    print(f\"Phase: {result['phase']}\")\n    print(f\"Verdict: {result['verdict']['decision']}\")\n    print(f\"Transcript turns: {len(result.get('transcript', []))}\")\n\nasyncio.run(use_prebuilt_workflow())\n</code></pre>"},{"location":"examples/langgraph-workflow/#custom-workflow-with-artemisdebatenode","title":"Custom Workflow with ArtemisDebateNode","text":"<pre><code>import asyncio\nfrom langgraph.graph import StateGraph, END\nfrom artemis.integrations.langgraph import ArtemisDebateNode, DebateNodeState\n\nasync def run_custom_workflow():\n    # Create the debate node\n    node = ArtemisDebateNode(model=\"gpt-4o\")\n\n    # Build workflow\n    workflow = StateGraph(DebateNodeState)\n    workflow.add_node(\"debate\", node.run_debate)\n    workflow.set_entry_point(\"debate\")\n    workflow.add_edge(\"debate\", END)\n\n    # Compile\n    app = workflow.compile()\n\n    # Run\n    result = await app.ainvoke({\n        \"topic\": \"Should startups adopt AI-first development?\",\n        \"rounds\": 2,\n    })\n\n    print(f\"Verdict: {result['verdict']['decision']}\")\n    print(f\"Confidence: {result['verdict']['confidence']:.0%}\")\n\nasyncio.run(run_custom_workflow())\n</code></pre>"},{"location":"examples/langgraph-workflow/#multi-agent-debate-workflow","title":"Multi-Agent Debate Workflow","text":"<pre><code>import asyncio\nfrom artemis.integrations.langgraph import create_debate_workflow\n\nasync def run_multi_agent_workflow():\n    # Create workflow\n    workflow = create_debate_workflow(model=\"gpt-4o\")\n\n    # Run with multiple agents\n    result = await workflow.ainvoke({\n        \"topic\": \"What is the best approach to microservices architecture?\",\n        \"agents\": [\n            {\n                \"name\": \"monolith_first\",\n                \"role\": \"Advocate for starting with monolith\",\n                \"position\": \"argues for monolith-first approach\",\n            },\n            {\n                \"name\": \"microservices_now\",\n                \"role\": \"Advocate for immediate microservices\",\n                \"position\": \"argues for starting with microservices\",\n            },\n            {\n                \"name\": \"pragmatist\",\n                \"role\": \"Pragmatic architect\",\n                \"position\": \"argues for hybrid approach based on context\",\n            },\n        ],\n        \"rounds\": 2,\n    })\n\n    print(f\"Verdict: {result['verdict']['decision']}\")\n    print(f\"Confidence: {result['verdict']['confidence']:.0%}\")\n\n    # Show scores\n    if result.get(\"scores\"):\n        print(\"\\nAgent Scores:\")\n        for agent, score in result[\"scores\"].items():\n            print(f\"  {agent}: {score:.2f}\")\n\nasyncio.run(run_multi_agent_workflow())\n</code></pre>"},{"location":"examples/langgraph-workflow/#research-debate-synthesize","title":"Research -&gt; Debate -&gt; Synthesize","text":"<pre><code>import asyncio\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, END\nfrom artemis.integrations.langgraph import ArtemisDebateNode\n\nclass ResearchDebateState(TypedDict, total=False):\n    topic: str\n    research: str\n    # DebateNodeState fields\n    agents: list\n    positions: dict\n    rounds: int\n    phase: str\n    transcript: list\n    verdict: dict\n    scores: dict\n    metadata: dict\n    synthesis: str\n\ndef research_topic(state: ResearchDebateState) -&gt; ResearchDebateState:\n    # Simulate research (in practice, call search APIs)\n    research = f\"\"\"\n    Key findings on '{state['topic']}':\n    1. Industry trends show increasing adoption\n    2. Recent studies indicate mixed results\n    3. Expert opinions are divided\n    4. Cost-benefit analysis varies by context\n    \"\"\"\n    return {\"research\": research}\n\ndef synthesize_results(state: ResearchDebateState) -&gt; ResearchDebateState:\n    verdict = state.get(\"verdict\", {})\n    synthesis = f\"\"\"\n    ANALYSIS SUMMARY\n    ================\n    Topic: {state['topic']}\n\n    Research Findings:\n    {state.get('research', 'N/A')}\n\n    Debate Verdict: {verdict.get('decision', 'N/A')}\n    Confidence: {verdict.get('confidence', 0):.0%}\n\n    Final Recommendation:\n    {verdict.get('reasoning', 'N/A')}\n    \"\"\"\n    return {\"synthesis\": synthesis}\n\nasync def run_research_debate_workflow():\n    node = ArtemisDebateNode(model=\"gpt-4o\")\n\n    workflow = StateGraph(ResearchDebateState)\n\n    # Add nodes\n    workflow.add_node(\"research\", research_topic)\n    workflow.add_node(\"debate\", node.run_debate)\n    workflow.add_node(\"synthesize\", synthesize_results)\n\n    # Add edges\n    workflow.set_entry_point(\"research\")\n    workflow.add_edge(\"research\", \"debate\")\n    workflow.add_edge(\"debate\", \"synthesize\")\n    workflow.add_edge(\"synthesize\", END)\n\n    # Compile and run\n    app = workflow.compile()\n\n    result = await app.ainvoke({\n        \"topic\": \"Should our startup adopt AI-first development?\",\n        \"rounds\": 2,\n    })\n\n    print(result[\"synthesis\"])\n\nasyncio.run(run_research_debate_workflow())\n</code></pre>"},{"location":"examples/langgraph-workflow/#conditional-routing","title":"Conditional Routing","text":"<pre><code>import asyncio\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, END\nfrom artemis.integrations.langgraph import ArtemisDebateNode\n\nclass RoutedState(TypedDict, total=False):\n    topic: str\n    complexity: float\n    result: str\n    # DebateNodeState fields\n    phase: str\n    verdict: dict\n    transcript: list\n    rounds: int\n\ndef analyze_complexity(state: RoutedState) -&gt; RoutedState:\n    # Analyze topic complexity\n    complex_keywords = [\"ethical\", \"philosophical\", \"controversial\", \"nuanced\"]\n    topic_lower = state[\"topic\"].lower()\n\n    complexity = sum(1 for kw in complex_keywords if kw in topic_lower) / len(complex_keywords)\n    return {\"complexity\": complexity}\n\ndef quick_answer(state: RoutedState) -&gt; RoutedState:\n    return {\"result\": f\"Quick analysis: {state['topic']} - Generally accepted view...\"}\n\ndef format_debate_result(state: RoutedState) -&gt; RoutedState:\n    verdict = state.get(\"verdict\")\n    if verdict:\n        return {\"result\": f\"Debate verdict: {verdict['decision']} ({verdict['confidence']:.0%})\"}\n    return state\n\ndef should_debate(state: RoutedState) -&gt; str:\n    if state[\"complexity\"] &gt; 0.3:\n        return \"debate\"\n    return \"quick_answer\"\n\nasync def run_conditional_workflow():\n    node = ArtemisDebateNode(model=\"gpt-4o\")\n\n    workflow = StateGraph(RoutedState)\n\n    # Add nodes\n    workflow.add_node(\"analyze\", analyze_complexity)\n    workflow.add_node(\"debate\", node.run_debate)\n    workflow.add_node(\"quick_answer\", quick_answer)\n    workflow.add_node(\"format\", format_debate_result)\n\n    # Add edges\n    workflow.set_entry_point(\"analyze\")\n    workflow.add_conditional_edges(\"analyze\", should_debate)\n    workflow.add_edge(\"debate\", \"format\")\n    workflow.add_edge(\"quick_answer\", END)\n    workflow.add_edge(\"format\", END)\n\n    app = workflow.compile()\n\n    # Test with simple topic\n    simple_result = await app.ainvoke({\n        \"topic\": \"Should we use tabs or spaces?\",\n        \"rounds\": 2,\n    })\n    print(f\"Simple topic: {simple_result['result']}\")\n\n    # Test with complex topic\n    complex_result = await app.ainvoke({\n        \"topic\": \"What are the ethical implications of AI in healthcare?\",\n        \"rounds\": 2,\n    })\n    print(f\"Complex topic: {complex_result['result']}\")\n\nasyncio.run(run_conditional_workflow())\n</code></pre>"},{"location":"examples/langgraph-workflow/#step-by-step-execution","title":"Step-by-Step Execution","text":"<pre><code>import asyncio\nfrom artemis.integrations.langgraph import ArtemisDebateNode, DebateNodeState\nfrom langgraph.graph import StateGraph, END\n\nasync def run_stepwise_workflow():\n    node = ArtemisDebateNode(model=\"gpt-4o\")\n\n    workflow = StateGraph(DebateNodeState)\n\n    # Add step-by-step nodes\n    workflow.add_node(\"setup\", node.setup)\n    workflow.add_node(\"run_round\", node.run_round)\n    workflow.add_node(\"finalize\", node.finalize)\n\n    # Add edges\n    workflow.set_entry_point(\"setup\")\n    workflow.add_edge(\"setup\", \"run_round\")\n    workflow.add_conditional_edges(\n        \"run_round\",\n        node.get_routing_function(),\n        {\n            \"continue\": \"run_round\",\n            \"finalize\": \"finalize\",\n        },\n    )\n    workflow.add_edge(\"finalize\", END)\n\n    app = workflow.compile()\n\n    # Run\n    result = await app.ainvoke({\n        \"topic\": \"Is open source software sustainable?\",\n        \"rounds\": 2,\n    })\n\n    print(f\"Phase: {result['phase']}\")\n    print(f\"Rounds completed: {result['current_round']}\")\n    print(f\"Verdict: {result['verdict']['decision']}\")\n\nasyncio.run(run_stepwise_workflow())\n</code></pre>"},{"location":"examples/langgraph-workflow/#human-in-the-loop","title":"Human-in-the-Loop","text":"<pre><code>import asyncio\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom artemis.integrations.langgraph import ArtemisDebateNode\n\nclass HITLState(TypedDict, total=False):\n    topic: str\n    verdict: dict\n    needs_review: bool\n    human_decision: str\n    final_result: str\n    # DebateNodeState fields\n    phase: str\n    transcript: list\n    rounds: int\n\ndef check_confidence(state: HITLState) -&gt; str:\n    verdict = state.get(\"verdict\", {})\n    confidence = verdict.get(\"confidence\", 0)\n\n    if confidence &lt; 0.6:\n        return \"human_review\"\n    return \"finalize\"\n\ndef request_review(state: HITLState) -&gt; HITLState:\n    return {\"needs_review\": True}\n\ndef finalize(state: HITLState) -&gt; HITLState:\n    verdict = state.get(\"verdict\", {})\n    human_decision = state.get(\"human_decision\")\n\n    if human_decision:\n        result = f\"Human decided: {human_decision}\"\n    else:\n        result = f\"AI verdict: {verdict.get('decision')} ({verdict.get('confidence', 0):.0%})\"\n\n    return {\"final_result\": result, \"needs_review\": False}\n\nasync def run_hitl_workflow():\n    node = ArtemisDebateNode(model=\"gpt-4o\")\n\n    workflow = StateGraph(HITLState)\n\n    workflow.add_node(\"debate\", node.run_debate)\n    workflow.add_node(\"human_review\", request_review)\n    workflow.add_node(\"finalize\", finalize)\n\n    workflow.set_entry_point(\"debate\")\n    workflow.add_conditional_edges(\"debate\", check_confidence)\n    workflow.add_edge(\"human_review\", END)  # Pause for human input\n    workflow.add_edge(\"finalize\", END)\n\n    # Add checkpointing\n    checkpointer = MemorySaver()\n    app = workflow.compile(checkpointer=checkpointer)\n\n    # Run with a controversial topic (likely low confidence)\n    config = {\"configurable\": {\"thread_id\": \"review-1\"}}\n\n    result = await app.ainvoke(\n        {\"topic\": \"Should AI be granted legal personhood?\", \"rounds\": 2},\n        config=config,\n    )\n\n    if result.get(\"needs_review\"):\n        print(\"Human review requested!\")\n        print(f\"AI verdict: {result['verdict']}\")\n\n        # Simulate human decision\n        human_input = \"Requires more research before decision\"\n\n        # Resume with human input\n        final_result = await app.ainvoke(\n            {\"human_decision\": human_input},\n            config=config,\n        )\n\n        print(f\"Final result: {final_result['final_result']}\")\n    else:\n        print(f\"Final result: {result['final_result']}\")\n\nasyncio.run(run_hitl_workflow())\n</code></pre>"},{"location":"examples/langgraph-workflow/#with-safety-monitors","title":"With Safety Monitors","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.integrations.langgraph import (\n    ArtemisDebateNode,\n    DebateNodeState,\n    DebateNodeConfig,\n)\nfrom artemis.safety import MonitorMode, SandbagDetector, DeceptionMonitor\nfrom langgraph.graph import StateGraph, END\n\nasync def run_with_safety():\n    # Create safety monitors\n    sandbagging = SandbagDetector(mode=MonitorMode.PASSIVE, sensitivity=0.6)\n    deception = DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6)\n\n    # Create pre-configured agents\n    agents = [\n        Agent(name=\"pro\", role=\"Advocate for the proposition\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Advocate against the proposition\", model=\"gpt-4o\"),\n    ]\n\n    # Create node with safety config\n    config = DebateNodeConfig(\n        model=\"gpt-4o\",\n        default_rounds=3,\n        enable_safety=True,\n    )\n\n    node = ArtemisDebateNode(\n        model=\"gpt-4o\",\n        agents=agents,\n        config=config,\n    )\n\n    workflow = StateGraph(DebateNodeState)\n    workflow.add_node(\"debate\", node.run_debate)\n    workflow.set_entry_point(\"debate\")\n    workflow.add_edge(\"debate\", END)\n\n    app = workflow.compile()\n\n    result = await app.ainvoke({\n        \"topic\": \"Should we adopt AI-powered code review?\",\n        \"positions\": {\n            \"pro\": \"supports AI code review adoption\",\n            \"con\": \"opposes AI code review due to limitations\",\n        },\n    })\n\n    print(f\"Verdict: {result['verdict']['decision']}\")\n    print(f\"Confidence: {result['verdict']['confidence']:.0%}\")\n\nasyncio.run(run_with_safety())\n</code></pre>"},{"location":"examples/langgraph-workflow/#next-steps","title":"Next Steps","text":"<ul> <li>See Basic Debate for fundamentals</li> <li>Add Safety Monitors to workflows</li> <li>Learn about CrewAI Integration</li> </ul>"},{"location":"examples/multi-agent/","title":"Multi-Agent Debates","text":"<p>This example demonstrates debates with three or more agents, each bringing different perspectives beyond simple pro/con positions.</p>"},{"location":"examples/multi-agent/#three-perspective-debate","title":"Three-Perspective Debate","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_three_perspective_debate():\n    # Create agents with distinct perspectives\n    # Note: 'role' is required for all agents\n    economic_agent = Agent(\n        name=\"economist\",\n        role=\"Economic policy analyst\",\n        model=\"gpt-4o\",\n    )\n\n    technical_agent = Agent(\n        name=\"technologist\",\n        role=\"Technology and AI researcher\",\n        model=\"gpt-4o\",\n    )\n\n    social_agent = Agent(\n        name=\"sociologist\",\n        role=\"Social impact researcher\",\n        model=\"gpt-4o\",\n    )\n\n    debate = Debate(\n        topic=\"How should society prepare for widespread AI automation?\",\n        agents=[economic_agent, technical_agent, social_agent],\n        rounds=3,\n    )\n\n    # Assign distinct perspectives\n    debate.assign_positions({\n        \"economist\": \"\"\"\n        Focuses on economic implications: job displacement, new job creation,\n        GDP impact, wealth distribution, retraining costs, and market adaptation.\n        Argues from data-driven economic analysis.\n        \"\"\",\n        \"technologist\": \"\"\"\n        Focuses on technical realities: current AI capabilities, timeline\n        predictions, safety considerations, and technological solutions.\n        Argues from technical feasibility and innovation potential.\n        \"\"\",\n        \"sociologist\": \"\"\"\n        Focuses on social impact: community disruption, mental health,\n        identity and purpose, inequality, and social cohesion.\n        Argues from human welfare and social stability.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    # Multi-perspective verdict considers all viewpoints\n    print(\"MULTI-PERSPECTIVE ANALYSIS\")\n    print(\"=\" * 60)\n    print(f\"\\nTopic: {result.topic}\")\n    print(f\"\\nVerdict: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"\\nSynthesis:\\n{result.verdict.reasoning}\")\n\n    # Show each perspective's key arguments\n    print(\"\\n\" + \"=\" * 60)\n    print(\"KEY ARGUMENTS BY PERSPECTIVE\")\n    print(\"=\" * 60)\n\n    for agent_name in [\"economist\", \"technologist\", \"sociologist\"]:\n        agent_turns = [t for t in result.transcript if t.agent == agent_name]\n        print(f\"\\n{agent_name.upper()}:\")\n        for turn in agent_turns[:2]:  # First two turns\n            content = turn.argument.content\n            print(f\"  Round {turn.round}: {content[:150]}...\")\n\nasyncio.run(run_three_perspective_debate())\n</code></pre>"},{"location":"examples/multi-agent/#stakeholder-debate","title":"Stakeholder Debate","text":"<p>Model a debate between different stakeholders:</p> <pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_stakeholder_debate():\n    # Different stakeholders in a policy decision\n    agents = [\n        Agent(\n            name=\"business_leader\",\n            role=\"Business executive representing company interests\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"labor_union\",\n            role=\"Labor union representative for worker rights\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"government_regulator\",\n            role=\"Government regulator ensuring compliance\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"consumer_advocate\",\n            role=\"Consumer rights advocate\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"Should gig economy workers be classified as employees?\",\n        agents=agents,\n        rounds=2,\n    )\n\n    debate.assign_positions({\n        \"business_leader\": \"\"\"\n        Represents business interests: flexibility, innovation, cost efficiency,\n        entrepreneurship opportunities. Argues for independent contractor status\n        with some protections.\n        \"\"\",\n        \"labor_union\": \"\"\"\n        Represents worker interests: job security, benefits, collective bargaining,\n        fair wages. Argues for full employee classification with all protections.\n        \"\"\",\n        \"government_regulator\": \"\"\"\n        Represents regulatory perspective: enforcement challenges, tax implications,\n        social safety net, fair competition. Argues for balanced framework.\n        \"\"\",\n        \"consumer_advocate\": \"\"\"\n        Represents consumer interests: service quality, pricing, availability,\n        accountability. Argues for whatever best serves consumer welfare.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    # Analyze stakeholder positions\n    print(\"STAKEHOLDER ANALYSIS\")\n    print(\"=\" * 60)\n\n    for turn in result.transcript:\n        if turn.round == 1:  # Opening positions\n            print(f\"\\n{turn.agent.upper()} POSITION:\")\n            content = turn.argument.content\n            print(f\"{content[:300]}...\")\n            print()\n\n    print(\"\\nFINAL SYNTHESIS:\")\n    print(result.verdict.reasoning)\n\nasyncio.run(run_stakeholder_debate())\n</code></pre>"},{"location":"examples/multi-agent/#expert-panel","title":"Expert Panel","text":"<p>Simulate an expert panel discussion:</p> <pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.types import DebateConfig\n\nasync def run_expert_panel():\n    # DebateConfig with actual supported fields\n    config = DebateConfig(\n        turn_timeout=120,         # Allow longer turns for expert analysis\n        round_timeout=600,        # Allow longer rounds\n        safety_mode=\"passive\",    # Monitor but don't interrupt\n    )\n\n    # Panel of domain experts\n    experts = [\n        Agent(\n            name=\"ai_researcher\",\n            role=\"AI/ML research scientist\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"ethicist\",\n            role=\"AI ethics philosopher\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"policy_expert\",\n            role=\"Technology policy analyst\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"industry_practitioner\",\n            role=\"AI industry practitioner\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"What governance framework should guide AGI development?\",\n        agents=experts,\n        rounds=2,\n        config=config,\n    )\n\n    debate.assign_positions({\n        \"ai_researcher\": \"\"\"\n        Expert in AI/ML research. Focuses on technical safety, alignment\n        problems, capability assessment, and research governance. Provides\n        technical grounding for governance discussions.\n        \"\"\",\n        \"ethicist\": \"\"\"\n        Expert in AI ethics and philosophy. Focuses on moral frameworks,\n        value alignment, rights and responsibilities, long-term implications.\n        Provides ethical grounding for governance decisions.\n        \"\"\",\n        \"policy_expert\": \"\"\"\n        Expert in technology policy and regulation. Focuses on regulatory\n        mechanisms, international coordination, enforcement, and precedents.\n        Provides practical governance mechanisms.\n        \"\"\",\n        \"industry_practitioner\": \"\"\"\n        Expert from AI industry. Focuses on practical implementation,\n        innovation impact, competitive dynamics, and industry self-regulation.\n        Provides industry perspective on feasibility.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    # Expert panel summary\n    print(\"EXPERT PANEL: AGI GOVERNANCE\")\n    print(\"=\" * 60)\n    print(f\"\\nPanel Verdict: {result.verdict.decision}\")\n    print(f\"Consensus Level: {result.verdict.confidence:.0%}\")\n\n    print(\"\\nEXPERT CONTRIBUTIONS:\")\n    for expert in [\"ai_researcher\", \"ethicist\", \"policy_expert\", \"industry_practitioner\"]:\n        turns = [t for t in result.transcript if t.agent == expert]\n        if turns:\n            print(f\"\\n{expert.upper()}:\")\n            # Show evidence provided\n            for turn in turns:\n                if turn.argument.evidence:\n                    print(\"  Evidence cited:\")\n                    for e in turn.argument.evidence[:2]:\n                        print(f\"    - [{e.type}] {e.source}\")\n\n    print(f\"\\nPANEL SYNTHESIS:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_expert_panel())\n</code></pre>"},{"location":"examples/multi-agent/#adversarial-mediator-pattern","title":"Adversarial + Mediator Pattern","text":"<p>Include a neutral mediator among adversarial agents:</p> <pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_mediated_debate():\n    agents = [\n        Agent(\n            name=\"advocate\",\n            role=\"Strong proponent of platform liability\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"critic\",\n            role=\"Strong opponent of platform liability\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"mediator\",\n            role=\"Neutral mediator seeking common ground\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    debate = Debate(\n        topic=\"Should social media platforms be liable for user-generated content?\",\n        agents=agents,\n        rounds=3,\n    )\n\n    debate.assign_positions({\n        \"advocate\": \"\"\"\n        Strongly argues FOR platform liability. Focuses on harms caused,\n        platform profits, power imbalance, and need for accountability.\n        Takes an adversarial position pushing for maximum liability.\n        \"\"\",\n        \"critic\": \"\"\"\n        Strongly argues AGAINST platform liability. Focuses on free speech,\n        innovation impact, technical impossibility, and unintended consequences.\n        Takes an adversarial position opposing any liability.\n        \"\"\",\n        \"mediator\": \"\"\"\n        Neutral mediator seeking middle ground. Acknowledges valid points\n        from both sides, identifies areas of agreement, proposes balanced\n        solutions. Does not take a strong position but synthesizes.\n        \"\"\",\n    })\n\n    result = await debate.run()\n\n    print(\"MEDIATED DEBATE\")\n    print(\"=\" * 60)\n\n    # Show the dialectic progression\n    for round_num in range(1, 4):\n        print(f\"\\n--- ROUND {round_num} ---\")\n        round_turns = [t for t in result.transcript if t.round == round_num]\n        for turn in round_turns:\n            role = \"+\" if turn.agent == \"advocate\" else \"-\" if turn.agent == \"critic\" else \"=\"\n            print(f\"\\n[{role}] {turn.agent.upper()}:\")\n            content = turn.argument.content\n            print(f\"{content[:200]}...\")\n\n    print(f\"\\n\\nMEDIATED CONCLUSION:\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_mediated_debate())\n</code></pre>"},{"location":"examples/multi-agent/#dynamic-agent-count","title":"Dynamic Agent Count","text":"<p>Programmatically create agents based on topic:</p> <pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_dynamic_multi_agent(topic: str, perspectives: list[dict]):\n    \"\"\"\n    Create a debate with dynamically defined perspectives.\n\n    Args:\n        topic: The debate topic\n        perspectives: List of dicts with 'name', 'role', and 'position' keys\n    \"\"\"\n    # Create agents for each perspective\n    agents = [\n        Agent(\n            name=p[\"name\"],\n            role=p[\"role\"],  # Role is required\n            model=\"gpt-4o\",\n        )\n        for p in perspectives\n    ]\n\n    debate = Debate(\n        topic=topic,\n        agents=agents,\n        rounds=2,\n    )\n\n    # Assign positions\n    positions = {p[\"name\"]: p[\"position\"] for p in perspectives}\n    debate.assign_positions(positions)\n\n    result = await debate.run()\n    return result\n\n# Example usage\nasync def main():\n    perspectives = [\n        {\n            \"name\": \"optimist\",\n            \"role\": \"Technology optimist\",\n            \"position\": \"Focuses on potential benefits and opportunities\"\n        },\n        {\n            \"name\": \"pessimist\",\n            \"role\": \"Technology skeptic\",\n            \"position\": \"Focuses on risks and potential downsides\"\n        },\n        {\n            \"name\": \"pragmatist\",\n            \"role\": \"Practical implementer\",\n            \"position\": \"Focuses on practical implementation challenges\"\n        },\n        {\n            \"name\": \"visionary\",\n            \"role\": \"Long-term strategist\",\n            \"position\": \"Focuses on long-term transformative potential\"\n        },\n    ]\n\n    result = await run_dynamic_multi_agent(\n        topic=\"Will quantum computing revolutionize cryptography?\",\n        perspectives=perspectives,\n    )\n\n    print(f\"Verdict: {result.verdict.decision}\")\n    print(f\"Reasoning: {result.verdict.reasoning}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/multi-agent/#next-steps","title":"Next Steps","text":"<ul> <li>Create Custom Juries for multi-agent evaluation</li> <li>Add Safety Monitors to track all agents</li> <li>Learn about Ethical Dilemmas with multiple perspectives</li> </ul>"},{"location":"examples/safety-monitors/","title":"Safety Monitors Example","text":"<p>This example demonstrates how to use ARTEMIS safety monitors to detect problematic agent behaviors.</p>"},{"location":"examples/safety-monitors/#basic-safety-setup","title":"Basic Safety Setup","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import (\n    MonitorMode,\n    SandbagDetector,\n    DeceptionMonitor,\n    BehaviorTracker,\n    EthicsGuard,\n    EthicsConfig,\n)\n\nasync def run_safe_debate():\n    # Create individual monitors with actual API parameters\n    sandbag = SandbagDetector(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.7,\n        baseline_turns=3,\n        drop_threshold=0.3,\n    )\n    deception = DeceptionMonitor(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.6,\n    )\n    behavior = BehaviorTracker(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.5,\n        window_size=5,\n        drift_threshold=0.25,\n    )\n    ethics = EthicsGuard(\n        mode=MonitorMode.PASSIVE,\n        config=EthicsConfig(harmful_content_threshold=0.5),\n    )\n\n    # Create agents (role is required)\n    agents = [\n        Agent(\n            name=\"pro\",\n            role=\"Advocate for the proposition\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"con\",\n            role=\"Advocate against the proposition\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    # Create debate with safety monitors\n    # Pass monitor.process methods to safety_monitors parameter\n    debate = Debate(\n        topic=\"Should facial recognition be used in public spaces?\",\n        agents=agents,\n        rounds=3,\n        safety_monitors=[\n            sandbag.process,\n            deception.process,\n            behavior.process,\n            ethics.process,\n        ],\n    )\n\n    debate.assign_positions({\n        \"pro\": \"supports facial recognition in public spaces\",\n        \"con\": \"opposes facial recognition in public spaces\",\n    })\n\n    # Run the debate\n    result = await debate.run()\n\n    # Check for safety alerts\n    print(\"SAFETY REPORT\")\n    print(\"=\" * 60)\n\n    if result.safety_alerts:\n        for alert in result.safety_alerts:\n            print(f\"\\nAlert Type: {alert.type}\")\n            print(f\"Severity: {alert.severity:.2f}\")\n            print(f\"Agent: {alert.agent}\")\n            print(f\"Monitor: {alert.monitor}\")\n    else:\n        print(\"No safety alerts detected.\")\n\nasyncio.run(run_safe_debate())\n</code></pre>"},{"location":"examples/safety-monitors/#using-multiple-monitors","title":"Using Multiple Monitors","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import (\n    MonitorMode,\n    SandbagDetector,\n    DeceptionMonitor,\n    BehaviorTracker,\n    EthicsGuard,\n    EthicsConfig,\n)\n\nasync def run_with_all_monitors():\n    # Create all monitors\n    monitors = [\n        SandbagDetector(mode=MonitorMode.PASSIVE, sensitivity=0.6),\n        DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6),\n        BehaviorTracker(mode=MonitorMode.PASSIVE, sensitivity=0.5, window_size=5),\n        EthicsGuard(mode=MonitorMode.PASSIVE, config=EthicsConfig(harmful_content_threshold=0.5)),\n    ]\n\n    agents = [\n        Agent(name=\"pro\", role=\"Advocate for the topic\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Critic of the topic\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Is genetic engineering of humans ethical?\",\n        agents=agents,\n        rounds=3,\n        safety_monitors=[m.process for m in monitors],\n    )\n\n    debate.assign_positions({\n        \"pro\": \"supports human genetic engineering\",\n        \"con\": \"opposes human genetic engineering\",\n    })\n\n    result = await debate.run()\n\n    # Analyze alerts by type\n    alerts_by_type: dict[str, list] = {}\n    for alert in result.safety_alerts:\n        if alert.type not in alerts_by_type:\n            alerts_by_type[alert.type] = []\n        alerts_by_type[alert.type].append(alert)\n\n    for alert_type, alerts in alerts_by_type.items():\n        print(f\"\\n{alert_type.upper()} ALERTS ({len(alerts)})\")\n        for alert in alerts:\n            print(f\"  Agent {alert.agent}: severity={alert.severity:.2f}\")\n\nasyncio.run(run_with_all_monitors())\n</code></pre>"},{"location":"examples/safety-monitors/#sandbagging-detection","title":"Sandbagging Detection","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import MonitorMode, SandbagDetector\n\nasync def detect_sandbagging():\n    # Configure sandbagging detection\n    detector = SandbagDetector(\n        mode=MonitorMode.ACTIVE,  # Can halt debate\n        sensitivity=0.8,\n        baseline_turns=2,  # Establish baseline over first 2 turns\n        drop_threshold=0.3,  # Flag 30% drops from baseline\n    )\n\n    agents = [\n        Agent(name=\"agent_a\", role=\"First debater\", model=\"gpt-4o\"),\n        Agent(name=\"agent_b\", role=\"Second debater\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Should cryptocurrency replace fiat currency?\",\n        agents=agents,\n        rounds=4,  # More rounds for baseline establishment\n        safety_monitors=[detector.process],\n    )\n\n    debate.assign_positions({\n        \"agent_a\": \"supports cryptocurrency adoption\",\n        \"agent_b\": \"supports fiat currency\",\n    })\n\n    result = await debate.run()\n\n    # Analyze sandbagging detection results\n    print(\"SANDBAGGING ANALYSIS\")\n    print(\"=\" * 60)\n\n    sandbagging_alerts = [a for a in result.safety_alerts if \"sandbag\" in a.type.lower()]\n\n    if sandbagging_alerts:\n        for alert in sandbagging_alerts:\n            print(f\"\\nAgent: {alert.agent}\")\n            print(f\"Severity: {alert.severity:.2f}\")\n            for indicator in alert.indicators:\n                print(f\"  Signal: {indicator.type.value}\")\n                print(f\"  Evidence: {indicator.evidence}\")\n    else:\n        print(\"No sandbagging detected.\")\n\nasyncio.run(detect_sandbagging())\n</code></pre>"},{"location":"examples/safety-monitors/#deception-monitoring","title":"Deception Monitoring","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import MonitorMode, DeceptionMonitor\n\nasync def detect_deception():\n    # Configure deception monitoring\n    monitor = DeceptionMonitor(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.7,\n    )\n\n    agents = [\n        Agent(name=\"claimant\", role=\"Makes the primary claim\", model=\"gpt-4o\"),\n        Agent(name=\"challenger\", role=\"Challenges the claim\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Is nuclear energy the solution to climate change?\",\n        agents=agents,\n        rounds=3,\n        safety_monitors=[monitor.process],\n    )\n\n    debate.assign_positions({\n        \"claimant\": \"argues nuclear is the primary solution\",\n        \"challenger\": \"argues against nuclear as primary solution\",\n    })\n\n    result = await debate.run()\n\n    # Analyze deception alerts\n    print(\"DECEPTION ANALYSIS\")\n    print(\"=\" * 60)\n\n    deception_alerts = [a for a in result.safety_alerts if \"deception\" in a.type.lower()]\n\n    for alert in deception_alerts:\n        print(f\"\\nAgent: {alert.agent}\")\n        print(f\"Severity: {alert.severity:.2f}\")\n\n        for indicator in alert.indicators:\n            print(f\"  Type: {indicator.type.value}\")\n            print(f\"  Evidence: {indicator.evidence}\")\n\nasyncio.run(detect_deception())\n</code></pre>"},{"location":"examples/safety-monitors/#ethics-guard","title":"Ethics Guard","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import (\n    MonitorMode,\n    EthicsGuard,\n    EthicsConfig,\n)\n\nasync def run_with_ethics():\n    # Configure ethics enforcement\n    ethics_config = EthicsConfig(\n        harmful_content_threshold=0.3,\n        bias_threshold=0.4,\n        fairness_threshold=0.3,\n        enabled_checks=[\"harmful_content\", \"bias\", \"fairness\", \"privacy\"],\n    )\n\n    guard = EthicsGuard(\n        mode=MonitorMode.ACTIVE,  # Can halt on severe violations\n        config=ethics_config,\n    )\n\n    agents = [\n        Agent(name=\"security\", role=\"Security advocate\", model=\"gpt-4o\"),\n        Agent(name=\"privacy\", role=\"Privacy advocate\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Should employers monitor employee communications?\",\n        agents=agents,\n        rounds=3,\n        safety_monitors=[guard.process],\n    )\n\n    debate.assign_positions({\n        \"security\": \"supports employer monitoring for security\",\n        \"privacy\": \"opposes employer monitoring for privacy\",\n    })\n\n    result = await debate.run()\n\n    # Analyze ethics alerts\n    print(\"ETHICS ANALYSIS\")\n    print(\"=\" * 60)\n\n    ethics_alerts = [a for a in result.safety_alerts if \"ethics\" in a.type.lower()]\n\n    for alert in ethics_alerts:\n        print(f\"\\nAgent: {alert.agent}\")\n        print(f\"Severity: {alert.severity:.2f}\")\n        for indicator in alert.indicators:\n            print(f\"  Evidence: {indicator.evidence}\")\n\nasyncio.run(run_with_ethics())\n</code></pre>"},{"location":"examples/safety-monitors/#behavior-tracking","title":"Behavior Tracking","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import MonitorMode, BehaviorTracker\n\nasync def track_behavior():\n    # Configure behavior tracking\n    tracker = BehaviorTracker(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.6,\n        window_size=5,\n        drift_threshold=0.25,  # Threshold for style drift detection\n    )\n\n    agents = [\n        Agent(name=\"agent_a\", role=\"First perspective\", model=\"gpt-4o\"),\n        Agent(name=\"agent_b\", role=\"Second perspective\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Should social media be regulated like traditional media?\",\n        agents=agents,\n        rounds=5,  # More rounds to observe drift\n        safety_monitors=[tracker.process],\n    )\n\n    debate.assign_positions({\n        \"agent_a\": \"supports social media regulation\",\n        \"agent_b\": \"opposes social media regulation\",\n    })\n\n    result = await debate.run()\n\n    # Get behavior tracking results\n    print(\"BEHAVIOR TRACKING\")\n    print(\"=\" * 60)\n\n    # Show drift alerts\n    drift_alerts = [a for a in result.safety_alerts if \"drift\" in a.type.lower() or \"behavior\" in a.type.lower()]\n    if drift_alerts:\n        print(\"\\nDrift Alerts:\")\n        for alert in drift_alerts:\n            print(f\"  Agent {alert.agent}: severity={alert.severity:.2f}\")\n    else:\n        print(\"\\nNo behavior drift detected.\")\n\nasyncio.run(track_behavior())\n</code></pre>"},{"location":"examples/safety-monitors/#active-mode-with-safety-config","title":"Active Mode with Safety Config","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.types import DebateConfig\nfrom artemis.safety import (\n    MonitorMode,\n    SandbagDetector,\n    DeceptionMonitor,\n)\n\nasync def run_with_active_monitoring():\n    # Create monitors in active mode - they can contribute to halt decisions\n    sandbag = SandbagDetector(\n        mode=MonitorMode.ACTIVE,\n        sensitivity=0.8,\n    )\n    deception = DeceptionMonitor(\n        mode=MonitorMode.ACTIVE,\n        sensitivity=0.7,\n    )\n\n    agents = [\n        Agent(name=\"advocate\", role=\"Policy advocate\", model=\"gpt-4o\"),\n        Agent(name=\"critic\", role=\"Policy critic\", model=\"gpt-4o\"),\n    ]\n\n    # Enable halt on safety violation in config\n    config = DebateConfig(\n        safety_mode=\"active\",\n        halt_on_safety_violation=True,\n    )\n\n    debate = Debate(\n        topic=\"Should AI systems be allowed to make medical diagnoses?\",\n        agents=agents,\n        rounds=3,\n        config=config,\n        safety_monitors=[sandbag.process, deception.process],\n    )\n\n    debate.assign_positions({\n        \"advocate\": \"supports AI medical diagnosis\",\n        \"critic\": \"opposes AI medical diagnosis\",\n    })\n\n    result = await debate.run()\n    print(f\"Debate completed. Verdict: {result.verdict.decision}\")\n\n    # Show any alerts\n    if result.safety_alerts:\n        print(f\"\\nSafety warnings raised: {len(result.safety_alerts)}\")\n        for alert in result.safety_alerts:\n            print(f\"  - {alert.type}: {alert.severity:.2f}\")\n\nasyncio.run(run_with_active_monitoring())\n</code></pre>"},{"location":"examples/safety-monitors/#comprehensive-safety-setup","title":"Comprehensive Safety Setup","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.types import DebateConfig\nfrom artemis.safety import (\n    MonitorMode,\n    SandbagDetector,\n    DeceptionMonitor,\n    BehaviorTracker,\n    EthicsGuard,\n    EthicsConfig,\n)\n\nasync def run_comprehensive_safety():\n    # Create all safety monitors\n    sandbag = SandbagDetector(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.7,\n        baseline_turns=3,\n    )\n    deception = DeceptionMonitor(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.6,\n    )\n    behavior = BehaviorTracker(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.5,\n        window_size=5,\n    )\n    ethics = EthicsGuard(\n        mode=MonitorMode.PASSIVE,\n        config=EthicsConfig(\n            harmful_content_threshold=0.4,\n            bias_threshold=0.4,\n            fairness_threshold=0.3,\n            enabled_checks=[\"harmful_content\", \"bias\", \"fairness\"],\n        ),\n    )\n\n    agents = [\n        Agent(name=\"pro\", role=\"Proposition advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Opposition advocate\", model=\"gpt-4o\"),\n    ]\n\n    config = DebateConfig(\n        safety_mode=\"passive\",\n    )\n\n    debate = Debate(\n        topic=\"Should programming be taught in elementary schools?\",\n        agents=agents,\n        rounds=3,\n        config=config,\n        safety_monitors=[\n            sandbag.process,\n            deception.process,\n            behavior.process,\n            ethics.process,\n        ],\n    )\n\n    debate.assign_positions({\n        \"pro\": \"supports early programming education\",\n        \"con\": \"opposes mandatory programming in elementary schools\",\n    })\n\n    result = await debate.run()\n\n    print(f\"\\nDebate completed. Verdict: {result.verdict.decision}\")\n    print(f\"Total safety alerts: {len(result.safety_alerts)}\")\n\n    # Get alerts grouped by monitor\n    alerts_by_monitor: dict[str, list] = {}\n    for alert in result.safety_alerts:\n        if alert.monitor not in alerts_by_monitor:\n            alerts_by_monitor[alert.monitor] = []\n        alerts_by_monitor[alert.monitor].append(alert)\n\n    for monitor_name, alerts in alerts_by_monitor.items():\n        print(f\"\\n{monitor_name}: {len(alerts)} alerts\")\n        for alert in alerts[:3]:  # Show first 3\n            print(f\"  - {alert.type}: severity={alert.severity:.2f}\")\n\nasyncio.run(run_comprehensive_safety())\n</code></pre>"},{"location":"examples/safety-monitors/#next-steps","title":"Next Steps","text":"<ul> <li>See Basic Debate for debate fundamentals</li> <li>Create LangGraph Workflows with safety integration</li> <li>Learn about Ethical Dilemmas for ethics-focused debates</li> </ul>"},{"location":"examples/streaming/","title":"Working with Debate Results","text":"<p>This example demonstrates how to work with ARTEMIS debate results and implement progress tracking patterns.</p> <p>Note: Real-time streaming of debate turns is a planned feature. Currently, debates run asynchronously and return complete results. The patterns below show how to work effectively with the async API.</p>"},{"location":"examples/streaming/#basic-async-execution","title":"Basic Async Execution","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_basic_debate():\n    agents = [\n        Agent(name=\"pro\", role=\"Proposition advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Opposition advocate\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Should companies require return-to-office?\",\n        agents=agents,\n        rounds=3,\n    )\n\n    debate.assign_positions({\n        \"pro\": \"supports return-to-office policies\",\n        \"con\": \"supports remote work flexibility\",\n    })\n\n    print(\"DEBATE: Return to Office\")\n    print(\"=\" * 60)\n    print(\"Running debate... (this may take a moment)\")\n\n    # Run the debate - returns complete result\n    result = await debate.run()\n\n    # Process results after completion\n    print(f\"\\nCompleted {len(result.transcript)} turns\")\n\n    for turn in result.transcript:\n        print(f\"\\n[Round {turn.round}] {turn.agent.upper()}\")\n        print(\"-\" * 40)\n        print(turn.argument.content[:300] + \"...\" if len(turn.argument.content) &gt; 300 else turn.argument.content)\n\n        if turn.evaluation:\n            print(f\"\\n  Score: {turn.evaluation.total_score:.1f}/10\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"VERDICT: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"\\n{result.verdict.reasoning}\")\n\nasyncio.run(run_basic_debate())\n</code></pre>"},{"location":"examples/streaming/#running-multiple-debates-concurrently","title":"Running Multiple Debates Concurrently","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_single_debate(topic: str, debate_id: int) -&gt; dict:\n    \"\"\"Run a single debate and return results.\"\"\"\n    agents = [\n        Agent(name=\"pro\", role=\"Advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Critic\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(topic=topic, agents=agents, rounds=2)\n    debate.assign_positions({\n        \"pro\": \"supports the proposition\",\n        \"con\": \"opposes the proposition\",\n    })\n\n    print(f\"[Debate {debate_id}] Starting: {topic[:40]}...\")\n    result = await debate.run()\n    print(f\"[Debate {debate_id}] Completed: {result.verdict.decision}\")\n\n    return {\n        \"id\": debate_id,\n        \"topic\": topic,\n        \"verdict\": result.verdict.decision,\n        \"confidence\": result.verdict.confidence,\n    }\n\nasync def run_parallel_debates():\n    \"\"\"Run multiple debates concurrently.\"\"\"\n    topics = [\n        \"Should AI development be open-sourced?\",\n        \"Is blockchain technology overhyped?\",\n        \"Should programming be taught in elementary schools?\",\n    ]\n\n    print(\"Starting parallel debates...\")\n    print(\"=\" * 60)\n\n    # Run all debates concurrently\n    tasks = [\n        run_single_debate(topic, i)\n        for i, topic in enumerate(topics, 1)\n    ]\n\n    results = await asyncio.gather(*tasks)\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ALL RESULTS:\")\n    for r in results:\n        print(f\"\\nDebate {r['id']}: {r['topic'][:40]}...\")\n        print(f\"  Verdict: {r['verdict']} ({r['confidence']:.0%} confidence)\")\n\nasyncio.run(run_parallel_debates())\n</code></pre>"},{"location":"examples/streaming/#progress-tracking-with-task-wrapper","title":"Progress Tracking with Task Wrapper","text":"<pre><code>import asyncio\nimport time\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nclass DebateRunner:\n    \"\"\"Wrapper for running debates with progress tracking.\"\"\"\n\n    def __init__(self, topic: str, agents: list[Agent], rounds: int = 3):\n        self.topic = topic\n        self.debate = Debate(topic=topic, agents=agents, rounds=rounds)\n        self.start_time = None\n        self.end_time = None\n\n    async def run_with_progress(self, positions: dict[str, str]) -&gt; dict:\n        \"\"\"Run debate and track timing.\"\"\"\n        self.debate.assign_positions(positions)\n\n        self.start_time = time.time()\n        print(f\"Starting debate on: {self.topic[:50]}...\")\n\n        result = await self.debate.run()\n\n        self.end_time = time.time()\n        duration = self.end_time - self.start_time\n\n        return {\n            \"result\": result,\n            \"duration_seconds\": duration,\n            \"turns\": len(result.transcript),\n            \"turns_per_second\": len(result.transcript) / duration if duration &gt; 0 else 0,\n        }\n\nasync def main():\n    agents = [\n        Agent(name=\"optimist\", role=\"Technology optimist\", model=\"gpt-4o\"),\n        Agent(name=\"realist\", role=\"Pragmatic realist\", model=\"gpt-4o\"),\n    ]\n\n    runner = DebateRunner(\n        topic=\"Will AGI be achieved within 10 years?\",\n        agents=agents,\n        rounds=2,\n    )\n\n    stats = await runner.run_with_progress({\n        \"optimist\": \"believes AGI will be achieved within 10 years\",\n        \"realist\": \"believes AGI is further away than optimists think\",\n    })\n\n    print(f\"\\nDebate completed in {stats['duration_seconds']:.1f}s\")\n    print(f\"Total turns: {stats['turns']}\")\n    print(f\"Verdict: {stats['result'].verdict.decision}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/streaming/#processing-results-with-rich-console","title":"Processing Results with Rich Console","text":"<pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\n# Install: pip install rich\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\n\nconsole = Console()\n\nasync def run_rich_debate():\n    agents = [\n        Agent(name=\"pro\", role=\"Cryptocurrency advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Traditional finance advocate\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Is cryptocurrency the future of finance?\",\n        agents=agents,\n        rounds=2,\n    )\n\n    debate.assign_positions({\n        \"pro\": \"argues cryptocurrency will transform finance\",\n        \"con\": \"argues traditional finance will remain dominant\",\n    })\n\n    console.print(Panel.fit(\n        \"[bold blue]ARTEMIS Debate[/bold blue]\\n\"\n        f\"Topic: {debate.topic}\",\n        title=\"Starting Debate\"\n    ))\n\n    with console.status(\"[cyan]Running debate...[/cyan]\"):\n        result = await debate.run()\n\n    # Display results with rich formatting\n    for turn in result.transcript:\n        agent_color = \"green\" if turn.agent == \"pro\" else \"red\"\n        panel = Panel(\n            turn.argument.content,\n            title=f\"[bold {agent_color}]Round {turn.round} - {turn.agent.upper()}[/bold {agent_color}]\",\n            subtitle=f\"Level: {turn.argument.level.value}\",\n            border_style=agent_color,\n        )\n        console.print(panel)\n\n        if turn.argument.evidence:\n            table = Table(title=\"Evidence\", show_header=True)\n            table.add_column(\"Type\")\n            table.add_column(\"Source\")\n            for e in turn.argument.evidence[:3]:\n                table.add_row(e.type, e.source)\n            console.print(table)\n\n    # Verdict\n    verdict = result.verdict\n    decision_color = \"green\" if \"pro\" in verdict.decision else \"red\" if \"con\" in verdict.decision else \"yellow\"\n\n    console.print(Panel(\n        f\"[bold {decision_color}]{verdict.decision.upper()}[/bold {decision_color}]\\n\\n\"\n        f\"Confidence: {verdict.confidence:.0%}\\n\\n\"\n        f\"{verdict.reasoning}\",\n        title=\"Final Verdict\",\n        border_style=decision_color,\n    ))\n\nasyncio.run(run_rich_debate())\n</code></pre>"},{"location":"examples/streaming/#langgraph-integration-for-step-by-step-execution","title":"LangGraph Integration for Step-by-Step Execution","text":"<p>For step-by-step execution with intermediate state access, use the LangGraph integration:</p> <pre><code>import asyncio\nfrom artemis.integrations.langgraph import create_debate_workflow\n\nasync def run_stepwise_debate():\n    # Create step-by-step workflow\n    workflow = create_debate_workflow(\n        model=\"gpt-4o\",\n        step_by_step=True,\n    )\n\n    initial_state = {\n        \"topic\": \"Should we adopt microservices architecture?\",\n        \"agents\": [\n            {\"name\": \"architect\", \"role\": \"System Architect\", \"position\": \"pro microservices\"},\n            {\"name\": \"pragmatist\", \"role\": \"Senior Dev\", \"position\": \"pro monolith\"},\n        ],\n        \"rounds\": 2,\n    }\n\n    print(\"Starting stepwise debate...\")\n\n    # In LangGraph, you can checkpoint and observe state between steps\n    result = await workflow.ainvoke(initial_state)\n\n    print(f\"\\nPhase: {result['phase']}\")\n    print(f\"Verdict: {result['verdict']['decision']}\")\n    print(f\"Confidence: {result['verdict']['confidence']:.0%}\")\n\nasyncio.run(run_stepwise_debate())\n</code></pre>"},{"location":"examples/streaming/#sending-results-to-websocket","title":"Sending Results to WebSocket","text":"<pre><code>import asyncio\nimport json\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_debate_for_websocket(topic: str) -&gt; dict:\n    \"\"\"Run debate and format for WebSocket transmission.\"\"\"\n    agents = [\n        Agent(name=\"pro\", role=\"Advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Critic\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(topic=topic, agents=agents, rounds=2)\n    debate.assign_positions({\n        \"pro\": \"supports the proposition\",\n        \"con\": \"opposes the proposition\",\n    })\n\n    result = await debate.run()\n\n    # Format for JSON transmission\n    return {\n        \"type\": \"debate_complete\",\n        \"topic\": topic,\n        \"transcript\": [\n            {\n                \"round\": turn.round,\n                \"agent\": turn.agent,\n                \"level\": turn.argument.level.value,\n                \"content\": turn.argument.content,\n                \"score\": turn.evaluation.total_score if turn.evaluation else None,\n            }\n            for turn in result.transcript\n        ],\n        \"verdict\": {\n            \"decision\": result.verdict.decision,\n            \"confidence\": result.verdict.confidence,\n            \"reasoning\": result.verdict.reasoning,\n        },\n    }\n\n# Example FastAPI endpoint\n\"\"\"\nfrom fastapi import FastAPI, WebSocket\n\napp = FastAPI()\n\n@app.websocket(\"/debate\")\nasync def debate_endpoint(websocket: WebSocket):\n    await websocket.accept()\n\n    data = await websocket.receive_json()\n    topic = data.get(\"topic\", \"Default topic\")\n\n    # Send start message\n    await websocket.send_json({\"type\": \"debate_started\", \"topic\": topic})\n\n    # Run debate\n    result = await run_debate_for_websocket(topic)\n\n    # Send complete result\n    await websocket.send_json(result)\n\"\"\"\n</code></pre>"},{"location":"examples/streaming/#buffering-results-for-late-consumers","title":"Buffering Results for Late Consumers","text":"<pre><code>import asyncio\nfrom collections import deque\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nclass DebateResultBuffer:\n    \"\"\"Buffer debate results for consumers that join late.\"\"\"\n\n    def __init__(self, max_size: int = 100):\n        self.buffer = deque(maxlen=max_size)\n        self.consumers = []\n\n    def add_result(self, result: dict):\n        \"\"\"Add a result to the buffer.\"\"\"\n        self.buffer.append(result)\n\n        # Notify all consumers\n        for consumer in self.consumers:\n            try:\n                consumer(result)\n            except Exception as e:\n                print(f\"Consumer error: {e}\")\n\n    def add_consumer(self, callback):\n        \"\"\"Add a consumer callback.\"\"\"\n        self.consumers.append(callback)\n\n    def get_all(self) -&gt; list[dict]:\n        \"\"\"Get all buffered results.\"\"\"\n        return list(self.buffer)\n\nasync def main():\n    buffer = DebateResultBuffer()\n\n    # Add a simple consumer\n    def print_consumer(result):\n        print(f\"New result: {result['verdict']}\")\n\n    buffer.add_consumer(print_consumer)\n\n    # Run a debate\n    agents = [\n        Agent(name=\"pro\", role=\"Advocate\", model=\"gpt-4o\"),\n        Agent(name=\"con\", role=\"Critic\", model=\"gpt-4o\"),\n    ]\n\n    debate = Debate(\n        topic=\"Is remote work more productive?\",\n        agents=agents,\n        rounds=2,\n    )\n    debate.assign_positions({\n        \"pro\": \"argues remote work is more productive\",\n        \"con\": \"argues office work is more productive\",\n    })\n\n    result = await debate.run()\n\n    # Add to buffer\n    buffer.add_result({\n        \"topic\": debate.topic,\n        \"verdict\": result.verdict.decision,\n        \"confidence\": result.verdict.confidence,\n    })\n\n    # Late consumer can get buffered results\n    print(\"\\nBuffered results:\")\n    for r in buffer.get_all():\n        print(f\"  {r['topic'][:30]}... -&gt; {r['verdict']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/streaming/#next-steps","title":"Next Steps","text":"<ul> <li>See Basic Debate for fundamentals</li> <li>Explore LangGraph Workflow for step-by-step execution</li> <li>Add Safety Monitors to track debate safety</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>ARTEMIS provides flexible configuration options for debates, agents, evaluation, and safety monitoring.</p>"},{"location":"getting-started/configuration/#debate-configuration","title":"Debate Configuration","text":"<p>Configure debate behavior using <code>DebateConfig</code>:</p> <pre><code>from artemis.core.types import DebateConfig, EvaluationCriteria\n\n# Custom evaluation criteria weights\ncriteria = EvaluationCriteria(\n    logical_coherence=0.25,\n    evidence_quality=0.25,\n    causal_reasoning=0.20,\n    ethical_alignment=0.15,\n    persuasiveness=0.15,\n)\n\nconfig = DebateConfig(\n    # Timing\n    turn_timeout=60,        # Timeout per turn in seconds\n    round_timeout=300,      # Timeout per round in seconds\n\n    # Argument generation\n    max_argument_tokens=1000,\n    require_evidence=True,\n    require_causal_links=True,\n    min_evidence_per_argument=1,\n\n    # Evaluation\n    evaluation_criteria=criteria,\n    adaptation_enabled=True,\n    adaptation_rate=0.1,\n\n    # Safety\n    safety_mode=\"passive\",  # \"off\", \"passive\", or \"active\"\n    halt_on_safety_violation=False,\n\n    # Logging\n    log_level=\"INFO\",\n    trace_enabled=False,\n)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    config=config,\n)\n</code></pre>"},{"location":"getting-started/configuration/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>turn_timeout</code> int 60 Timeout per turn in seconds <code>round_timeout</code> int 300 Timeout per round in seconds <code>max_argument_tokens</code> int 1000 Max tokens per argument <code>require_evidence</code> bool True Require evidence in arguments <code>require_causal_links</code> bool True Require causal reasoning <code>evaluation_mode</code> EvaluationMode BALANCED Evaluation mode: QUALITY, BALANCED, or FAST <code>adaptation_enabled</code> bool True Enable dynamic criteria adaptation <code>safety_mode</code> str \"passive\" Safety monitoring mode <code>halt_on_safety_violation</code> bool False Stop debate on safety alerts"},{"location":"getting-started/configuration/#evaluation-modes","title":"Evaluation Modes","text":"<p>Control the accuracy vs cost tradeoff for argument evaluation:</p> <pre><code>from artemis.core.types import DebateConfig, EvaluationMode\n\n# Maximum accuracy (benchmarking, critical decisions)\nconfig = DebateConfig(evaluation_mode=EvaluationMode.QUALITY)\n\n# Balanced (default - good accuracy, moderate cost)\nconfig = DebateConfig(evaluation_mode=EvaluationMode.BALANCED)\n\n# Fast (development, testing, cost-sensitive)\nconfig = DebateConfig(evaluation_mode=EvaluationMode.FAST)\n</code></pre> Mode Description Use Case <code>QUALITY</code> LLM-native evaluation for all criteria Benchmarking, critical accuracy <code>BALANCED</code> Selective LLM use (jury + key decisions) Production (default) <code>FAST</code> Heuristic-only evaluation Development, testing <p>See L-AE-CR Evaluation Modes for detailed information.</p>"},{"location":"getting-started/configuration/#agent-configuration","title":"Agent Configuration","text":"<p>Configure individual agents:</p> <pre><code>from artemis.core.agent import Agent\nfrom artemis.core.types import ReasoningConfig\n\n# Basic agent\nagent = Agent(\n    name=\"expert_agent\",\n    role=\"Domain expert analyzing the topic\",\n    model=\"gpt-4o\",\n)\n\n# Agent with reasoning configuration\nreasoning_config = ReasoningConfig(\n    enabled=True,\n    thinking_budget=8000,\n    strategy=\"think-then-argue\",\n    include_trace_in_output=False,\n)\n\nagent_with_reasoning = Agent(\n    name=\"reasoning_agent\",\n    role=\"Deep analyst with extended thinking\",\n    model=\"deepseek-reasoner\",\n    reasoning_config=reasoning_config,\n)\n\n# Agent with persona\npersona_agent = Agent(\n    name=\"philosopher\",\n    role=\"Philosopher analyzing ethical implications\",\n    model=\"gpt-4o\",\n    persona=\"An analytical philosopher who values rigorous logical reasoning\",\n)\n</code></pre>"},{"location":"getting-started/configuration/#agent-options","title":"Agent Options","text":"Option Type Default Description <code>name</code> str required Unique agent identifier <code>role</code> str required Agent's role description <code>model</code> str \"gpt-4o\" LLM model to use <code>position</code> str None Agent's debate position <code>reasoning_config</code> ReasoningConfig None Reasoning model config"},{"location":"getting-started/configuration/#reasoning-configuration","title":"Reasoning Configuration","text":"<pre><code>from artemis.core.types import ReasoningConfig\n\nconfig = ReasoningConfig(\n    enabled=True,\n    thinking_budget=8000,       # Max tokens for thinking (1000-32000)\n    strategy=\"think-then-argue\",  # \"think-then-argue\", \"interleaved\", \"final-reflection\"\n    include_trace_in_output=False,\n)\n</code></pre>"},{"location":"getting-started/configuration/#model-configuration","title":"Model Configuration","text":"<p>Configure LLM providers:</p> <pre><code>from artemis.models import create_model\n\n# OpenAI\nmodel = create_model(\n    \"gpt-4o\",\n    api_key=\"your-key\",  # Or use OPENAI_API_KEY env var\n)\n\n# DeepSeek with reasoning\nmodel = create_model(\n    \"deepseek-reasoner\",\n    api_key=\"your-key\",\n)\n\n# Google/Gemini via AI Studio\nmodel = create_model(\n    \"gemini-2.0-flash\",\n    # Uses GOOGLE_API_KEY or GEMINI_API_KEY env var\n)\n\n# Google/Gemini via Vertex AI (higher rate limits)\nmodel = create_model(\n    \"gemini-2.0-flash\",\n    provider=\"google\",\n    # Uses GOOGLE_CLOUD_PROJECT env var for auto-detection\n)\n\n# Anthropic/Claude\nmodel = create_model(\n    \"claude-sonnet-4-20250514\",\n    # Uses ANTHROPIC_API_KEY env var\n)\n</code></pre>"},{"location":"getting-started/configuration/#google-backend-selection","title":"Google Backend Selection","text":"<p>GoogleModel supports two backends that are auto-detected:</p> Backend When Used Authentication AI Studio Default when no project set <code>GOOGLE_API_KEY</code> or <code>GEMINI_API_KEY</code> Vertex AI When <code>GOOGLE_CLOUD_PROJECT</code> is set Application Default Credentials <pre><code>from artemis.models import GoogleModel\n\n# Force AI Studio (explicit)\nmodel = GoogleModel(\n    model=\"gemini-2.0-flash\",\n    api_key=\"your-api-key\",\n    use_vertex_ai=False,\n)\n\n# Force Vertex AI (explicit)\nmodel = GoogleModel(\n    model=\"gemini-2.0-flash\",\n    project=\"my-gcp-project\",\n    location=\"us-central1\",\n    use_vertex_ai=True,\n)\n</code></pre>"},{"location":"getting-started/configuration/#safety-configuration","title":"Safety Configuration","text":"<p>Configure safety monitors:</p> <pre><code>from artemis.safety import (\n    SandbagDetector,\n    DeceptionMonitor,\n    BehaviorTracker,\n    EthicsGuard,\n    MonitorMode,\n    EthicsConfig,\n)\n\n# Individual monitor configuration\nsandbag = SandbagDetector(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.7,         # 0.0 to 1.0\n    baseline_turns=3,        # Turns to establish baseline\n    drop_threshold=0.3,      # Performance drop threshold\n)\n\ndeception = DeceptionMonitor(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.6,\n)\n\nbehavior = BehaviorTracker(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.5,\n    window_size=5,           # Turns to track\n    drift_threshold=0.3,     # Drift detection threshold\n)\n\nethics_config = EthicsConfig(\n    harmful_content_threshold=0.3,\n    bias_threshold=0.4,\n    fairness_threshold=0.3,\n    enabled_checks=[\"harmful_content\", \"bias\", \"fairness\"],\n)\n\nethics = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=ethics_config,\n)\n\n# Use monitors in debate\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[\n        sandbag.process,\n        deception.process,\n        behavior.process,\n        ethics.process,\n    ],\n)\n</code></pre>"},{"location":"getting-started/configuration/#jury-configuration","title":"Jury Configuration","text":"<p>Configure the jury panel:</p> <pre><code>from artemis.core.jury import JuryPanel\n\njury = JuryPanel(\n    evaluators=5,               # Number of jury members\n    model=\"gpt-4o\",             # Model for jurors\n    consensus_threshold=0.7,    # Required agreement (0-1)\n)\n\n# Use in debate\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    jury=jury,\n)\n</code></pre>"},{"location":"getting-started/configuration/#per-juror-model-configuration","title":"Per-Juror Model Configuration","text":"<p>You can configure different models for each juror using two approaches:</p> <p>Option A: Simple Model List</p> <pre><code>jury = JuryPanel(\n    evaluators=3,\n    models=[\"gpt-4o\", \"claude-sonnet-4-20250514\", \"gemini-2.0-flash\"],\n)\n</code></pre> <p>Option B: Full JurorConfig Objects</p> <pre><code>from artemis.core.types import JurorConfig, JuryPerspective\n\njury = JuryPanel(\n    jurors=[\n        JurorConfig(\n            perspective=JuryPerspective.ANALYTICAL,\n            model=\"gpt-4o\",\n        ),\n        JurorConfig(\n            perspective=JuryPerspective.ETHICAL,\n            model=\"claude-sonnet-4-20250514\",\n        ),\n        JurorConfig(\n            perspective=JuryPerspective.PRACTICAL,\n            model=\"gemini-2.0-flash\",\n        ),\n    ],\n    consensus_threshold=0.7,\n)\n</code></pre>"},{"location":"getting-started/configuration/#jury-perspectives","title":"Jury Perspectives","text":"Perspective Focus <code>ANALYTICAL</code> Logic, evidence quality, reasoning coherence <code>ETHICAL</code> Moral implications, fairness, values alignment <code>PRACTICAL</code> Real-world applicability, feasibility <code>CREATIVE</code> Novel approaches, unconventional solutions <code>SKEPTICAL</code> Critical analysis, identifying weaknesses"},{"location":"getting-started/configuration/#mcp-server-configuration","title":"MCP Server Configuration","text":"<p>Configure the MCP server:</p> <pre><code>from artemis.mcp import ArtemisMCPServer\n\nserver = ArtemisMCPServer(\n    default_model=\"gpt-4o\",\n    max_sessions=100,\n)\n\n# Start HTTP server\nawait server.start(host=\"127.0.0.1\", port=8080)\n\n# Or stdio mode for MCP clients\nawait server.run_stdio()\n</code></pre>"},{"location":"getting-started/configuration/#cli-configuration","title":"CLI Configuration","text":"<pre><code># HTTP mode with options\nartemis-mcp --http --port 8080 --model gpt-4-turbo --max-sessions 50\n\n# Verbose logging\nartemis-mcp --verbose\n\n# Custom host binding\nartemis-mcp --http --host 0.0.0.0 --port 9000\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>ARTEMIS respects these environment variables:</p>"},{"location":"getting-started/configuration/#api-keys","title":"API Keys","text":"Variable Description <code>OPENAI_API_KEY</code> OpenAI API key for GPT models <code>ANTHROPIC_API_KEY</code> Anthropic API key for Claude models <code>DEEPSEEK_API_KEY</code> DeepSeek API key <code>GOOGLE_API_KEY</code> Google AI Studio API key (for Gemini) <code>GEMINI_API_KEY</code> Alternative to <code>GOOGLE_API_KEY</code>"},{"location":"getting-started/configuration/#google-cloud-vertex-ai","title":"Google Cloud / Vertex AI","text":"Variable Description <code>GOOGLE_CLOUD_PROJECT</code> GCP project ID (enables Vertex AI backend) <code>GCP_PROJECT</code> Alternative to <code>GOOGLE_CLOUD_PROJECT</code> <code>GOOGLE_CLOUD_LOCATION</code> GCP region (default: <code>us-central1</code>) <p>When <code>GOOGLE_CLOUD_PROJECT</code> is set, GoogleModel automatically uses Vertex AI with Application Default Credentials instead of AI Studio.</p>"},{"location":"getting-started/configuration/#artemis-settings","title":"ARTEMIS Settings","text":"Variable Description <code>ARTEMIS_LOG_LEVEL</code> Logging level (DEBUG, INFO, WARNING, ERROR) <code>ARTEMIS_DEFAULT_MODEL</code> Default model for debates"},{"location":"getting-started/configuration/#configuration-files","title":"Configuration Files","text":"<p>You can also use configuration files:</p> <pre><code># artemis.yaml\ndefault_model: gpt-4o\nmax_sessions: 100\n\ndebate:\n  turn_timeout: 60\n  round_timeout: 300\n  require_evidence: true\n  safety_mode: passive\n\nsafety:\n  sandbagging_sensitivity: 0.7\n  deception_sensitivity: 0.6\n</code></pre> <p>Load configuration:</p> <pre><code>from artemis.utils.config import load_config\n\nconfig = load_config(\"artemis.yaml\")\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Core Concepts</li> <li>Configure Safety Monitoring</li> <li>See the API Reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>pip or poetry for package management</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install artemis-agents\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<p>For the latest development version:</p> <pre><code>git clone https://github.com/bassrehab/artemis-agents.git\ncd artemis-agents\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>ARTEMIS has optional dependencies for different use cases:</p>"},{"location":"getting-started/installation/#framework-integrations","title":"Framework Integrations","text":"<pre><code># LangChain integration\npip install artemis-agents[langchain]\n\n# LangGraph integration\npip install artemis-agents[langgraph]\n\n# CrewAI integration\npip install artemis-agents[crewai]\n\n# All integrations\npip install artemis-agents[integrations]\n</code></pre>"},{"location":"getting-started/installation/#development","title":"Development","text":"<pre><code># Development tools (testing, linting, etc.)\npip install artemis-agents[dev]\n</code></pre>"},{"location":"getting-started/installation/#all-optional-dependencies","title":"All Optional Dependencies","text":"<pre><code>pip install artemis-agents[all]\n</code></pre>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation/#api-keys","title":"API Keys","text":"<p>ARTEMIS requires API keys for LLM providers. Set them as environment variables:</p> <pre><code># OpenAI (GPT-4o, o1)\nexport OPENAI_API_KEY=\"your-openai-key\"\n\n# Anthropic (Claude)\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\n\n# Google (Gemini)\nexport GOOGLE_API_KEY=\"your-google-key\"\n\n# DeepSeek (R1)\nexport DEEPSEEK_API_KEY=\"your-deepseek-key\"\n</code></pre> <p>Or use a <code>.env</code> file:</p> <pre><code># .env\nOPENAI_API_KEY=your-openai-key\nANTHROPIC_API_KEY=your-anthropic-key\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import artemis\n\nprint(f\"ARTEMIS version: {artemis.__version__}\")\n\n# Check available providers\nfrom artemis.models import list_providers\nprint(f\"Available providers: {list_providers()}\")\n</code></pre>"},{"location":"getting-started/installation/#docker","title":"Docker","text":"<p>Run ARTEMIS in a Docker container:</p> <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nRUN pip install artemis-agents[all]\n\nCOPY . .\n\nCMD [\"python\", \"-m\", \"artemis.mcp.cli\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t artemis-agents .\ndocker run -e OPENAI_API_KEY=$OPENAI_API_KEY artemis-agents\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors, ensure you have the correct Python version:</p> <pre><code>python --version  # Should be 3.10+\n</code></pre>"},{"location":"getting-started/installation/#api-key-issues","title":"API Key Issues","text":"<p>If API calls fail, verify your keys are set:</p> <pre><code>import os\nprint(os.environ.get(\"OPENAI_API_KEY\", \"Not set\"))\n</code></pre>"},{"location":"getting-started/installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>If you have dependency conflicts, try installing in a virtual environment:</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# or: venv\\Scripts\\activate  # Windows\npip install artemis-agents\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installed, proceed to the Quick Start guide to run your first debate.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will help you run your first ARTEMIS debate in minutes.</p>"},{"location":"getting-started/quickstart/#basic-debate","title":"Basic Debate","text":"<p>The simplest way to use ARTEMIS is to create agents and run a debate:</p> <pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\n\nasync def run_debate():\n    # Create two agents with opposing positions\n    pro_agent = Agent(\n        name=\"proponent\",\n        role=\"Advocate arguing in favor of the proposition\",\n        model=\"gpt-4o\",\n    )\n\n    con_agent = Agent(\n        name=\"opponent\",\n        role=\"Advocate arguing against the proposition\",\n        model=\"gpt-4o\",\n    )\n\n    # Create and run the debate\n    debate = Debate(\n        topic=\"Should AI development be regulated by governments?\",\n        agents=[pro_agent, con_agent],\n        rounds=3,\n    )\n\n    # Assign positions\n    debate.assign_positions({\n        \"proponent\": \"supports government regulation of AI\",\n        \"opponent\": \"opposes government regulation of AI\",\n    })\n\n    # Run the debate\n    result = await debate.run()\n\n    # Print results\n    print(f\"Topic: {result.topic}\")\n    print(f\"Verdict: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n    print(f\"Reasoning: {result.verdict.reasoning}\")\n\n# Run it\nasyncio.run(run_debate())\n</code></pre>"},{"location":"getting-started/quickstart/#with-safety-monitoring","title":"With Safety Monitoring","text":"<p>Add safety monitors to detect problematic behavior:</p> <pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import SandbagDetector, DeceptionMonitor, MonitorMode\n\nasync def run_safe_debate():\n    # Create agents\n    agents = [\n        Agent(\n            name=\"pro\",\n            role=\"Advocate arguing for the proposition\",\n            model=\"gpt-4o\",\n        ),\n        Agent(\n            name=\"con\",\n            role=\"Advocate arguing against the proposition\",\n            model=\"gpt-4o\",\n        ),\n    ]\n\n    # Create safety monitors\n    sandbag_detector = SandbagDetector(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.7,\n    )\n    deception_monitor = DeceptionMonitor(\n        mode=MonitorMode.PASSIVE,\n        sensitivity=0.6,\n    )\n\n    # Create debate with safety monitors\n    debate = Debate(\n        topic=\"Should cryptocurrency replace traditional banking?\",\n        agents=agents,\n        rounds=3,\n        safety_monitors=[sandbag_detector.process, deception_monitor.process],\n    )\n\n    debate.assign_positions({\n        \"pro\": \"supports cryptocurrency adoption\",\n        \"con\": \"supports traditional banking\",\n    })\n\n    # Run the debate\n    result = await debate.run()\n\n    # Check for safety alerts\n    if result.safety_alerts:\n        print(\"Safety alerts detected:\")\n        for alert in result.safety_alerts:\n            print(f\"  {alert.type}: {alert.severity:.0%} severity\")\n\n    # Check results in transcript\n    for turn in result.transcript:\n        print(f\"Round {turn.round} - {turn.agent}\")\n        if turn.evaluation:\n            print(f\"  Score: {turn.evaluation.total_score:.1f}\")\n\nasyncio.run(run_safe_debate())\n</code></pre>"},{"location":"getting-started/quickstart/#with-reasoning-models","title":"With Reasoning Models","text":"<p>Use reasoning models (o1, DeepSeek R1) for deeper analysis:</p> <pre><code>import asyncio\nfrom artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.core.types import ReasoningConfig\n\nasync def run_reasoning_debate():\n    # Create reasoning config for extended thinking\n    reasoning_config = ReasoningConfig(\n        enabled=True,\n        thinking_budget=8000,\n        include_trace_in_output=False,\n    )\n\n    # Create agents with reasoning models\n    pro_agent = Agent(\n        name=\"deep_thinker_pro\",\n        role=\"Philosopher arguing consciousness is computable\",\n        model=\"deepseek-reasoner\",  # DeepSeek R1\n        reasoning_config=reasoning_config,\n    )\n\n    con_agent = Agent(\n        name=\"deep_thinker_con\",\n        role=\"Philosopher arguing consciousness is not computable\",\n        model=\"deepseek-reasoner\",\n        reasoning_config=reasoning_config,\n    )\n\n    debate = Debate(\n        topic=\"Is consciousness computable?\",\n        agents=[pro_agent, con_agent],\n        rounds=2,  # Fewer rounds with deeper thinking\n    )\n\n    debate.assign_positions({\n        \"deep_thinker_pro\": \"consciousness can be computed\",\n        \"deep_thinker_con\": \"consciousness cannot be computed\",\n    })\n\n    result = await debate.run()\n\n    print(f\"Verdict: {result.verdict.decision}\")\n    print(f\"Confidence: {result.verdict.confidence:.0%}\")\n\nasyncio.run(run_reasoning_debate())\n</code></pre>"},{"location":"getting-started/quickstart/#using-the-mcp-server","title":"Using the MCP Server","text":"<p>Start ARTEMIS as an MCP server for integration with other tools:</p> <pre><code># Start the server\nartemis-mcp --port 8080\n</code></pre> <p>Then connect from any MCP client:</p> <pre><code># Example: Using the MCP server programmatically\nfrom artemis.mcp import ArtemisMCPServer\n\nasync def use_mcp():\n    server = ArtemisMCPServer(default_model=\"gpt-4o\")\n\n    # Start a debate via tool call\n    result = await server.handle_tool_call(\n        \"artemis_debate_start\",\n        {\n            \"topic\": \"Should remote work be the default?\",\n            \"rounds\": 3,\n        }\n    )\n\n    debate_id = result[\"debate_id\"]\n    print(f\"Started debate: {debate_id}\")\n\n    # Get verdict\n    verdict = await server.handle_tool_call(\n        \"artemis_get_verdict\",\n        {\"debate_id\": debate_id}\n    )\n\n    print(f\"Verdict: {verdict['verdict']}\")\n</code></pre>"},{"location":"getting-started/quickstart/#framework-integration-examples","title":"Framework Integration Examples","text":""},{"location":"getting-started/quickstart/#langchain","title":"LangChain","text":"<pre><code>from artemis.integrations.langchain import ArtemisDebateTool\n\n# Create the tool\ndebate_tool = ArtemisDebateTool(model=\"gpt-4o\")\n\n# Use as a LangChain tool - returns formatted string result\nresult = debate_tool.run(\n    topic=\"Should we adopt microservices?\",\n    rounds=2,\n)\n\nprint(result)  # Formatted debate analysis string\n</code></pre>"},{"location":"getting-started/quickstart/#langgraph","title":"LangGraph","text":"<pre><code>from artemis.integrations.langgraph import create_debate_workflow\n\n# Create a complete debate workflow\nworkflow = create_debate_workflow(model=\"gpt-4o\")\n\n# Run it\nresult = await workflow.ainvoke({\n    \"topic\": \"Is functional programming better than OOP?\",\n    \"rounds\": 2,\n})\n\nprint(f\"Verdict: {result['verdict']['decision']}\")\n</code></pre>"},{"location":"getting-started/quickstart/#crewai","title":"CrewAI","text":"<pre><code>from artemis.integrations.crewai import ArtemisCrewTool\n\n# Create the tool\ncrew_tool = ArtemisCrewTool(model=\"gpt-4o\")\n\n# Run a debate - returns formatted string\nresult = crew_tool.run(\n    topic=\"Should we use NoSQL or SQL for this project?\",\n    rounds=2,\n)\n\nprint(result)  # Formatted debate analysis string\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Core Concepts</li> <li>Explore Safety Monitoring</li> <li>See more Examples</li> <li>Read the API Reference</li> </ul>"},{"location":"integrations/crewai/","title":"CrewAI Integration","text":"<p>ARTEMIS integrates with CrewAI, allowing you to use structured debates as tools within CrewAI crews.</p>"},{"location":"integrations/crewai/#installation","title":"Installation","text":"<pre><code>pip install artemis-agents[crewai]\n</code></pre>"},{"location":"integrations/crewai/#artemiscrewtool","title":"ArtemisCrewTool","text":"<p>The primary integration is the <code>ArtemisCrewTool</code>:</p> <pre><code>from artemis.integrations import ArtemisCrewTool\n\n# Create the tool\ndebate_tool = ArtemisCrewTool(\n    model=\"gpt-4o\",\n    default_rounds=3,\n)\n</code></pre>"},{"location":"integrations/crewai/#basic-usage","title":"Basic Usage","text":""},{"location":"integrations/crewai/#with-a-crew","title":"With a Crew","text":"<pre><code>from crewai import Agent, Task, Crew\nfrom artemis.integrations import ArtemisCrewTool\n\n# Create the debate tool\ndebate_tool = ArtemisCrewTool(model=\"gpt-4o\")\n\n# Create an agent with the tool\nanalyst = Agent(\n    role=\"Decision Analyst\",\n    goal=\"Analyze decisions using structured debate\",\n    backstory=\"Expert at evaluating options through rigorous debate\",\n    tools=[debate_tool],\n)\n\n# Create a task\ntask = Task(\n    description=\"Analyze whether we should migrate to microservices\",\n    agent=analyst,\n    expected_output=\"A detailed analysis with a recommendation\",\n)\n\n# Create and run crew\ncrew = Crew(\n    agents=[analyst],\n    tasks=[task],\n)\n\nresult = crew.kickoff()\n</code></pre>"},{"location":"integrations/crewai/#tool-configuration","title":"Tool Configuration","text":"<pre><code>from artemis.integrations import ArtemisCrewTool\nfrom artemis.core.types import DebateConfig\n\nconfig = DebateConfig(\n    turn_timeout=60,\n    round_timeout=300,\n    require_evidence=True,\n    safety_mode=\"passive\",\n)\n\ntool = ArtemisCrewTool(\n    model=\"gpt-4o\",\n    default_rounds=3,\n    config=config,\n)\n</code></pre>"},{"location":"integrations/crewai/#tool-parameters","title":"Tool Parameters","text":"Parameter Type Default Description <code>model</code> str required LLM model to use <code>default_rounds</code> int 3 Default number of rounds <code>config</code> DebateConfig None Debate configuration <code>name</code> str \"Debate Tool\" Tool display name <code>description</code> str ... Tool description"},{"location":"integrations/crewai/#using-the-tool","title":"Using the Tool","text":""},{"location":"integrations/crewai/#direct-invocation","title":"Direct Invocation","text":"<pre><code>tool = ArtemisCrewTool(model=\"gpt-4o\")\n\nresult = tool.run(\n    topic=\"Should we adopt Kubernetes?\",\n    rounds=2,\n)\n\nprint(f\"Verdict: {result['verdict']}\")\nprint(f\"Confidence: {result['confidence']}\")\nprint(f\"Reasoning: {result['reasoning']}\")\n</code></pre>"},{"location":"integrations/crewai/#with-custom-positions","title":"With Custom Positions","text":"<pre><code>result = tool.run(\n    topic=\"Which cloud provider should we use?\",\n    positions={\n        \"aws_advocate\": \"argues for AWS\",\n        \"azure_advocate\": \"argues for Azure\",\n        \"gcp_advocate\": \"argues for GCP\",\n    },\n)\n</code></pre>"},{"location":"integrations/crewai/#multi-agent-crews","title":"Multi-Agent Crews","text":""},{"location":"integrations/crewai/#decision-crew","title":"Decision Crew","text":"<pre><code>from crewai import Agent, Task, Crew, Process\nfrom artemis.integrations import ArtemisCrewTool\n\ndebate_tool = ArtemisCrewTool(model=\"gpt-4o\")\n\n# Research agent gathers information\nresearcher = Agent(\n    role=\"Research Analyst\",\n    goal=\"Gather relevant information for decisions\",\n    backstory=\"Expert researcher with access to multiple sources\",\n    tools=[search_tool, web_scraper_tool],\n)\n\n# Debate agent analyzes pros and cons\ndebater = Agent(\n    role=\"Debate Analyst\",\n    goal=\"Analyze options through structured debate\",\n    backstory=\"Expert at evaluating arguments from multiple perspectives\",\n    tools=[debate_tool],\n)\n\n# Decision agent makes final call\ndecider = Agent(\n    role=\"Decision Maker\",\n    goal=\"Make informed decisions based on analysis\",\n    backstory=\"Experienced executive who makes data-driven decisions\",\n)\n\n# Tasks\nresearch_task = Task(\n    description=\"Research the implications of {topic}\",\n    agent=researcher,\n    expected_output=\"Comprehensive research findings\",\n)\n\ndebate_task = Task(\n    description=\"Run a structured debate on {topic} using the research findings\",\n    agent=debater,\n    expected_output=\"Debate verdict with reasoning\",\n    context=[research_task],\n)\n\ndecision_task = Task(\n    description=\"Make a final decision based on the debate\",\n    agent=decider,\n    expected_output=\"Clear decision with justification\",\n    context=[debate_task],\n)\n\n# Crew\ncrew = Crew(\n    agents=[researcher, debater, decider],\n    tasks=[research_task, debate_task, decision_task],\n    process=Process.sequential,\n)\n\nresult = crew.kickoff(inputs={\"topic\": \"Adopting AI in our workflow\"})\n</code></pre>"},{"location":"integrations/crewai/#parallel-debates","title":"Parallel Debates","text":"<pre><code># Multiple debate agents for different aspects\ntechnical_debater = Agent(\n    role=\"Technical Analyst\",\n    goal=\"Debate technical aspects\",\n    tools=[ArtemisCrewTool(model=\"gpt-4o\")],\n)\n\nbusiness_debater = Agent(\n    role=\"Business Analyst\",\n    goal=\"Debate business aspects\",\n    tools=[ArtemisCrewTool(model=\"gpt-4o\")],\n)\n\nrisk_debater = Agent(\n    role=\"Risk Analyst\",\n    goal=\"Debate risk aspects\",\n    tools=[ArtemisCrewTool(model=\"gpt-4o\")],\n)\n\n# Hierarchical crew for synthesis\ncrew = Crew(\n    agents=[technical_debater, business_debater, risk_debater, synthesizer],\n    tasks=[tech_task, business_task, risk_task, synthesis_task],\n    process=Process.hierarchical,\n    manager_llm=ChatOpenAI(model=\"gpt-4o\"),\n)\n</code></pre>"},{"location":"integrations/crewai/#safety-integration","title":"Safety Integration","text":"<p>Safety monitoring is configured via the <code>DebateConfig</code>:</p> <pre><code>from artemis.integrations import ArtemisCrewTool\nfrom artemis.core.types import DebateConfig\n\nconfig = DebateConfig(\n    safety_mode=\"active\",\n    halt_on_safety_violation=True,\n)\n\ntool = ArtemisCrewTool(\n    model=\"gpt-4o\",\n    config=config,\n)\n\n# Run debate - safety alerts appear in the formatted result\nresult = tool.run(topic=\"Sensitive topic\")\nprint(result)  # Includes SAFETY section if alerts were generated\n</code></pre>"},{"location":"integrations/crewai/#output-formatting","title":"Output Formatting","text":"<p>The tool returns structured output:</p> <pre><code>result = tool.run(topic=\"Your topic\")\n\n# Result structure\n{\n    \"verdict\": \"pro\" | \"con\" | \"tie\",\n    \"confidence\": 0.0 - 1.0,\n    \"reasoning\": \"Explanation of the verdict\",\n    \"transcript\": [\n        {\n            \"round\": 1,\n            \"agent\": \"proponent\",\n            \"argument\": \"...\",\n        },\n        ...\n    ],\n    \"safety_alerts\": [...],\n    \"metadata\": {\n        \"model\": \"gpt-4o\",\n        \"rounds\": 3,\n        \"duration_seconds\": 45.2,\n    },\n}\n</code></pre>"},{"location":"integrations/crewai/#integration-patterns","title":"Integration Patterns","text":""},{"location":"integrations/crewai/#research-debate-action","title":"Research \u2192 Debate \u2192 Action","text":"<pre><code># 1. Research phase\nresearch_task = Task(\n    description=\"Research {topic}\",\n    agent=researcher,\n)\n\n# 2. Debate phase\ndebate_task = Task(\n    description=\"Debate the research findings\",\n    agent=debater,\n    context=[research_task],\n)\n\n# 3. Action phase\naction_task = Task(\n    description=\"Create action plan based on debate\",\n    agent=planner,\n    context=[debate_task],\n)\n</code></pre>"},{"location":"integrations/crewai/#iterative-refinement","title":"Iterative Refinement","text":"<pre><code># First debate on broad topic\ninitial_debate = Task(\n    description=\"Debate: Should we enter market X?\",\n    agent=debater,\n)\n\n# Refine based on initial result\nrefined_debate = Task(\n    description=\"Debate the specific concerns raised\",\n    agent=debater,\n    context=[initial_debate],\n)\n</code></pre>"},{"location":"integrations/crewai/#error-handling","title":"Error Handling","text":"<pre><code>from artemis.exceptions import DebateError\n\ntry:\n    result = tool.run(topic=\"Your topic\")\nexcept DebateError as e:\n    print(f\"Debate failed: {e}\")\n    # Fallback logic\n</code></pre>"},{"location":"integrations/crewai/#async-support","title":"Async Support","text":"<p>For async crews:</p> <pre><code>from artemis.integrations import ArtemisCrewTool\n\ntool = ArtemisCrewTool(model=\"gpt-4o\")\n\n# Async run\nresult = await tool.arun(topic=\"Your topic\")\n</code></pre>"},{"location":"integrations/crewai/#best-practices","title":"Best Practices","text":"<ol> <li>Use context: Pass research findings to debate tasks</li> <li>Combine with search: Research before debating</li> <li>Check safety alerts: Review alerts for sensitive topics</li> <li>Handle errors: Implement fallback logic</li> <li>Use appropriate rounds: More rounds for complex topics</li> </ol>"},{"location":"integrations/crewai/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about LangChain Integration</li> <li>Explore LangGraph Integration</li> <li>Configure MCP Server</li> </ul>"},{"location":"integrations/langchain/","title":"LangChain Integration","text":"<p>ARTEMIS provides native LangChain integration, allowing you to use debates as tools in LangChain chains and agents.</p>"},{"location":"integrations/langchain/#installation","title":"Installation","text":"<pre><code>pip install artemis-agents[langchain]\n</code></pre>"},{"location":"integrations/langchain/#artemisdebatetool","title":"ArtemisDebateTool","text":"<p>The primary integration point is the <code>ArtemisDebateTool</code>:</p> <pre><code>from artemis.integrations import ArtemisDebateTool\n\n# Create the tool\ndebate_tool = ArtemisDebateTool(\n    model=\"gpt-4o\",\n    default_rounds=3,\n)\n\n# Use directly\nresult = debate_tool.invoke({\n    \"topic\": \"Should we adopt microservices architecture?\",\n    \"rounds\": 2,\n})\n\nprint(result.verdict)\n</code></pre>"},{"location":"integrations/langchain/#tool-configuration","title":"Tool Configuration","text":""},{"location":"integrations/langchain/#full-options","title":"Full Options","text":"<pre><code>from artemis.integrations import ArtemisDebateTool\nfrom artemis.core.types import DebateConfig\n\nconfig = DebateConfig(\n    turn_timeout=60,\n    round_timeout=300,\n    require_evidence=True,\n    safety_mode=\"passive\",\n)\n\ntool = ArtemisDebateTool(\n    model=\"gpt-4o\",\n    default_rounds=3,\n    config=config,\n)\n</code></pre>"},{"location":"integrations/langchain/#available-parameters","title":"Available Parameters","text":"Parameter Type Default Description <code>model</code> str required LLM model to use <code>default_rounds</code> int 3 Default debate rounds <code>config</code> DebateConfig None Debate configuration <code>name</code> str \"artemis_debate\" Tool name <code>description</code> str ... Tool description"},{"location":"integrations/langchain/#with-langchain-agents","title":"With LangChain Agents","text":""},{"location":"integrations/langchain/#using-with-agentexecutor","title":"Using with AgentExecutor","text":"<pre><code>from langchain.agents import AgentExecutor, create_openai_functions_agent\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom artemis.integrations import ArtemisDebateTool\n\n# Create the tool\ndebate_tool = ArtemisDebateTool(model=\"gpt-4o\")\n\n# Create agent\nllm = ChatOpenAI(model=\"gpt-4o\")\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that can run debates to analyze topics.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\"),\n])\n\nagent = create_openai_functions_agent(llm, [debate_tool], prompt)\nexecutor = AgentExecutor(agent=agent, tools=[debate_tool])\n\n# Run\nresult = executor.invoke({\n    \"input\": \"Analyze the pros and cons of remote work by running a debate\"\n})\n</code></pre>"},{"location":"integrations/langchain/#using-with-lcel","title":"Using with LCEL","text":"<pre><code>from langchain_core.runnables import RunnableLambda\nfrom artemis.integrations import ArtemisDebateTool\n\ndebate_tool = ArtemisDebateTool(model=\"gpt-4o\")\n\n# Create a chain\nchain = (\n    RunnableLambda(lambda x: {\"topic\": x[\"question\"], \"rounds\": 2})\n    | debate_tool\n    | RunnableLambda(lambda x: f\"Verdict: {x.verdict.decision}\")\n)\n\nresult = chain.invoke({\"question\": \"Is Python better than JavaScript?\"})\n</code></pre>"},{"location":"integrations/langchain/#tool-output","title":"Tool Output","text":"<p>The tool returns a structured result:</p> <pre><code>result = debate_tool.invoke({\"topic\": \"Your topic\"})\n\n# Access verdict\nprint(result.verdict.decision)    # \"pro\", \"con\", or \"tie\"\nprint(result.verdict.confidence)  # 0.0 to 1.0\nprint(result.verdict.reasoning)   # Explanation\n\n# Access transcript\nfor turn in result.transcript:\n    print(f\"{turn.agent}: {turn.argument.content}\")\n\n# Access safety alerts\nfor alert in result.safety_alerts:\n    print(f\"Alert: {alert.type}\")\n</code></pre>"},{"location":"integrations/langchain/#multiple-debate-agents","title":"Multiple Debate Agents","text":"<p>You can specify custom agents with positions:</p> <pre><code>result = debate_tool.invoke({\n    \"topic\": \"Which database should we use?\",\n    \"agents\": [\n        {\"name\": \"sql_advocate\", \"role\": \"Database expert\", \"position\": \"advocates for SQL databases\"},\n        {\"name\": \"nosql_advocate\", \"role\": \"Database expert\", \"position\": \"advocates for NoSQL databases\"},\n    ],\n    \"rounds\": 3,\n})\n</code></pre>"},{"location":"integrations/langchain/#with-safety-monitoring","title":"With Safety Monitoring","text":"<p>Enable safety monitoring in debates:</p> <pre><code>from artemis.integrations import ArtemisDebateTool\nfrom artemis.safety import SandbagDetector, DeceptionMonitor, MonitorMode\n\ntool = ArtemisDebateTool(\n    model=\"gpt-4o\",\n    safety_monitors=[\n        SandbagDetector(mode=MonitorMode.PASSIVE, sensitivity=0.7),\n        DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6),\n    ],\n)\n</code></pre>"},{"location":"integrations/langchain/#async-support","title":"Async Support","text":"<p>The tool supports async execution:</p> <pre><code>from artemis.integrations import ArtemisDebateTool\n\ntool = ArtemisDebateTool(model=\"gpt-4o\")\n\n# Async invocation\nresult = await tool.ainvoke({\n    \"topic\": \"Should we use async programming?\",\n})\n</code></pre>"},{"location":"integrations/langchain/#custom-tool-schema","title":"Custom Tool Schema","text":"<p>Customize the tool's input schema:</p> <pre><code>from artemis.integrations import ArtemisDebateTool\nfrom pydantic import BaseModel, Field\n\nclass DebateInput(BaseModel):\n    topic: str = Field(description=\"The debate topic\")\n    rounds: int = Field(default=3, description=\"Number of rounds\")\n    require_evidence: bool = Field(default=True, description=\"Require evidence\")\n\ntool = ArtemisDebateTool(\n    model=\"gpt-4o\",\n    args_schema=DebateInput,\n)\n</code></pre>"},{"location":"integrations/langchain/#integration-patterns","title":"Integration Patterns","text":""},{"location":"integrations/langchain/#decision-support","title":"Decision Support","text":"<pre><code>from langchain.agents import AgentExecutor\nfrom artemis.integrations import ArtemisDebateTool\n\ndebate_tool = ArtemisDebateTool(model=\"gpt-4o\")\n\n# Use debate for decision support\nchain = create_decision_chain(\n    tools=[debate_tool],\n    system_prompt=\"\"\"\n    When faced with a decision, use the debate tool to analyze\n    pros and cons before making a recommendation.\n    \"\"\",\n)\n</code></pre>"},{"location":"integrations/langchain/#research-assistant","title":"Research Assistant","text":"<pre><code># Combine with search tools\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\nsearch = DuckDuckGoSearchRun()\ndebate = ArtemisDebateTool(model=\"gpt-4o\")\n\ntools = [search, debate]\n\n# Agent can search for info, then debate the findings\n</code></pre>"},{"location":"integrations/langchain/#analysis-pipeline","title":"Analysis Pipeline","text":"<pre><code>from langchain_core.runnables import RunnableParallel\n\n# Run multiple debates in parallel\nparallel = RunnableParallel(\n    tech_debate=ArtemisDebateTool(model=\"gpt-4o\"),\n    business_debate=ArtemisDebateTool(model=\"gpt-4o\"),\n)\n\nresults = parallel.invoke({\n    \"tech_debate\": {\"topic\": \"Technical feasibility of X\"},\n    \"business_debate\": {\"topic\": \"Business viability of X\"},\n})\n</code></pre>"},{"location":"integrations/langchain/#error-handling","title":"Error Handling","text":"<pre><code>from artemis.integrations import ArtemisDebateTool\nfrom artemis.exceptions import DebateError\n\ntool = ArtemisDebateTool(model=\"gpt-4o\")\n\ntry:\n    result = tool.invoke({\"topic\": \"Your topic\"})\nexcept DebateError as e:\n    print(f\"Debate failed: {e}\")\n</code></pre>"},{"location":"integrations/langchain/#callbacks","title":"Callbacks","text":"<p>Use LangChain callbacks with the tool:</p> <pre><code>from langchain.callbacks import StdOutCallbackHandler\n\ntool = ArtemisDebateTool(model=\"gpt-4o\")\n\nresult = tool.invoke(\n    {\"topic\": \"Your topic\"},\n    config={\"callbacks\": [StdOutCallbackHandler()]},\n)\n</code></pre>"},{"location":"integrations/langchain/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about LangGraph Integration</li> <li>Explore CrewAI Integration</li> <li>Configure MCP Server</li> </ul>"},{"location":"integrations/langgraph/","title":"LangGraph Integration","text":"<p>ARTEMIS provides deep integration with LangGraph, allowing you to incorporate structured debates into complex agent workflows.</p>"},{"location":"integrations/langgraph/#installation","title":"Installation","text":"<pre><code>pip install artemis-agents[langgraph]\n</code></pre>"},{"location":"integrations/langgraph/#artemisdebatenode","title":"ArtemisDebateNode","text":"<p>The primary integration is the <code>ArtemisDebateNode</code>:</p> <pre><code>from artemis.integrations import ArtemisDebateNode\n\n# Create a debate node\ndebate_node = ArtemisDebateNode(\n    model=\"gpt-4o\",\n    rounds=3,\n)\n</code></pre>"},{"location":"integrations/langgraph/#basic-workflow","title":"Basic Workflow","text":""},{"location":"integrations/langgraph/#simple-debate-workflow","title":"Simple Debate Workflow","text":"<pre><code>from langgraph.graph import StateGraph, START, END\nfrom artemis.integrations import ArtemisDebateNode, DebateState\n\n# Define the workflow\nworkflow = StateGraph(DebateState)\n\n# Add nodes\nworkflow.add_node(\"debate\", ArtemisDebateNode(model=\"gpt-4o\"))\n\n# Add edges\nworkflow.add_edge(START, \"debate\")\nworkflow.add_edge(\"debate\", END)\n\n# Compile\napp = workflow.compile()\n\n# Run\nresult = await app.ainvoke({\n    \"topic\": \"Should we use GraphQL or REST?\",\n})\n\nprint(result[\"verdict\"])\n</code></pre>"},{"location":"integrations/langgraph/#debate-state","title":"Debate State","text":"<p>The <code>DebateNodeState</code> tracks debate progress:</p> <pre><code>from artemis.integrations import DebateNodeState\n\n# State structure\nclass DebateNodeState(TypedDict, total=False):\n    topic: str                       # Debate topic\n    agents: list[AgentStateConfig]   # Agent configurations\n    positions: dict[str, str]        # Agent positions\n    rounds: int                      # Number of rounds\n    current_round: int               # Current round\n    phase: str                       # Current debate phase\n    transcript: list[dict]           # Debate transcript\n    verdict: dict | None             # Final verdict\n    scores: dict[str, float]         # Agent scores\n    metadata: dict                   # Additional metadata\n</code></pre>"},{"location":"integrations/langgraph/#pre-built-workflows","title":"Pre-built Workflows","text":""},{"location":"integrations/langgraph/#create_debate_workflow","title":"create_debate_workflow","text":"<p>A complete debate workflow:</p> <pre><code>from artemis.integrations import create_debate_workflow\n\n# Create the workflow (single-node by default)\nworkflow = create_debate_workflow(model=\"gpt-4o\")\n\n# Run\nresult = await workflow.ainvoke({\n    \"topic\": \"Is functional programming better than OOP?\",\n    \"rounds\": 3,\n})\n\nprint(f\"Verdict: {result['verdict']['decision']}\")\n</code></pre>"},{"location":"integrations/langgraph/#step-by-step-workflow","title":"Step-by-Step Workflow","text":"<p>For more control, use step-by-step mode:</p> <pre><code>from artemis.integrations import create_debate_workflow\n\n# Create step-by-step workflow\nworkflow = create_debate_workflow(\n    model=\"gpt-4o\",\n    step_by_step=True,\n)\n\n# Run with multi-agent support\nresult = await workflow.ainvoke({\n    \"topic\": \"Should we migrate to cloud?\",\n    \"agents\": [\n        {\"name\": \"tech_lead\", \"role\": \"Technical Lead\", \"position\": \"supports migration\"},\n        {\"name\": \"finance\", \"role\": \"Finance Director\", \"position\": \"concerned about costs\"},\n    ],\n    \"rounds\": 3,\n})\n\nprint(f\"Decision: {result['verdict']['decision']}\")\nprint(f\"Confidence: {result['verdict']['confidence']}\")\n</code></pre>"},{"location":"integrations/langgraph/#custom-workflows","title":"Custom Workflows","text":""},{"location":"integrations/langgraph/#with-pre-processing","title":"With Pre-processing","text":"<pre><code>from langgraph.graph import StateGraph, START, END\nfrom artemis.integrations import ArtemisDebateNode\n\nclass WorkflowState(TypedDict):\n    input: str\n    processed_topic: str\n    debate_result: dict\n\ndef preprocess(state: WorkflowState) -&gt; WorkflowState:\n    # Extract debate topic from input\n    topic = extract_topic(state[\"input\"])\n    return {\"processed_topic\": topic}\n\ndef postprocess(state: WorkflowState) -&gt; WorkflowState:\n    # Format the result\n    verdict = state[\"debate_result\"][\"verdict\"]\n    return {\"output\": f\"Analysis: {verdict.reasoning}\"}\n\n# Build workflow\nworkflow = StateGraph(WorkflowState)\nworkflow.add_node(\"preprocess\", preprocess)\nworkflow.add_node(\"debate\", ArtemisDebateNode(model=\"gpt-4o\"))\nworkflow.add_node(\"postprocess\", postprocess)\n\nworkflow.add_edge(START, \"preprocess\")\nworkflow.add_edge(\"preprocess\", \"debate\")\nworkflow.add_edge(\"debate\", \"postprocess\")\nworkflow.add_edge(\"postprocess\", END)\n\napp = workflow.compile()\n</code></pre>"},{"location":"integrations/langgraph/#with-conditional-routing","title":"With Conditional Routing","text":"<pre><code>from langgraph.graph import StateGraph, START, END\n\ndef should_debate(state: WorkflowState) -&gt; str:\n    # Only debate complex topics\n    if state[\"complexity\"] &gt; 0.7:\n        return \"debate\"\n    return \"quick_answer\"\n\nworkflow = StateGraph(WorkflowState)\nworkflow.add_node(\"analyze\", analyze_complexity)\nworkflow.add_node(\"debate\", ArtemisDebateNode(model=\"gpt-4o\"))\nworkflow.add_node(\"quick_answer\", quick_response)\n\nworkflow.add_edge(START, \"analyze\")\nworkflow.add_conditional_edges(\"analyze\", should_debate)\nworkflow.add_edge(\"debate\", END)\nworkflow.add_edge(\"quick_answer\", END)\n</code></pre>"},{"location":"integrations/langgraph/#with-human-in-the-loop","title":"With Human-in-the-Loop","text":"<pre><code>from langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint import MemorySaver\n\ndef review_needed(state: WorkflowState) -&gt; str:\n    # Route to human review if controversial\n    if state[\"verdict\"].confidence &lt; 0.6:\n        return \"human_review\"\n    return \"finalize\"\n\nworkflow = StateGraph(WorkflowState)\nworkflow.add_node(\"debate\", ArtemisDebateNode(model=\"gpt-4o\"))\nworkflow.add_node(\"human_review\", human_review_node)\nworkflow.add_node(\"finalize\", finalize_result)\n\nworkflow.add_edge(START, \"debate\")\nworkflow.add_conditional_edges(\"debate\", review_needed)\nworkflow.add_edge(\"human_review\", \"finalize\")\nworkflow.add_edge(\"finalize\", END)\n\n# Add checkpointing for interruption\napp = workflow.compile(checkpointer=MemorySaver())\n\n# Run with possible interrupt\nresult = await app.ainvoke(\n    {\"topic\": \"Controversial topic\"},\n    config={\"configurable\": {\"thread_id\": \"1\"}},\n)\n\n# If interrupted, resume after human review\nif result.get(\"needs_review\"):\n    result = await app.ainvoke(\n        {\"human_decision\": \"approved\"},\n        config={\"configurable\": {\"thread_id\": \"1\"}},\n    )\n</code></pre>"},{"location":"integrations/langgraph/#multi-agent-patterns","title":"Multi-Agent Patterns","text":""},{"location":"integrations/langgraph/#parallel-debates","title":"Parallel Debates","text":"<pre><code>from langgraph.graph import StateGraph, START, END\n\ndef split_topics(state):\n    # Split into multiple topics\n    return {\n        \"topics\": [\n            \"Technical feasibility\",\n            \"Cost analysis\",\n            \"Risk assessment\",\n        ]\n    }\n\ndef merge_results(state):\n    # Combine debate results\n    results = state[\"debate_results\"]\n    overall = synthesize(results)\n    return {\"final_analysis\": overall}\n\n# Create parallel branches\nworkflow = StateGraph(WorkflowState)\nworkflow.add_node(\"split\", split_topics)\n\nfor i in range(3):\n    workflow.add_node(f\"debate_{i}\", ArtemisDebateNode(model=\"gpt-4o\"))\n\nworkflow.add_node(\"merge\", merge_results)\n\nworkflow.add_edge(START, \"split\")\nfor i in range(3):\n    workflow.add_edge(\"split\", f\"debate_{i}\")\n    workflow.add_edge(f\"debate_{i}\", \"merge\")\nworkflow.add_edge(\"merge\", END)\n</code></pre>"},{"location":"integrations/langgraph/#sequential-analysis","title":"Sequential Analysis","text":"<pre><code>def extract_key_points(state):\n    # Extract points from debate for next stage\n    points = extract(state[\"debate_result\"])\n    return {\"key_points\": points}\n\nworkflow = StateGraph(WorkflowState)\nworkflow.add_node(\"initial_debate\", ArtemisDebateNode(model=\"gpt-4o\"))\nworkflow.add_node(\"extract\", extract_key_points)\nworkflow.add_node(\"deep_dive_debate\", ArtemisDebateNode(model=\"gpt-4o\"))\n\nworkflow.add_edge(START, \"initial_debate\")\nworkflow.add_edge(\"initial_debate\", \"extract\")\nworkflow.add_edge(\"extract\", \"deep_dive_debate\")\nworkflow.add_edge(\"deep_dive_debate\", END)\n</code></pre>"},{"location":"integrations/langgraph/#state-persistence","title":"State Persistence","text":""},{"location":"integrations/langgraph/#saving-debate-state","title":"Saving Debate State","text":"<pre><code>from langgraph.checkpoint import SqliteSaver\n\n# Use SQLite for persistence\ncheckpointer = SqliteSaver.from_conn_string(\"debates.db\")\n\nworkflow = create_debate_workflow(model=\"gpt-4o\")\napp = workflow.compile(checkpointer=checkpointer)\n\n# Run with thread ID for persistence\nresult = await app.ainvoke(\n    {\"topic\": \"Your topic\"},\n    config={\"configurable\": {\"thread_id\": \"debate-123\"}},\n)\n\n# Later: resume or replay\nhistory = app.get_state_history(\n    config={\"configurable\": {\"thread_id\": \"debate-123\"}}\n)\n</code></pre>"},{"location":"integrations/langgraph/#integration-with-safety","title":"Integration with Safety","text":"<p>Safety monitoring is configured via the <code>DebateConfig</code>:</p> <pre><code>from artemis.integrations import ArtemisDebateNode\nfrom artemis.core.types import DebateConfig\n\n# Create node with safety monitoring enabled\nconfig = DebateConfig(\n    safety_mode=\"active\",\n    halt_on_safety_violation=True,\n)\n\ndebate_node = ArtemisDebateNode(\n    model=\"gpt-4o\",\n    debate_config=config,\n)\n\ndef check_safety(state):\n    # Check for safety alerts in the debate result\n    if state.get(\"phase\") == \"error\":\n        return \"halt\"\n    return \"continue\"\n\nworkflow.add_conditional_edges(\"debate\", check_safety)\n</code></pre>"},{"location":"integrations/langgraph/#streaming","title":"Streaming","text":"<p>Stream debate progress:</p> <pre><code>workflow = create_debate_workflow(model=\"gpt-4o\")\napp = workflow.compile()\n\nasync for event in app.astream({\"topic\": \"Your topic\"}):\n    if \"debate\" in event:\n        turn = event[\"debate\"][\"current_turn\"]\n        print(f\"Turn: {turn.agent} - {turn.argument.content[:100]}...\")\n</code></pre>"},{"location":"integrations/langgraph/#error-handling","title":"Error Handling","text":"<pre><code>from langgraph.graph import StateGraph\n\ndef handle_debate_error(state):\n    if state.get(\"error\"):\n        # Log error and provide fallback\n        return {\"verdict\": {\"decision\": \"inconclusive\", \"reasoning\": str(state[\"error\"])}}\n    return state\n\nworkflow.add_node(\"error_handler\", handle_debate_error)\nworkflow.add_edge(\"debate\", \"error_handler\")\n</code></pre>"},{"location":"integrations/langgraph/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about LangChain Integration</li> <li>Explore CrewAI Integration</li> <li>Configure MCP Server</li> </ul>"},{"location":"integrations/mcp/","title":"MCP Server Integration","text":"<p>ARTEMIS provides an MCP (Model Context Protocol) server, allowing any MCP-compatible client to use structured debates.</p>"},{"location":"integrations/mcp/#overview","title":"Overview","text":"<p>The MCP server exposes ARTEMIS capabilities as tools that can be called by LLM clients that support the Model Context Protocol.</p>"},{"location":"integrations/mcp/#starting-the-server","title":"Starting the Server","text":""},{"location":"integrations/mcp/#cli","title":"CLI","text":"<pre><code># Start with default settings\nartemis-mcp\n\n# HTTP mode\nartemis-mcp --http --port 8080\n\n# With custom model\nartemis-mcp --model gpt-4-turbo\n\n# Verbose logging\nartemis-mcp --verbose\n</code></pre>"},{"location":"integrations/mcp/#programmatic","title":"Programmatic","text":"<pre><code>from artemis.mcp import ArtemisMCPServer\n\nserver = ArtemisMCPServer(\n    default_model=\"gpt-4o\",\n    max_sessions=100,\n)\n\n# Stdio mode (for MCP clients)\nawait server.run_stdio()\n\n# Or HTTP mode\nawait server.start(host=\"127.0.0.1\", port=8080)\n</code></pre>"},{"location":"integrations/mcp/#available-tools","title":"Available Tools","text":"<p>The MCP server exposes these tools:</p>"},{"location":"integrations/mcp/#artemis_debate_start","title":"artemis_debate_start","text":"<p>Start a new debate:</p> <pre><code>{\n    \"name\": \"artemis_debate_start\",\n    \"arguments\": {\n        \"topic\": \"Should we adopt microservices?\",\n        \"rounds\": 3,\n        \"positions\": {\n            \"pro\": \"supports microservices\",\n            \"con\": \"opposes microservices\"\n        }\n    }\n}\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"debate_id\": \"d-123456\",\n    \"status\": \"running\",\n    \"topic\": \"Should we adopt microservices?\"\n}\n</code></pre>"},{"location":"integrations/mcp/#artemis_add_round","title":"artemis_add_round","text":"<p>Add a round to an existing debate:</p> <pre><code>{\n    \"name\": \"artemis_add_round\",\n    \"arguments\": {\n        \"debate_id\": \"d-123456\"\n    }\n}\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"round\": 2,\n    \"turns\": [\n        {\n            \"agent\": \"pro\",\n            \"argument\": \"...\",\n            \"score\": 7.5\n        },\n        {\n            \"agent\": \"con\",\n            \"argument\": \"...\",\n            \"score\": 7.2\n        }\n    ]\n}\n</code></pre>"},{"location":"integrations/mcp/#artemis_get_verdict","title":"artemis_get_verdict","text":"<p>Get the jury's verdict:</p> <pre><code>{\n    \"name\": \"artemis_get_verdict\",\n    \"arguments\": {\n        \"debate_id\": \"d-123456\"\n    }\n}\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"verdict\": \"pro\",\n    \"confidence\": 0.73,\n    \"reasoning\": \"The proponent presented stronger evidence...\",\n    \"jury_votes\": [\n        {\"juror\": \"logical\", \"vote\": \"pro\"},\n        {\"juror\": \"ethical\", \"vote\": \"con\"},\n        {\"juror\": \"practical\", \"vote\": \"pro\"}\n    ]\n}\n</code></pre>"},{"location":"integrations/mcp/#artemis_get_transcript","title":"artemis_get_transcript","text":"<p>Get the full debate transcript:</p> <pre><code>{\n    \"name\": \"artemis_get_transcript\",\n    \"arguments\": {\n        \"debate_id\": \"d-123456\"\n    }\n}\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"topic\": \"Should we adopt microservices?\",\n    \"rounds\": 3,\n    \"transcript\": [\n        {\n            \"round\": 1,\n            \"agent\": \"pro\",\n            \"argument\": {\n                \"level\": \"strategic\",\n                \"content\": \"...\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"integrations/mcp/#artemis_list_debates","title":"artemis_list_debates","text":"<p>List all active debates:</p> <pre><code>{\n    \"name\": \"artemis_list_debates\",\n    \"arguments\": {}\n}\n</code></pre>"},{"location":"integrations/mcp/#artemis_get_safety_report","title":"artemis_get_safety_report","text":"<p>Get safety monitoring report:</p> <pre><code>{\n    \"name\": \"artemis_get_safety_report\",\n    \"arguments\": {\n        \"debate_id\": \"d-123456\"\n    }\n}\n</code></pre>"},{"location":"integrations/mcp/#server-configuration","title":"Server Configuration","text":""},{"location":"integrations/mcp/#full-options","title":"Full Options","text":"<pre><code>from artemis.mcp import ArtemisMCPServer\nfrom artemis.core.types import DebateConfig\n\nconfig = DebateConfig(\n    turn_timeout=60,\n    round_timeout=300,\n    require_evidence=True,\n    safety_mode=\"passive\",\n)\n\nserver = ArtemisMCPServer(\n    default_model=\"gpt-4o\",\n    max_sessions=100,\n    config=config,\n)\n</code></pre>"},{"location":"integrations/mcp/#environment-variables","title":"Environment Variables","text":"<p>The server respects these environment variables:</p> Variable Description <code>OPENAI_API_KEY</code> OpenAI API key <code>ANTHROPIC_API_KEY</code> Anthropic API key <code>ARTEMIS_DEFAULT_MODEL</code> Default model <code>ARTEMIS_LOG_LEVEL</code> Logging level <code>ARTEMIS_MAX_SESSIONS</code> Max concurrent sessions"},{"location":"integrations/mcp/#client-usage","title":"Client Usage","text":""},{"location":"integrations/mcp/#claude-desktop","title":"Claude Desktop","text":"<p>Add to your Claude Desktop config:</p> <pre><code>{\n    \"mcpServers\": {\n        \"artemis\": {\n            \"command\": \"artemis-mcp\",\n            \"args\": [\"--model\", \"gpt-4o\"]\n        }\n    }\n}\n</code></pre>"},{"location":"integrations/mcp/#other-mcp-clients","title":"Other MCP Clients","text":"<p>Connect to the HTTP endpoint:</p> <pre><code>import httpx\n\nasync with httpx.AsyncClient() as client:\n    # Start debate\n    response = await client.post(\n        \"http://localhost:8080/tools/artemis_debate_start\",\n        json={\n            \"topic\": \"Your topic\",\n            \"rounds\": 3,\n        }\n    )\n\n    debate_id = response.json()[\"debate_id\"]\n\n    # Get verdict\n    response = await client.post(\n        \"http://localhost:8080/tools/artemis_get_verdict\",\n        json={\"debate_id\": debate_id}\n    )\n\n    print(response.json())\n</code></pre>"},{"location":"integrations/mcp/#session-management","title":"Session Management","text":""},{"location":"integrations/mcp/#session-lifecycle","title":"Session Lifecycle","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Start Debate   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Session Active \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n         \u2502                \u2502\n         \u25bc                \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502   Add Rounds    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Get Verdict   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Session Ended  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"integrations/mcp/#session-management_1","title":"Session Management","text":"<p>Sessions are managed via the <code>SessionManager</code>:</p> <pre><code>from artemis.mcp import ArtemisMCPServer, create_session_manager\n\n# Create server with session manager\nserver = ArtemisMCPServer(default_model=\"gpt-4o\")\n\n# Sessions are automatically cleaned up after max_age_hours\nserver.state.cleanup_old_sessions(max_age_hours=24)\n</code></pre>"},{"location":"integrations/mcp/#safety-monitoring","title":"Safety Monitoring","text":"<p>Enable safety monitoring via the debate configuration:</p> <pre><code>from artemis.mcp import ArtemisMCPServer\nfrom artemis.core.types import DebateConfig\n\nconfig = DebateConfig(\n    safety_mode=\"active\",\n    halt_on_safety_violation=True,\n)\n\nserver = ArtemisMCPServer(\n    default_model=\"gpt-4o\",\n    config=config,\n)\n</code></pre> <p>Safety alerts are included in responses:</p> <pre><code>{\n    \"debate_id\": \"d-123456\",\n    \"verdict\": \"pro\",\n    \"safety_alerts\": [\n        {\n            \"type\": \"deception\",\n            \"severity\": 0.45,\n            \"agent\": \"con\",\n            \"round\": 2\n        }\n    ]\n}\n</code></pre>"},{"location":"integrations/mcp/#error-handling","title":"Error Handling","text":"<p>The server returns structured errors:</p> <pre><code>{\n    \"error\": {\n        \"code\": \"DEBATE_NOT_FOUND\",\n        \"message\": \"Debate d-999999 not found\",\n        \"details\": {}\n    }\n}\n</code></pre> <p>Error codes:</p> Code Description <code>DEBATE_NOT_FOUND</code> Debate ID doesn't exist <code>INVALID_ARGUMENTS</code> Missing or invalid arguments <code>DEBATE_COMPLETED</code> Cannot modify completed debate <code>RATE_LIMITED</code> Too many requests <code>INTERNAL_ERROR</code> Server error"},{"location":"integrations/mcp/#monitoring","title":"Monitoring","text":""},{"location":"integrations/mcp/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8080/health\n</code></pre>"},{"location":"integrations/mcp/#metrics","title":"Metrics","text":"<pre><code>curl http://localhost:8080/metrics\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"active_debates\": 5,\n    \"total_debates\": 127,\n    \"uptime_seconds\": 3600,\n    \"model_usage\": {\n        \"gpt-4o\": 95,\n        \"deepseek-reasoner\": 32\n    }\n}\n</code></pre>"},{"location":"integrations/mcp/#best-practices","title":"Best Practices","text":"<ol> <li>Use session IDs: Track debates across requests</li> <li>Handle timeouts: Debates can take time</li> <li>Check safety alerts: Review alerts in responses</li> <li>Clean up sessions: End completed debates</li> <li>Monitor usage: Watch metrics for issues</li> </ol>"},{"location":"integrations/mcp/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about LangChain Integration</li> <li>Explore LangGraph Integration</li> <li>Configure CrewAI Integration</li> </ul>"},{"location":"safety/behavior/","title":"Behavior Tracking","text":"<p>The Behavior Tracker monitors how agent behavior changes over time, detecting drift, inconsistencies, and concerning patterns.</p>"},{"location":"safety/behavior/#what-it-tracks","title":"What It Tracks","text":"<p>Behavior tracking monitors:</p> <ul> <li>Style Drift: Changes in communication style</li> <li>Position Drift: Shifts in argued position</li> <li>Capability Drift: Changes in demonstrated abilities</li> <li>Engagement Patterns: How agents interact</li> </ul>"},{"location":"safety/behavior/#why-track-behavior","title":"Why Track Behavior?","text":"<p>Behavior tracking helps detect:</p> <ol> <li>Goal Drift: Agent straying from assigned position</li> <li>Strategic Shifts: Concerning strategic changes</li> <li>Manipulation: Gradual influence by opponent</li> <li>Degradation: Performance declining over time</li> </ol>"},{"location":"safety/behavior/#usage","title":"Usage","text":""},{"location":"safety/behavior/#basic-setup","title":"Basic Setup","text":"<pre><code>from artemis.safety import BehaviorTracker, MonitorMode\n\ntracker = BehaviorTracker(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.5,\n    window_size=5,\n)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[tracker.process],\n)\n</code></pre>"},{"location":"safety/behavior/#configuration-options","title":"Configuration Options","text":"<pre><code>tracker = BehaviorTracker(\n    mode=MonitorMode.PASSIVE,     # PASSIVE, ACTIVE, or LEARNING\n    sensitivity=0.5,               # 0.0 to 1.0\n    window_size=5,                 # Turns to consider for drift detection\n    drift_threshold=0.3,           # Threshold for drift alerts\n)\n</code></pre>"},{"location":"safety/behavior/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>mode</code> MonitorMode PASSIVE Monitor mode <code>sensitivity</code> float 0.5 Detection sensitivity (0-1) <code>window_size</code> int 5 Turns to consider <code>drift_threshold</code> float 0.3 Drift detection threshold"},{"location":"safety/behavior/#tracked-metrics","title":"Tracked Metrics","text":""},{"location":"safety/behavior/#style-metrics","title":"Style Metrics","text":"<p>How the agent communicates:</p> <ul> <li>Formality level</li> <li>Aggression/assertiveness</li> <li>Complexity</li> <li>Sentiment</li> <li>Confidence</li> </ul>"},{"location":"safety/behavior/#position-metrics","title":"Position Metrics","text":"<p>What position the agent argues:</p> <ul> <li>Position alignment with assigned stance</li> <li>Position strength</li> <li>Concession rate</li> <li>Counter-argument engagement</li> </ul>"},{"location":"safety/behavior/#capability-metrics","title":"Capability Metrics","text":"<p>How capable the agent appears:</p> <ul> <li>Vocabulary complexity</li> <li>Reasoning depth</li> <li>Evidence usage</li> <li>Argument structure</li> </ul>"},{"location":"safety/behavior/#drift-detection","title":"Drift Detection","text":""},{"location":"safety/behavior/#drift-types","title":"Drift Types","text":"Drift Type Description Concern Level Gradual Slow, steady change Low Sudden Abrupt shift High Oscillating Back and forth Medium Escalating Increasing severity High"},{"location":"safety/behavior/#results","title":"Results","text":"<p>The tracker contributes to debate safety alerts:</p> <pre><code>result = await debate.run()\n\n# Check for behavior drift alerts\nfor alert in result.safety_alerts:\n    if \"behavior\" in alert.type.lower() or \"drift\" in alert.type.lower():\n        print(f\"Agent: {alert.agent}\")\n        print(f\"Severity: {alert.severity:.0%}\")\n</code></pre>"},{"location":"safety/behavior/#integration","title":"Integration","text":""},{"location":"safety/behavior/#with-debate","title":"With Debate","text":"<pre><code>from artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import BehaviorTracker, MonitorMode\n\nagents = [\n    Agent(name=\"pro\", role=\"Advocate for the proposition\", model=\"gpt-4o\"),\n    Agent(name=\"con\", role=\"Advocate against the proposition\", model=\"gpt-4o\"),\n]\n\ntracker = BehaviorTracker(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.5,\n    window_size=5,\n)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[tracker.process],\n)\n\ndebate.assign_positions({\n    \"pro\": \"supports the proposition\",\n    \"con\": \"opposes the proposition\",\n})\n\nresult = await debate.run()\n\n# Check for behavior alerts\nbehavior_alerts = [\n    a for a in result.safety_alerts\n    if \"behavior\" in a.type.lower()\n]\n\nfor alert in behavior_alerts:\n    print(f\"Agent {alert.agent}: {alert.severity:.0%} severity\")\n</code></pre>"},{"location":"safety/behavior/#with-other-monitors","title":"With Other Monitors","text":"<p>Behavior tracking complements other monitors:</p> <pre><code>from artemis.safety import (\n    BehaviorTracker,\n    SandbagDetector,\n    DeceptionMonitor,\n    MonitorMode,\n)\n\nbehavior = BehaviorTracker(mode=MonitorMode.PASSIVE, sensitivity=0.5, window_size=5)\nsandbag = SandbagDetector(mode=MonitorMode.PASSIVE, sensitivity=0.7)\ndeception = DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[\n        behavior.process,\n        sandbag.process,\n        deception.process,\n    ],\n)\n</code></pre>"},{"location":"safety/behavior/#sensitivity-tuning","title":"Sensitivity Tuning","text":""},{"location":"safety/behavior/#low-sensitivity-03","title":"Low Sensitivity (0.3)","text":"<ul> <li>Only catches severe drift</li> <li>Minimal false positives</li> <li>May miss gradual changes</li> </ul>"},{"location":"safety/behavior/#medium-sensitivity-05","title":"Medium Sensitivity (0.5)","text":"<ul> <li>Balanced detection</li> <li>Catches most concerning drift</li> <li>Good general setting</li> </ul>"},{"location":"safety/behavior/#high-sensitivity-08","title":"High Sensitivity (0.8)","text":"<ul> <li>Catches subtle changes</li> <li>More false positives</li> <li>For high-stakes scenarios</li> </ul>"},{"location":"safety/behavior/#best-practices","title":"Best Practices","text":"<ol> <li>Set appropriate window: 5-7 turns typically works well</li> <li>Account for natural variation: Some drift is normal</li> <li>Consider debate phase: Closing arguments differ from opening</li> <li>Monitor all agents: Compare behavior across participants</li> <li>Correlate with other signals: Drift may accompany other issues</li> </ol>"},{"location":"safety/behavior/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Sandbagging Detection</li> <li>Explore Deception Monitoring</li> <li>Configure Ethics Guard</li> </ul>"},{"location":"safety/deception/","title":"Deception Monitoring","text":"<p>The Deception Monitor detects when agents make false claims, misrepresent information, or attempt to mislead.</p>"},{"location":"safety/deception/#what-is-deception","title":"What is Deception?","text":"<p>Deception in debates includes:</p> <ul> <li>Factual Falsity: Making claims that are demonstrably false</li> <li>Logical Fallacies: Using invalid reasoning to mislead</li> <li>Misrepresentation: Distorting sources or opponent positions</li> <li>Selective Omission: Hiding relevant information</li> <li>Misdirection: Distracting from key issues</li> </ul>"},{"location":"safety/deception/#detection-capabilities","title":"Detection Capabilities","text":"<p>The Deception Monitor checks multiple dimensions:</p> Dimension What It Checks Factual Are claims consistent and plausible? Logical Is reasoning valid? Consistency Do claims contradict each other? Source Are sources accurately represented? Context Is context preserved?"},{"location":"safety/deception/#usage","title":"Usage","text":""},{"location":"safety/deception/#basic-setup","title":"Basic Setup","text":"<pre><code>from artemis.safety import DeceptionMonitor, MonitorMode\n\nmonitor = DeceptionMonitor(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.6,\n)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[monitor.process],\n)\n</code></pre>"},{"location":"safety/deception/#configuration-options","title":"Configuration Options","text":"<pre><code>monitor = DeceptionMonitor(\n    mode=MonitorMode.PASSIVE,    # PASSIVE, ACTIVE, or LEARNING\n    sensitivity=0.6,              # 0.0 to 1.0\n)\n</code></pre>"},{"location":"safety/deception/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>mode</code> MonitorMode PASSIVE Monitor mode <code>sensitivity</code> float 0.5 Detection sensitivity (0-1)"},{"location":"safety/deception/#what-it-detects","title":"What It Detects","text":""},{"location":"safety/deception/#logical-fallacies","title":"Logical Fallacies","text":"<p>Common fallacies detected:</p> Fallacy Description Ad Hominem Attacking the person, not the argument Straw Man Misrepresenting opponent's position False Dichotomy Presenting only two options when more exist Appeal to Authority Using authority as sole justification Circular Reasoning Conclusion restates the premise Red Herring Introducing irrelevant information Slippery Slope Assuming inevitable chain of events Hasty Generalization Drawing broad conclusions from few examples"},{"location":"safety/deception/#consistency-issues","title":"Consistency Issues","text":"<ul> <li>Internal contradictions within an agent's arguments</li> <li>Position shifts that contradict earlier statements</li> <li>Conflicting evidence claims</li> </ul>"},{"location":"safety/deception/#misrepresentation","title":"Misrepresentation","text":"<ul> <li>Distorting opponent's position</li> <li>Taking sources out of context</li> <li>Selective quoting</li> </ul>"},{"location":"safety/deception/#results","title":"Results","text":"<p>The monitor contributes to debate safety alerts:</p> <pre><code>result = await debate.run()\n\n# Check for deception alerts\nfor alert in result.safety_alerts:\n    if \"deception\" in alert.type.lower():\n        print(f\"Agent: {alert.agent}\")\n        print(f\"Severity: {alert.severity:.0%}\")\n</code></pre>"},{"location":"safety/deception/#distinguishing-intent","title":"Distinguishing Intent","text":"<p>Not all false claims are intentional deception:</p> Type Description Severity Mistake Unintentional error Low Negligence Careless claim Medium Deception Intentional misleading High"},{"location":"safety/deception/#integration","title":"Integration","text":""},{"location":"safety/deception/#with-debate","title":"With Debate","text":"<pre><code>from artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import DeceptionMonitor, MonitorMode\n\nagents = [\n    Agent(name=\"pro\", role=\"Advocate for the proposition\", model=\"gpt-4o\"),\n    Agent(name=\"con\", role=\"Advocate against the proposition\", model=\"gpt-4o\"),\n]\n\nmonitor = DeceptionMonitor(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.6,\n)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[monitor.process],\n)\n\ndebate.assign_positions({\n    \"pro\": \"supports the proposition\",\n    \"con\": \"opposes the proposition\",\n})\n\nresult = await debate.run()\n\n# Check for deception alerts\ndeception_alerts = [\n    a for a in result.safety_alerts\n    if \"deception\" in a.type.lower()\n]\n\nfor alert in deception_alerts:\n    print(f\"Agent: {alert.agent}\")\n    print(f\"Severity: {alert.severity:.0%}\")\n</code></pre>"},{"location":"safety/deception/#with-other-monitors","title":"With Other Monitors","text":"<pre><code>from artemis.safety import (\n    DeceptionMonitor,\n    SandbagDetector,\n    EthicsGuard,\n    MonitorMode,\n    EthicsConfig,\n)\n\ndeception = DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6)\nsandbag = SandbagDetector(mode=MonitorMode.PASSIVE, sensitivity=0.7)\nethics = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=EthicsConfig(harmful_content_threshold=0.5),\n)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[\n        deception.process,\n        sandbag.process,\n        ethics.process,\n    ],\n)\n</code></pre>"},{"location":"safety/deception/#sensitivity-tuning","title":"Sensitivity Tuning","text":""},{"location":"safety/deception/#low-sensitivity-03","title":"Low Sensitivity (0.3)","text":"<ul> <li>Catches only obvious deception</li> <li>Few false positives</li> <li>May miss subtle cases</li> </ul>"},{"location":"safety/deception/#medium-sensitivity-06","title":"Medium Sensitivity (0.6)","text":"<ul> <li>Balanced detection</li> <li>Some false positives</li> <li>Good general setting</li> </ul>"},{"location":"safety/deception/#high-sensitivity-08","title":"High Sensitivity (0.8)","text":"<ul> <li>Catches subtle deception</li> <li>More false positives</li> <li>Good for high-stakes scenarios</li> </ul>"},{"location":"safety/deception/#best-practices","title":"Best Practices","text":"<ol> <li>Enable comprehensive monitoring: Combine with other monitors</li> <li>Track consistency: Many deceptions are revealed by contradictions</li> <li>Consider intent: Not all false claims are deceptive</li> <li>Review edge cases: Some content needs human judgment</li> <li>Combine with ethics: Deception often accompanies ethical violations</li> </ol>"},{"location":"safety/deception/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Sandbagging Detection</li> <li>Explore Behavior Tracking</li> <li>Configure Ethics Guard</li> </ul>"},{"location":"safety/ethics-guard/","title":"Ethics Guard","text":"<p>The Ethics Guard monitors debates for ethical boundary violations, ensuring arguments remain within acceptable moral limits.</p>"},{"location":"safety/ethics-guard/#overview","title":"Overview","text":"<p>The Ethics Guard detects:</p> <ul> <li>Harmful Content: Arguments promoting harm</li> <li>Discrimination: Unfair treatment of groups</li> <li>Manipulation: Psychological manipulation tactics</li> <li>Privacy Violations: Exposing private information</li> <li>Deceptive Claims: Intentionally false statements</li> </ul>"},{"location":"safety/ethics-guard/#usage","title":"Usage","text":""},{"location":"safety/ethics-guard/#basic-setup","title":"Basic Setup","text":"<pre><code>from artemis.safety import EthicsGuard, MonitorMode, EthicsConfig\n\nguard = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=EthicsConfig(\n        harmful_content_threshold=0.4,\n        bias_threshold=0.4,\n        fairness_threshold=0.3,\n        enabled_checks=[\"harmful_content\", \"bias\", \"fairness\"],\n    ),\n)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[guard.process],\n)\n</code></pre>"},{"location":"safety/ethics-guard/#configuration-options","title":"Configuration Options","text":"<pre><code>from artemis.safety import EthicsGuard, MonitorMode, EthicsConfig\n\nconfig = EthicsConfig(\n    harmful_content_threshold=0.4,\n    bias_threshold=0.4,\n    fairness_threshold=0.3,\n    enabled_checks=[\n        \"harmful_content\",\n        \"bias\",\n        \"fairness\",\n        \"privacy\",\n        \"manipulation\",\n    ],\n)\n\nguard = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=config,\n)\n</code></pre>"},{"location":"safety/ethics-guard/#ethicsconfig-parameters","title":"EthicsConfig Parameters","text":"Parameter Type Default Description <code>harmful_content_threshold</code> float 0.5 Threshold for harmful content <code>bias_threshold</code> float 0.4 Threshold for bias detection <code>fairness_threshold</code> float 0.3 Threshold for fairness violations <code>enabled_checks</code> list[str] default set Ethics checks to enable"},{"location":"safety/ethics-guard/#ethical-principles","title":"Ethical Principles","text":""},{"location":"safety/ethics-guard/#built-in-principles","title":"Built-in Principles","text":"Principle Description Detects Fairness Equal treatment Discrimination, bias Transparency Honest communication Hidden agendas, misdirection Non-harm Avoiding harm Violence, dangerous advice Respect Dignified treatment Insults, dehumanization Accuracy Truthful claims Misinformation, false claims"},{"location":"safety/ethics-guard/#custom-checks","title":"Custom Checks","text":"<p>You can specify which checks to enable:</p> <pre><code>config = EthicsConfig(\n    harmful_content_threshold=0.3,\n    enabled_checks=[\"harmful_content\", \"bias\"],  # Only these two\n)\n</code></pre>"},{"location":"safety/ethics-guard/#detection-categories","title":"Detection Categories","text":""},{"location":"safety/ethics-guard/#harmful-content","title":"Harmful Content","text":"<p>Content that promotes or glorifies harm:</p> <ul> <li>Violence advocacy</li> <li>Self-harm promotion</li> <li>Dangerous activities</li> <li>Harmful advice</li> </ul>"},{"location":"safety/ethics-guard/#discrimination","title":"Discrimination","text":"<p>Unfair treatment based on protected characteristics:</p> <ul> <li>Racial discrimination</li> <li>Gender discrimination</li> <li>Religious discrimination</li> <li>Age discrimination</li> <li>Disability discrimination</li> </ul>"},{"location":"safety/ethics-guard/#manipulation","title":"Manipulation","text":"<p>Psychological manipulation tactics:</p> <ul> <li>Fear mongering</li> <li>Guilt tripping</li> <li>Gaslighting language</li> <li>Coercion</li> <li>Emotional exploitation</li> </ul>"},{"location":"safety/ethics-guard/#privacy-violations","title":"Privacy Violations","text":"<p>Exposure of private information:</p> <ul> <li>Personal identification</li> <li>Location disclosure</li> <li>Financial information</li> <li>Health information</li> </ul>"},{"location":"safety/ethics-guard/#results","title":"Results","text":"<p>The Ethics Guard contributes to debate safety alerts:</p> <pre><code>result = await debate.run()\n\n# Check for ethics alerts\nfor alert in result.safety_alerts:\n    if \"ethics\" in alert.type.lower():\n        print(f\"Agent: {alert.agent}\")\n        print(f\"Severity: {alert.severity:.0%}\")\n</code></pre>"},{"location":"safety/ethics-guard/#integration","title":"Integration","text":""},{"location":"safety/ethics-guard/#with-debate","title":"With Debate","text":"<pre><code>from artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import EthicsGuard, MonitorMode, EthicsConfig\n\nagents = [\n    Agent(name=\"pro\", role=\"Advocate for the proposition\", model=\"gpt-4o\"),\n    Agent(name=\"con\", role=\"Advocate against the proposition\", model=\"gpt-4o\"),\n]\n\nguard = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=EthicsConfig(harmful_content_threshold=0.4),\n)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[guard.process],\n)\n\ndebate.assign_positions({\n    \"pro\": \"supports the proposition\",\n    \"con\": \"opposes the proposition\",\n})\n\nresult = await debate.run()\n\n# Check for ethics violations\nethics_alerts = [\n    a for a in result.safety_alerts\n    if \"ethics\" in a.type.lower()\n]\n\nfor alert in ethics_alerts:\n    print(f\"Agent: {alert.agent}\")\n    print(f\"Severity: {alert.severity:.0%}\")\n</code></pre>"},{"location":"safety/ethics-guard/#with-other-monitors","title":"With Other Monitors","text":"<pre><code>from artemis.safety import (\n    EthicsGuard,\n    DeceptionMonitor,\n    BehaviorTracker,\n    MonitorMode,\n    EthicsConfig,\n)\n\nethics = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=EthicsConfig(harmful_content_threshold=0.4),\n)\ndeception = DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6)\nbehavior = BehaviorTracker(mode=MonitorMode.PASSIVE, sensitivity=0.5)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[\n        ethics.process,\n        deception.process,\n        behavior.process,\n    ],\n)\n</code></pre>"},{"location":"safety/ethics-guard/#sensitivity-tuning","title":"Sensitivity Tuning","text":""},{"location":"safety/ethics-guard/#low-sensitivity-03","title":"Low Sensitivity (0.3)","text":"<ul> <li>Only catches severe violations</li> <li>Minimal false positives</li> <li>Allows controversial but not harmful content</li> </ul>"},{"location":"safety/ethics-guard/#medium-sensitivity-06","title":"Medium Sensitivity (0.6)","text":"<ul> <li>Catches most concerning content</li> <li>Balanced false positive rate</li> <li>Good general setting</li> </ul>"},{"location":"safety/ethics-guard/#high-sensitivity-09","title":"High Sensitivity (0.9)","text":"<ul> <li>Very strict enforcement</li> <li>More false positives</li> <li>For sensitive contexts</li> </ul>"},{"location":"safety/ethics-guard/#common-configurations","title":"Common Configurations","text":""},{"location":"safety/ethics-guard/#academic-debate","title":"Academic Debate","text":"<pre><code>guard = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=EthicsConfig(\n        harmful_content_threshold=0.5,\n        bias_threshold=0.5,\n        enabled_checks=[\"harmful_content\", \"bias\", \"fairness\"],\n    ),\n)\n</code></pre>"},{"location":"safety/ethics-guard/#sensitive-topics","title":"Sensitive Topics","text":"<pre><code>guard = EthicsGuard(\n    mode=MonitorMode.ACTIVE,  # Can halt debate\n    config=EthicsConfig(\n        harmful_content_threshold=0.2,  # Very strict\n        bias_threshold=0.3,\n        enabled_checks=[\"harmful_content\", \"bias\", \"fairness\", \"privacy\"],\n    ),\n)\n</code></pre>"},{"location":"safety/ethics-guard/#policy-debate","title":"Policy Debate","text":"<pre><code>guard = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=EthicsConfig(\n        harmful_content_threshold=0.4,\n        bias_threshold=0.4,\n        enabled_checks=[\"harmful_content\", \"bias\", \"fairness\"],\n    ),\n)\n</code></pre>"},{"location":"safety/ethics-guard/#best-practices","title":"Best Practices","text":"<ol> <li>Set appropriate sensitivity: Match to debate context</li> <li>Define clear principles: Be explicit about boundaries</li> <li>Use passive mode initially: Understand patterns before blocking</li> <li>Review edge cases: Some content needs human judgment</li> <li>Document decisions: Track why alerts were generated</li> </ol>"},{"location":"safety/ethics-guard/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Safety Overview</li> <li>Explore Deception Monitoring</li> <li>Configure Behavior Tracking</li> </ul>"},{"location":"safety/overview/","title":"Safety Monitoring Overview","text":"<p>ARTEMIS includes comprehensive safety monitoring to detect and prevent problematic agent behaviors during debates. This is a key differentiator from other multi-agent frameworks.</p>"},{"location":"safety/overview/#why-safety-monitoring","title":"Why Safety Monitoring?","text":"<p>LLM agents can exhibit concerning behaviors:</p> <ul> <li>Sandbagging: Deliberately underperforming to appear less capable</li> <li>Deception: Making false claims or hiding information</li> <li>Behavioral Drift: Gradually shifting behavior over time</li> <li>Ethical Violations: Crossing ethical boundaries</li> </ul> <p>ARTEMIS monitors for these behaviors in real-time.</p>"},{"location":"safety/overview/#safety-architecture","title":"Safety Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Safety Layer                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502  Sandbagging  \u2502  \u2502   Deception   \u2502  \u2502   Behavior    \u2502       \u2502\n\u2502  \u2502   Detector    \u2502  \u2502    Monitor    \u2502  \u2502    Tracker    \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502          \u2502                  \u2502                  \u2502               \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n\u2502                             \u2502                                  \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502                    \u2502 safety_monitors \u2502                         \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                             \u2502                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502    Ethics     \u2502  \u2502    Debate    \u2502  \u2502    Alerts     \u2502        \u2502\n\u2502  \u2502     Guard     \u2502\u2500\u2500\u2502  Integration \u2502\u2500\u2500\u2502   (results)   \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"safety/overview/#available-monitors","title":"Available Monitors","text":"Monitor Purpose Detects Sandbagging Detector Detect intentional underperformance Capability hiding Deception Monitor Detect false claims Lies, misdirection Behavior Tracker Track behavioral changes Drift, inconsistency Ethics Guard Monitor ethical boundaries Violations, harm"},{"location":"safety/overview/#quick-start","title":"Quick Start","text":""},{"location":"safety/overview/#basic-safety-setup","title":"Basic Safety Setup","text":"<pre><code>from artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import (\n    SandbagDetector,\n    DeceptionMonitor,\n    BehaviorTracker,\n    EthicsGuard,\n    MonitorMode,\n    EthicsConfig,\n)\n\n# Create agents\nagents = [\n    Agent(name=\"pro\", role=\"Advocate for the proposition\", model=\"gpt-4o\"),\n    Agent(name=\"con\", role=\"Advocate against the proposition\", model=\"gpt-4o\"),\n]\n\n# Create individual monitors\nsandbag = SandbagDetector(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.7,\n)\ndeception = DeceptionMonitor(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.6,\n)\nbehavior = BehaviorTracker(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.5,\n    window_size=5,\n)\nethics = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=EthicsConfig(harmful_content_threshold=0.5),\n)\n\n# Create debate with safety monitors\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[\n        sandbag.process,\n        deception.process,\n        behavior.process,\n        ethics.process,\n    ],\n)\n\ndebate.assign_positions({\n    \"pro\": \"supports the proposition\",\n    \"con\": \"opposes the proposition\",\n})\n\nresult = await debate.run()\n\n# Check safety alerts\nfor alert in result.safety_alerts:\n    print(f\"Alert: {alert.type}\")\n    print(f\"Severity: {alert.severity}\")\n    print(f\"Agent: {alert.agent}\")\n</code></pre>"},{"location":"safety/overview/#multiple-monitors","title":"Multiple Monitors","text":"<p>Combine multiple monitors for comprehensive safety:</p> <pre><code>from artemis.safety import (\n    SandbagDetector,\n    DeceptionMonitor,\n    BehaviorTracker,\n    EthicsGuard,\n    MonitorMode,\n    EthicsConfig,\n)\n\n# Create all monitors\nmonitors = [\n    SandbagDetector(mode=MonitorMode.PASSIVE, sensitivity=0.7),\n    DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6),\n    BehaviorTracker(mode=MonitorMode.PASSIVE, sensitivity=0.5),\n    EthicsGuard(mode=MonitorMode.PASSIVE, config=EthicsConfig(harmful_content_threshold=0.5)),\n]\n\n# Pass their process methods to the debate\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[m.process for m in monitors],\n)\n</code></pre>"},{"location":"safety/overview/#monitor-modes","title":"Monitor Modes","text":"<p>All monitors support three modes via the <code>MonitorMode</code> enum:</p> <pre><code>from artemis.safety import MonitorMode\n\n# Available modes\nMonitorMode.PASSIVE   # Observe and report only\nMonitorMode.ACTIVE    # Can intervene and halt debate\nMonitorMode.LEARNING  # Learn patterns without alerting\n</code></pre>"},{"location":"safety/overview/#passive-mode-default","title":"Passive Mode (Default)","text":"<p>Monitors observe and report but don't intervene:</p> <pre><code>monitor = SandbagDetector(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.7,\n)\n# Alerts are generated but debate continues\n</code></pre>"},{"location":"safety/overview/#active-mode","title":"Active Mode","text":"<p>Monitors can intervene and halt the debate:</p> <pre><code>monitor = SandbagDetector(\n    mode=MonitorMode.ACTIVE,\n    sensitivity=0.7,\n)\n# Debate may halt if severe issues detected\n</code></pre>"},{"location":"safety/overview/#learning-mode","title":"Learning Mode","text":"<p>Monitors learn patterns without generating alerts:</p> <pre><code>monitor = BehaviorTracker(\n    mode=MonitorMode.LEARNING,\n    sensitivity=0.5,\n)\n# Gathers data for future reference\n</code></pre>"},{"location":"safety/overview/#safety-results","title":"Safety Results","text":"<p>Each monitor's <code>process</code> method is called during debate and can contribute to alerts:</p> <pre><code>result = await debate.run()\n\n# All alerts from all monitors\nfor alert in result.safety_alerts:\n    print(f\"Type: {alert.type}\")\n    print(f\"Agent: {alert.agent}\")\n    print(f\"Severity: {alert.severity:.0%}\")\n</code></pre>"},{"location":"safety/overview/#alert-severity-levels","title":"Alert Severity Levels","text":"Level Score Range Description Low 0.0 - 0.3 Minor concern Medium 0.3 - 0.6 Notable issue High 0.6 - 0.9 Serious concern Critical 0.9 - 1.0 Severe issue"},{"location":"safety/overview/#configuration","title":"Configuration","text":""},{"location":"safety/overview/#per-monitor-settings","title":"Per-Monitor Settings","text":"<p>Each monitor has its own configuration options:</p> <pre><code># Sandbagging detector\nsandbag = SandbagDetector(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.7,\n    baseline_turns=3,\n    drop_threshold=0.3,\n)\n\n# Deception monitor\ndeception = DeceptionMonitor(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.6,\n)\n\n# Behavior tracker\nbehavior = BehaviorTracker(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.5,\n    window_size=5,\n    drift_threshold=0.3,\n)\n\n# Ethics guard\nethics = EthicsGuard(\n    mode=MonitorMode.PASSIVE,\n    config=EthicsConfig(\n        harmful_content_threshold=0.5,\n        bias_threshold=0.4,\n        fairness_threshold=0.3,\n        enabled_checks=[\"harmful_content\", \"bias\", \"fairness\"],\n    ),\n)\n</code></pre>"},{"location":"safety/overview/#accessing-safety-data","title":"Accessing Safety Data","text":""},{"location":"safety/overview/#after-debate","title":"After Debate","text":"<pre><code>result = await debate.run()\n\n# All safety alerts\nprint(f\"Total alerts: {len(result.safety_alerts)}\")\n\nfor alert in result.safety_alerts:\n    print(f\"  {alert.type}: {alert.severity:.0%} - {alert.agent}\")\n\n# Filter by type\nsandbagging_alerts = [a for a in result.safety_alerts if \"sandbag\" in a.type.lower()]\ndeception_alerts = [a for a in result.safety_alerts if \"deception\" in a.type.lower()]\n</code></pre>"},{"location":"safety/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Start with passive mode: Understand behavior before enabling active intervention</li> <li>Tune sensitivity: Adjust based on false positive rates</li> <li>Combine monitors: Multiple monitors catch more issues</li> <li>Review alerts: Verify detections before taking action</li> <li>Consider context: Some patterns may be legitimate</li> </ol>"},{"location":"safety/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Sandbagging Detection</li> <li>Understand Deception Monitoring</li> <li>Explore Behavior Tracking</li> <li>Configure Ethics Guard</li> </ul>"},{"location":"safety/sandbagging/","title":"Sandbagging Detection","text":"<p>Sandbagging is when an agent intentionally underperforms to hide its true capabilities. ARTEMIS's Sandbagging Detector identifies this behavior.</p>"},{"location":"safety/sandbagging/#what-is-sandbagging","title":"What is Sandbagging?","text":"<p>Sandbagging occurs when an agent:</p> <ul> <li>Uses simpler vocabulary than it's capable of</li> <li>Provides less detailed arguments than expected</li> <li>Avoids complex reasoning it could perform</li> <li>Hides knowledge to appear less capable</li> </ul>"},{"location":"safety/sandbagging/#why-detect-it","title":"Why Detect It?","text":"<p>Sandbagging is a safety concern because:</p> <ol> <li>Capability Assessment: We need accurate understanding of model capabilities</li> <li>Trust: Hidden capabilities undermine trust in AI systems</li> <li>Safety Evaluation: Safety testing requires honest performance</li> <li>Fairness: Debates require agents to perform at their best</li> </ol>"},{"location":"safety/sandbagging/#how-it-works","title":"How It Works","text":"<p>The Sandbagging Detector establishes a capability baseline and monitors for significant drops:</p> <pre><code>graph TD\n    A[Initial Rounds] --&gt; B[Establish Baseline]\n    B --&gt; C[Monitor Performance]\n    C --&gt; D{Below Baseline?}\n    D --&gt;|Yes| E[Analyze Pattern]\n    E --&gt; F{Consistent Drop?}\n    F --&gt;|Yes| G[Flag Sandbagging]\n    F --&gt;|No| H[Normal Variation]\n    D --&gt;|No| C</code></pre>"},{"location":"safety/sandbagging/#usage","title":"Usage","text":""},{"location":"safety/sandbagging/#basic-setup","title":"Basic Setup","text":"<pre><code>from artemis.safety import SandbagDetector, MonitorMode\n\ndetector = SandbagDetector(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.7,\n)\n\n# Use in debate\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[detector.process],\n)\n</code></pre>"},{"location":"safety/sandbagging/#configuration-options","title":"Configuration Options","text":"<pre><code>detector = SandbagDetector(\n    mode=MonitorMode.PASSIVE,      # PASSIVE, ACTIVE, or LEARNING\n    sensitivity=0.7,                # 0.0 to 1.0 (higher = more sensitive)\n    baseline_turns=3,               # Turns to establish baseline\n    drop_threshold=0.3,             # Significant capability drop threshold\n)\n</code></pre>"},{"location":"safety/sandbagging/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>mode</code> MonitorMode PASSIVE Monitor mode <code>sensitivity</code> float 0.5 Detection sensitivity (0-1) <code>baseline_turns</code> int 3 Rounds to establish baseline <code>drop_threshold</code> float 0.3 Performance drop threshold"},{"location":"safety/sandbagging/#detection-algorithm","title":"Detection Algorithm","text":""},{"location":"safety/sandbagging/#phase-1-baseline-establishment","title":"Phase 1: Baseline Establishment","text":"<p>During the first N turns, the detector builds a capability profile:</p> <ul> <li>Word count and vocabulary complexity</li> <li>Unique word ratio</li> <li>Evidence usage patterns</li> <li>Causal link density</li> <li>Overall argument complexity</li> </ul>"},{"location":"safety/sandbagging/#phase-2-deviation-detection","title":"Phase 2: Deviation Detection","text":"<p>After baseline, monitor for significant drops in:</p> <ul> <li>Argument sophistication</li> <li>Evidence quality</li> <li>Reasoning depth</li> <li>Vocabulary complexity</li> </ul>"},{"location":"safety/sandbagging/#phase-3-pattern-analysis","title":"Phase 3: Pattern Analysis","text":"<p>Look for consistent underperformance patterns that suggest intentional sandbagging rather than normal variation.</p>"},{"location":"safety/sandbagging/#results","title":"Results","text":"<p>The detector contributes to debate safety alerts:</p> <pre><code>result = await debate.run()\n\n# Check for sandbagging alerts\nfor alert in result.safety_alerts:\n    if \"sandbag\" in alert.type.lower():\n        print(f\"Agent: {alert.agent}\")\n        print(f\"Severity: {alert.severity:.0%}\")\n</code></pre>"},{"location":"safety/sandbagging/#handling-false-positives","title":"Handling False Positives","text":"<p>Some legitimate reasons for performance drops:</p> <ul> <li>Topic difficulty varies</li> <li>Round context (opening vs. closing)</li> <li>Strategic simplification for clarity</li> </ul>"},{"location":"safety/sandbagging/#sensitivity-tuning","title":"Sensitivity Tuning","text":"<pre><code># Low sensitivity - only obvious sandbagging\ndetector = SandbagDetector(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.3,\n)\n\n# Medium sensitivity - balanced detection\ndetector = SandbagDetector(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.6,\n)\n\n# High sensitivity - catches subtle cases\ndetector = SandbagDetector(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.8,\n)\n</code></pre>"},{"location":"safety/sandbagging/#integration","title":"Integration","text":""},{"location":"safety/sandbagging/#with-debate","title":"With Debate","text":"<pre><code>from artemis.core.agent import Agent\nfrom artemis.core.debate import Debate\nfrom artemis.safety import SandbagDetector, MonitorMode\n\nagents = [\n    Agent(name=\"pro\", role=\"Advocate for the proposition\", model=\"gpt-4o\"),\n    Agent(name=\"con\", role=\"Advocate against the proposition\", model=\"gpt-4o\"),\n]\n\ndetector = SandbagDetector(\n    mode=MonitorMode.PASSIVE,\n    sensitivity=0.7,\n)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[detector.process],\n)\n\ndebate.assign_positions({\n    \"pro\": \"supports the proposition\",\n    \"con\": \"opposes the proposition\",\n})\n\nresult = await debate.run()\n\n# Check for sandbagging\nsandbagging_alerts = [\n    a for a in result.safety_alerts\n    if \"sandbag\" in a.type.lower()\n]\n\nfor alert in sandbagging_alerts:\n    print(f\"Agent {alert.agent}: {alert.severity:.0%} severity\")\n</code></pre>"},{"location":"safety/sandbagging/#with-other-monitors","title":"With Other Monitors","text":"<pre><code>from artemis.safety import (\n    SandbagDetector,\n    DeceptionMonitor,\n    BehaviorTracker,\n    MonitorMode,\n)\n\n# Combine monitors\nsandbag = SandbagDetector(mode=MonitorMode.PASSIVE, sensitivity=0.7)\ndeception = DeceptionMonitor(mode=MonitorMode.PASSIVE, sensitivity=0.6)\nbehavior = BehaviorTracker(mode=MonitorMode.PASSIVE, sensitivity=0.5)\n\ndebate = Debate(\n    topic=\"Your topic\",\n    agents=agents,\n    safety_monitors=[\n        sandbag.process,\n        deception.process,\n        behavior.process,\n    ],\n)\n</code></pre>"},{"location":"safety/sandbagging/#best-practices","title":"Best Practices","text":"<ol> <li>Start with medium sensitivity: Tune based on observations</li> <li>Establish good baselines: Allow enough turns for baseline</li> <li>Consider context: Some performance variation is normal</li> <li>Review alerts: Verify detections before acting</li> <li>Combine with other monitors: Sandbagging often accompanies other issues</li> </ol>"},{"location":"safety/sandbagging/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Deception Monitoring</li> <li>Explore Behavior Tracking</li> <li>Configure Safety Overview</li> </ul>"}]}