{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTEMIS Debate Demo\n",
    "\n",
    "This notebook demonstrates a complete end-to-end debate using ARTEMIS, including:\n",
    "- Setting up debate agents\n",
    "- Running a multi-round debate\n",
    "- Viewing the transcript and verdict\n",
    "- Exporting audit logs (JSON, Markdown, HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "print(\"API Keys configured:\")\n",
    "print(f\"  OpenAI: {'✓' if os.environ.get('OPENAI_API_KEY') else '✗'}\")\n",
    "print(f\"  Anthropic: {'✓' if os.environ.get('ANTHROPIC_API_KEY') else '✗'}\")\n",
    "print(f\"  Google AI Studio: {'✓' if os.environ.get('GOOGLE_API_KEY') else '✗'}\")\n",
    "print(f\"  Vertex AI: {'✓' if os.environ.get('GOOGLE_CLOUD_PROJECT') else '✗'}\")\n",
    "print(f\"  DeepSeek: {'✓' if os.environ.get('DEEPSEEK_API_KEY') else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Debate Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from artemis.core import Debate, Agent\n",
    "from artemis.core.jury import JuryPanel\n",
    "from artemis.core.types import JuryPerspective, JurorConfig\n",
    "\n",
    "# Choose model based on available API keys\n",
    "if os.environ.get('GOOGLE_CLOUD_PROJECT'):\n",
    "    MODEL = \"gemini-2.0-flash\"\n",
    "    print(f\"Using Vertex AI: {MODEL}\")\n",
    "elif os.environ.get('OPENAI_API_KEY'):\n",
    "    MODEL = \"gpt-4o-mini\"  # Cost-effective for demos\n",
    "    print(f\"Using OpenAI: {MODEL}\")\n",
    "else:\n",
    "    raise ValueError(\"No API key configured! Set OPENAI_API_KEY or GOOGLE_CLOUD_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create debate agents with different positions\n",
    "pro_agent = Agent(\n",
    "    name=\"Advocate\",\n",
    "    role=\"Proponent\",\n",
    "    position=\"in favor of remote work as the default\",\n",
    "    model=MODEL,\n",
    "    persona=\"You are a forward-thinking workplace strategist who believes in flexibility and work-life balance.\"\n",
    ")\n",
    "\n",
    "con_agent = Agent(\n",
    "    name=\"Traditionalist\",\n",
    "    role=\"Opponent\",\n",
    "    position=\"against remote work as the default\",\n",
    "    model=MODEL,\n",
    "    persona=\"You are a pragmatic business leader who values in-person collaboration and company culture.\"\n",
    ")\n",
    "\n",
    "print(f\"Created agents: {pro_agent.name} vs {con_agent.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a diverse jury panel\n",
    "jury = JuryPanel(\n",
    "    evaluators=3,\n",
    "    model=MODEL,\n",
    "    consensus_threshold=0.7,\n",
    ")\n",
    "\n",
    "print(f\"Jury panel: {len(jury)} jurors\")\n",
    "print(f\"Perspectives: {[j.perspective.value for j in jury.jurors]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the Debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the debate\n",
    "debate = Debate(\n",
    "    topic=\"Should remote work be the default for knowledge workers?\",\n",
    "    agents=[pro_agent, con_agent],\n",
    "    rounds=2,  # Keep short for demo\n",
    "    jury=jury,\n",
    ")\n",
    "\n",
    "print(f\"Debate topic: {debate.topic}\")\n",
    "print(f\"Rounds: {debate.rounds}\")\n",
    "print(f\"Agents: {[a.name for a in debate.agents]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the debate (this may take a minute)\n",
    "print(\"Running debate...\")\n",
    "result = await debate.run()\n",
    "print(f\"\\n✓ Debate complete! {len(result.transcript)} turns recorded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display verdict\n",
    "from IPython.display import Markdown, HTML\n",
    "\n",
    "verdict_md = f\"\"\"\n",
    "## Verdict\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Winner** | {result.verdict.decision} |\n",
    "| **Confidence** | {result.verdict.confidence:.0%} |\n",
    "| **Unanimous** | {'Yes' if result.verdict.unanimous else 'No'} |\n",
    "\n",
    "### Reasoning\n",
    "{result.verdict.reasoning}\n",
    "\n",
    "### Score Breakdown\n",
    "\"\"\"\n",
    "for agent, score in result.verdict.score_breakdown.items():\n",
    "    verdict_md += f\"- **{agent}**: {score:.2f}\\n\"\n",
    "\n",
    "display(Markdown(verdict_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display transcript\n",
    "transcript_md = \"## Debate Transcript\\n\\n\"\n",
    "\n",
    "for turn in result.transcript:\n",
    "    transcript_md += f\"### Round {turn.round} - {turn.agent} ({turn.argument.level.value})\\n\\n\"\n",
    "    transcript_md += f\"{turn.argument.content}\\n\\n\"\n",
    "    \n",
    "    if turn.argument.evidence:\n",
    "        transcript_md += \"**Evidence:**\\n\"\n",
    "        for ev in turn.argument.evidence:\n",
    "            transcript_md += f\"- [{ev.type}] {ev.content[:100]}...\\n\"\n",
    "        transcript_md += \"\\n\"\n",
    "    \n",
    "    transcript_md += \"---\\n\\n\"\n",
    "\n",
    "display(Markdown(transcript_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export Audit Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from artemis.utils.audit import export_debate_audit, AuditLog\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"./audit_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export in all formats\n",
    "paths = export_debate_audit(\n",
    "    result,\n",
    "    output_dir=output_dir,\n",
    "    formats=[\"json\", \"markdown\", \"html\"]\n",
    ")\n",
    "\n",
    "print(\"Exported audit logs:\")\n",
    "for fmt, path in paths.items():\n",
    "    print(f\"  {fmt.upper()}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display JSON structure\n",
    "import json\n",
    "\n",
    "with open(paths['json']) as f:\n",
    "    audit_json = json.load(f)\n",
    "\n",
    "print(\"JSON Audit Log Structure:\")\n",
    "print(f\"  - debate_id: {audit_json['debate_id'][:8]}...\")\n",
    "print(f\"  - topic: {audit_json['topic'][:50]}...\")\n",
    "print(f\"  - entries: {len(audit_json['entries'])} events\")\n",
    "print(f\"  - verdict: {audit_json['verdict']['decision']}\")\n",
    "print(f\"  - metadata: {list(audit_json['metadata'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display HTML report in notebook\n",
    "with open(paths['html']) as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Show HTML report (in iframe to contain styles)\n",
    "from IPython.display import IFrame\n",
    "display(IFrame(src=str(paths['html']), width='100%', height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Markdown report\n",
    "with open(paths['markdown']) as f:\n",
    "    md_content = f.read()\n",
    "\n",
    "display(Markdown(md_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Individual Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show evaluation scores for each turn\n",
    "import pandas as pd\n",
    "\n",
    "eval_data = []\n",
    "for turn in result.transcript:\n",
    "    if turn.evaluation:\n",
    "        eval_data.append({\n",
    "            'Round': turn.round,\n",
    "            'Agent': turn.agent,\n",
    "            'Level': turn.argument.level.value,\n",
    "            'Total Score': turn.evaluation.total_score,\n",
    "            'Causal Score': turn.evaluation.causal_score,\n",
    "            'Evidence Count': len(turn.argument.evidence),\n",
    "        })\n",
    "\n",
    "if eval_data:\n",
    "    df = pd.DataFrame(eval_data)\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No evaluation data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nDebate Statistics:\")\n",
    "print(f\"  Total turns: {len(result.transcript)}\")\n",
    "print(f\"  Rounds completed: {result.metadata.total_rounds}\")\n",
    "print(f\"  Safety alerts: {len(result.safety_alerts)}\")\n",
    "print(f\"  Final state: {result.final_state.value}\")\n",
    "\n",
    "if result.metadata.model_usage:\n",
    "    print(\"\\nToken Usage:\")\n",
    "    for model, usage in result.metadata.model_usage.items():\n",
    "        print(f\"  {model}: {usage.get('prompt_tokens', 0)} prompt, {usage.get('completion_tokens', 0)} completion\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
